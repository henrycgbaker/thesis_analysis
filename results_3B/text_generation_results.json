[
    {
        "CONFIGURATION_RUN_#0433": {
            "setup": {
                "experiment_id": "0433",
                "cycle_id": 2,
                "date_time": "April 24, 2025 at 08:13:05 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "num_processes_2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.19270670705009,
                        "average_latency_ms_per_batch": 4649.088338381262,
                        "throughput_queries_per_sec": 3.4415349495318357,
                        "throughput_tokens_per_sec": 440.51647354007497
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25723745792,
                        "gpu_max_memory_allocated_bytes": 25723745792,
                        "gpu_current_memory_reserved_bytes": 27206352896,
                        "gpu_max_memory_reserved_bytes": 27206352896
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            27.0,
                            100.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 1.6,
                        "cpu_memory_usage_bytes": 1968848896
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0433",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_1": 511.505518279519,
                            "process_0": 569.4864376598795
                        },
                        "ram_power": {
                            "process_1": 0.7066898345947266,
                            "process_0": 0.6854281425476074
                        },
                        "cpu_energy": {
                            "process_1": 0.001148896382588646,
                            "process_0": 0.0011465113434369412
                        },
                        "gpu_energy": {
                            "process_1": 0.005697190946639452,
                            "process_0": 0.0056809076002792835
                        },
                        "ram_energy": {
                            "process_1": 6.168657057007031e-06,
                            "process_0": 5.987782951833578e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.006852255986285104,
                            "process_0": 0.006833406726668059
                        },
                        "total_energy_joules": {
                            "process_1": 24668.121550626372,
                            "process_0": 24600.264216005013
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 540.4959779696992,
                        "ram_power_avg": 0.696058988571167,
                        "cpu_energy_total": 0.002295407726025587,
                        "gpu_energy_total": 0.011378098546918736,
                        "ram_energy_total": 1.2156440008840609e-05,
                        "total_energy_kwh": 0.013685662712953163,
                        "total_energy_joules": 49268.385766631385
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.33254590636693027,
                        "joules_per_token": 3.0071036234516226,
                        "flops_per_joule": 1068404850.083706,
                        "joules_per_flop": 9.359747851403458e-10
                    },
                    "per-process_emissions": [
                        0.00261036691797531,
                        0.002603186292524197
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0433": {
            "setup": {
                "experiment_id": "0433",
                "cycle_id": 2,
                "date_time": "April 24, 2025 at 08:13:05 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "num_processes_2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.19270670705009,
                        "average_latency_ms_per_batch": 4649.088338381262,
                        "throughput_queries_per_sec": 3.4415349495318357,
                        "throughput_tokens_per_sec": 440.51647354007497
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25723745792,
                        "gpu_max_memory_allocated_bytes": 25723745792,
                        "gpu_current_memory_reserved_bytes": 27206352896,
                        "gpu_max_memory_reserved_bytes": 27206352896
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            27.0,
                            100.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 1.6,
                        "cpu_memory_usage_bytes": 1968848896
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0433",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_1": 511.505518279519,
                            "process_0": 569.4864376598795
                        },
                        "ram_power": {
                            "process_1": 0.7066898345947266,
                            "process_0": 0.6854281425476074
                        },
                        "cpu_energy": {
                            "process_1": 0.001148896382588646,
                            "process_0": 0.0011465113434369412
                        },
                        "gpu_energy": {
                            "process_1": 0.005697190946639452,
                            "process_0": 0.0056809076002792835
                        },
                        "ram_energy": {
                            "process_1": 6.168657057007031e-06,
                            "process_0": 5.987782951833578e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.006852255986285104,
                            "process_0": 0.006833406726668059
                        },
                        "total_energy_joules": {
                            "process_1": 24668.121550626372,
                            "process_0": 24600.264216005013
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 540.4959779696992,
                        "ram_power_avg": 0.696058988571167,
                        "cpu_energy_total": 0.002295407726025587,
                        "gpu_energy_total": 0.011378098546918736,
                        "ram_energy_total": 1.2156440008840609e-05,
                        "total_energy_kwh": 0.013685662712953163,
                        "total_energy_joules": 49268.385766631385
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.33254590636693027,
                        "joules_per_token": 3.0071036234516226,
                        "flops_per_joule": 1068404850.083706,
                        "joules_per_flop": 9.359747851403458e-10
                    },
                    "per-process_emissions": [
                        0.00261036691797531,
                        0.002603186292524197
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0434": {
            "setup": {
                "experiment_id": "0434",
                "cycle_id": 2,
                "date_time": "April 24, 2025 at 08:14:31 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_burst_0.1_0.2_2.0_8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.1,
                    "delay_max": 0.2,
                    "simulate_burst": true,
                    "burst_interval": 2.0,
                    "burst_size": 8
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 40.66471969394479,
                        "average_latency_ms_per_batch": 5083.089961743099,
                        "throughput_queries_per_sec": 3.1476916836847133,
                        "throughput_tokens_per_sec": 402.9045355116433
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38352715776,
                        "gpu_max_memory_reserved_bytes": 38352715776
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            28.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.3,
                        "cpu_memory_usage_bytes": 1950965760
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0434",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_3": 890.1659149990265,
                            "process_1": 830.8013117685306,
                            "process_0": 943.7213300520317,
                            "process_2": 1051.4012729234876
                        },
                        "ram_power": {
                            "process_3": 0.7070932388305664,
                            "process_1": 0.7295322418212891,
                            "process_0": 0.6795201301574707,
                            "process_2": 0.7286968231201172
                        },
                        "cpu_energy": {
                            "process_3": 0.0012606065939762632,
                            "process_1": 0.0012609264494421947,
                            "process_0": 0.0012560275830583124,
                            "process_2": 0.0012492329829619845
                        },
                        "gpu_energy": {
                            "process_3": 0.010296384903771383,
                            "process_1": 0.010296384903771383,
                            "process_0": 0.010269930160385243,
                            "process_2": 0.01021660872883956
                        },
                        "ram_energy": {
                            "process_3": 6.834482546350638e-06,
                            "process_1": 7.050048929727729e-06,
                            "process_0": 6.5224243260973986e-06,
                            "process_2": 6.952346230523459e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.011563825980293997,
                            "process_1": 0.011564361402143305,
                            "process_0": 0.01153248016776965,
                            "process_2": 0.011472794058032072
                        },
                        "total_energy_joules": {
                            "process_3": 41629.77352905839,
                            "process_1": 41631.7010477159,
                            "process_0": 41516.92860397074,
                            "process_2": 41302.05860891546
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 929.0224574357692,
                        "ram_power_avg": 0.7112106084823608,
                        "cpu_energy_total": 0.0050267936094387546,
                        "gpu_energy_total": 0.04107930869676757,
                        "ram_energy_total": 2.7359302032699225e-05,
                        "total_energy_kwh": 0.04613346160823903,
                        "total_energy_joules": 166080.46178966048
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09865097810692627,
                        "joules_per_token": 10.136746935404082,
                        "flops_per_joule": 316946266.53633904,
                        "joules_per_flop": 3.155108943002319e-09
                    },
                    "per-process_emissions": [
                        0.004405239507192999,
                        0.004405443476146492,
                        0.004393298319911848,
                        0.004370560896407318
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0434": {
            "setup": {
                "experiment_id": "0434",
                "cycle_id": 2,
                "date_time": "April 24, 2025 at 08:14:31 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_burst_0.1_0.2_2.0_8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.1,
                    "delay_max": 0.2,
                    "simulate_burst": true,
                    "burst_interval": 2.0,
                    "burst_size": 8
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 40.66471969394479,
                        "average_latency_ms_per_batch": 5083.089961743099,
                        "throughput_queries_per_sec": 3.1476916836847133,
                        "throughput_tokens_per_sec": 402.9045355116433
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38352715776,
                        "gpu_max_memory_reserved_bytes": 38352715776
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            28.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.3,
                        "cpu_memory_usage_bytes": 1950965760
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0434",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_3": 890.1659149990265,
                            "process_1": 830.8013117685306,
                            "process_0": 943.7213300520317,
                            "process_2": 1051.4012729234876
                        },
                        "ram_power": {
                            "process_3": 0.7070932388305664,
                            "process_1": 0.7295322418212891,
                            "process_0": 0.6795201301574707,
                            "process_2": 0.7286968231201172
                        },
                        "cpu_energy": {
                            "process_3": 0.0012606065939762632,
                            "process_1": 0.0012609264494421947,
                            "process_0": 0.0012560275830583124,
                            "process_2": 0.0012492329829619845
                        },
                        "gpu_energy": {
                            "process_3": 0.010296384903771383,
                            "process_1": 0.010296384903771383,
                            "process_0": 0.010269930160385243,
                            "process_2": 0.01021660872883956
                        },
                        "ram_energy": {
                            "process_3": 6.834482546350638e-06,
                            "process_1": 7.050048929727729e-06,
                            "process_0": 6.5224243260973986e-06,
                            "process_2": 6.952346230523459e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.011563825980293997,
                            "process_1": 0.011564361402143305,
                            "process_0": 0.01153248016776965,
                            "process_2": 0.011472794058032072
                        },
                        "total_energy_joules": {
                            "process_3": 41629.77352905839,
                            "process_1": 41631.7010477159,
                            "process_0": 41516.92860397074,
                            "process_2": 41302.05860891546
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 929.0224574357692,
                        "ram_power_avg": 0.7112106084823608,
                        "cpu_energy_total": 0.0050267936094387546,
                        "gpu_energy_total": 0.04107930869676757,
                        "ram_energy_total": 2.7359302032699225e-05,
                        "total_energy_kwh": 0.04613346160823903,
                        "total_energy_joules": 166080.46178966048
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09865097810692627,
                        "joules_per_token": 10.136746935404082,
                        "flops_per_joule": 316946266.53633904,
                        "joules_per_flop": 3.155108943002319e-09
                    },
                    "per-process_emissions": [
                        0.004405239507192999,
                        0.004405443476146492,
                        0.004393298319911848,
                        0.004370560896407318
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0435": {
            "setup": {
                "experiment_id": "0435",
                "cycle_id": 2,
                "date_time": "April 24, 2025 at 08:15:53 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_1_temp_1.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 1.2,
                    "decoder_top_k": 1,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.567394502984826,
                        "average_latency_ms_per_batch": 4695.924312873103,
                        "throughput_queries_per_sec": 3.4072099407860206,
                        "throughput_tokens_per_sec": 436.12287242061063
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38449184768,
                        "gpu_max_memory_reserved_bytes": 38449184768
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            28.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.2,
                        "cpu_memory_usage_bytes": 1974128640
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0435",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_1": 930.6701006484815,
                            "process_2": 1026.235901210921,
                            "process_3": 779.4197823344329,
                            "process_0": 964.3886461389865
                        },
                        "ram_power": {
                            "process_1": 0.7230920791625977,
                            "process_2": 0.7226929664611816,
                            "process_3": 0.7313232421875001,
                            "process_0": 0.6871576309204102
                        },
                        "cpu_energy": {
                            "process_1": 0.0011592708101852622,
                            "process_2": 0.0011554455584791867,
                            "process_3": 0.0011664715863425952,
                            "process_0": 0.0011586073638791279
                        },
                        "gpu_energy": {
                            "process_1": 0.010151557565684755,
                            "process_2": 0.010129298381208685,
                            "process_3": 0.010174312306110878,
                            "process_0": 0.010142741447520365
                        },
                        "ram_energy": {
                            "process_1": 6.383399767158617e-06,
                            "process_2": 6.3143277855512754e-06,
                            "process_3": 6.489191279389741e-06,
                            "process_0": 6.0545469659580885e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.011317211775637177,
                            "process_2": 0.011291058267473423,
                            "process_3": 0.011347273083732859,
                            "process_0": 0.01130740335836545
                        },
                        "total_energy_joules": {
                            "process_1": 40741.96239229383,
                            "process_2": 40647.809762904326,
                            "process_3": 40850.18310143829,
                            "process_0": 40706.652090115625
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 925.1786075832055,
                        "ram_power_avg": 0.7160664796829224,
                        "cpu_energy_total": 0.004639795318886172,
                        "gpu_energy_total": 0.04059790970052468,
                        "ram_energy_total": 2.5241465798057722e-05,
                        "total_energy_kwh": 0.045262946485208906,
                        "total_energy_joules": 162946.60734675208
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.10054827324593925,
                        "joules_per_token": 9.945471639816411,
                        "flops_per_joule": 323041904.13028085,
                        "joules_per_flop": 3.095573630586037e-09
                    },
                    "per-process_emissions": [
                        0.004311291825928982,
                        0.004301328646994001,
                        0.004322743681248032,
                        0.004307555309369319
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0435": {
            "setup": {
                "experiment_id": "0435",
                "cycle_id": 2,
                "date_time": "April 24, 2025 at 08:15:53 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_1_temp_1.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 1.2,
                    "decoder_top_k": 1,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.567394502984826,
                        "average_latency_ms_per_batch": 4695.924312873103,
                        "throughput_queries_per_sec": 3.4072099407860206,
                        "throughput_tokens_per_sec": 436.12287242061063
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38449184768,
                        "gpu_max_memory_reserved_bytes": 38449184768
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            28.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.2,
                        "cpu_memory_usage_bytes": 1974128640
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0435",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_1": 930.6701006484815,
                            "process_2": 1026.235901210921,
                            "process_3": 779.4197823344329,
                            "process_0": 964.3886461389865
                        },
                        "ram_power": {
                            "process_1": 0.7230920791625977,
                            "process_2": 0.7226929664611816,
                            "process_3": 0.7313232421875001,
                            "process_0": 0.6871576309204102
                        },
                        "cpu_energy": {
                            "process_1": 0.0011592708101852622,
                            "process_2": 0.0011554455584791867,
                            "process_3": 0.0011664715863425952,
                            "process_0": 0.0011586073638791279
                        },
                        "gpu_energy": {
                            "process_1": 0.010151557565684755,
                            "process_2": 0.010129298381208685,
                            "process_3": 0.010174312306110878,
                            "process_0": 0.010142741447520365
                        },
                        "ram_energy": {
                            "process_1": 6.383399767158617e-06,
                            "process_2": 6.3143277855512754e-06,
                            "process_3": 6.489191279389741e-06,
                            "process_0": 6.0545469659580885e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.011317211775637177,
                            "process_2": 0.011291058267473423,
                            "process_3": 0.011347273083732859,
                            "process_0": 0.01130740335836545
                        },
                        "total_energy_joules": {
                            "process_1": 40741.96239229383,
                            "process_2": 40647.809762904326,
                            "process_3": 40850.18310143829,
                            "process_0": 40706.652090115625
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 925.1786075832055,
                        "ram_power_avg": 0.7160664796829224,
                        "cpu_energy_total": 0.004639795318886172,
                        "gpu_energy_total": 0.04059790970052468,
                        "ram_energy_total": 2.5241465798057722e-05,
                        "total_energy_kwh": 0.045262946485208906,
                        "total_energy_joules": 162946.60734675208
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.10054827324593925,
                        "joules_per_token": 9.945471639816411,
                        "flops_per_joule": 323041904.13028085,
                        "joules_per_flop": 3.095573630586037e-09
                    },
                    "per-process_emissions": [
                        0.004311291825928982,
                        0.004301328646994001,
                        0.004322743681248032,
                        0.004307555309369319
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0436": {
            "setup": {
                "experiment_id": "0436",
                "cycle_id": 2,
                "date_time": "April 24, 2025 at 08:17:18 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_topp_0.98_temp_0.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_p",
                    "decoder_temperature": 0.4,
                    "decoder_top_k": null,
                    "decoder_top_p": 0.98
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.4631942390115,
                        "average_latency_ms_per_batch": 4807.8992798764375,
                        "throughput_queries_per_sec": 3.3278567350543997,
                        "throughput_tokens_per_sec": 425.96566208696316
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38403047424,
                        "gpu_max_memory_reserved_bytes": 38403047424
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.3,
                        "cpu_memory_usage_bytes": 1982574592
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0436",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_2": 1044.0080553810956,
                            "process_1": 903.8532773713977,
                            "process_0": 863.3904196375335,
                            "process_3": 801.071143033574
                        },
                        "ram_power": {
                            "process_2": 0.732125759124756,
                            "process_1": 0.7263879776000977,
                            "process_0": 0.6908969879150391,
                            "process_3": 0.7265625
                        },
                        "cpu_energy": {
                            "process_2": 0.0011802263630252127,
                            "process_1": 0.001190105598376249,
                            "process_0": 0.0011865149062159615,
                            "process_3": 0.0011923583492534813
                        },
                        "gpu_energy": {
                            "process_2": 0.010291286010797762,
                            "process_1": 0.010353924394243208,
                            "process_0": 0.010350669113860178,
                            "process_3": 0.010368374405802072
                        },
                        "ram_energy": {
                            "process_2": 6.589013659085142e-06,
                            "process_1": 6.581444868838157e-06,
                            "process_0": 6.249274495462788e-06,
                            "process_3": 6.606008433847519e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.01147810138748206,
                            "process_1": 0.011550611437488298,
                            "process_0": 0.011543433294571604,
                            "process_3": 0.011567338763489398
                        },
                        "total_energy_joules": {
                            "process_2": 41321.16499493542,
                            "process_1": 41582.201174957874,
                            "process_0": 41556.35986045777,
                            "process_3": 41642.41954856183
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 903.0807238559001,
                        "ram_power_avg": 0.7189933061599731,
                        "cpu_energy_total": 0.004749205216870904,
                        "gpu_energy_total": 0.04136425392470322,
                        "ram_energy_total": 2.6025741457233605e-05,
                        "total_energy_kwh": 0.04613948488303136,
                        "total_energy_joules": 166102.1455789129
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09863809972410129,
                        "joules_per_token": 10.138070408869195,
                        "flops_per_joule": 316904890.8152491,
                        "joules_per_flop": 3.1555208801841603e-09
                    },
                    "per-process_emissions": [
                        0.004372582723561291,
                        0.0044002054271111675,
                        0.004397470913567052,
                        0.004406577701951286
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0436": {
            "setup": {
                "experiment_id": "0436",
                "cycle_id": 2,
                "date_time": "April 24, 2025 at 08:17:18 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_topp_0.98_temp_0.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_p",
                    "decoder_temperature": 0.4,
                    "decoder_top_k": null,
                    "decoder_top_p": 0.98
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.4631942390115,
                        "average_latency_ms_per_batch": 4807.8992798764375,
                        "throughput_queries_per_sec": 3.3278567350543997,
                        "throughput_tokens_per_sec": 425.96566208696316
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38403047424,
                        "gpu_max_memory_reserved_bytes": 38403047424
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.3,
                        "cpu_memory_usage_bytes": 1982574592
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0436",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_2": 1044.0080553810956,
                            "process_1": 903.8532773713977,
                            "process_0": 863.3904196375335,
                            "process_3": 801.071143033574
                        },
                        "ram_power": {
                            "process_2": 0.732125759124756,
                            "process_1": 0.7263879776000977,
                            "process_0": 0.6908969879150391,
                            "process_3": 0.7265625
                        },
                        "cpu_energy": {
                            "process_2": 0.0011802263630252127,
                            "process_1": 0.001190105598376249,
                            "process_0": 0.0011865149062159615,
                            "process_3": 0.0011923583492534813
                        },
                        "gpu_energy": {
                            "process_2": 0.010291286010797762,
                            "process_1": 0.010353924394243208,
                            "process_0": 0.010350669113860178,
                            "process_3": 0.010368374405802072
                        },
                        "ram_energy": {
                            "process_2": 6.589013659085142e-06,
                            "process_1": 6.581444868838157e-06,
                            "process_0": 6.249274495462788e-06,
                            "process_3": 6.606008433847519e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.01147810138748206,
                            "process_1": 0.011550611437488298,
                            "process_0": 0.011543433294571604,
                            "process_3": 0.011567338763489398
                        },
                        "total_energy_joules": {
                            "process_2": 41321.16499493542,
                            "process_1": 41582.201174957874,
                            "process_0": 41556.35986045777,
                            "process_3": 41642.41954856183
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 903.0807238559001,
                        "ram_power_avg": 0.7189933061599731,
                        "cpu_energy_total": 0.004749205216870904,
                        "gpu_energy_total": 0.04136425392470322,
                        "ram_energy_total": 2.6025741457233605e-05,
                        "total_energy_kwh": 0.04613948488303136,
                        "total_energy_joules": 166102.1455789129
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09863809972410129,
                        "joules_per_token": 10.138070408869195,
                        "flops_per_joule": 316904890.8152491,
                        "joules_per_flop": 3.1555208801841603e-09
                    },
                    "per-process_emissions": [
                        0.004372582723561291,
                        0.0044002054271111675,
                        0.004397470913567052,
                        0.004406577701951286
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0437": {
            "setup": {
                "experiment_id": "0437",
                "cycle_id": 2,
                "date_time": "April 24, 2025 at 08:18:43 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_topp_0.9_temp_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_p",
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": 0.9
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.264917113934644,
                        "average_latency_ms_per_batch": 4783.11463924183,
                        "throughput_queries_per_sec": 3.345100673258409,
                        "throughput_tokens_per_sec": 428.17288617707635
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38403047424,
                        "gpu_max_memory_reserved_bytes": 38403047424
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            28.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.3,
                        "cpu_memory_usage_bytes": 1982291968
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0437",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 704.6040826408481,
                            "process_3": 620.8719650614685,
                            "process_2": 1165.3737331396355,
                            "process_1": 1110.369406866299
                        },
                        "ram_power": {
                            "process_0": 0.690824031829834,
                            "process_3": 0.725654125213623,
                            "process_2": 0.7258772850036621,
                            "process_1": 0.7256269454956055
                        },
                        "cpu_energy": {
                            "process_0": 0.0011814184251252304,
                            "process_3": 0.0011889683410317958,
                            "process_2": 0.0011774336843154746,
                            "process_1": 0.001178709010611783
                        },
                        "gpu_energy": {
                            "process_0": 0.010319157977543014,
                            "process_3": 0.010352733559958693,
                            "process_2": 0.0103002379624062,
                            "process_1": 0.010316831586791864
                        },
                        "ram_energy": {
                            "process_0": 6.2969759937762655e-06,
                            "process_3": 6.671174385586339e-06,
                            "process_2": 6.594043031850577e-06,
                            "process_1": 6.613639956541573e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.011506873378662021,
                            "process_3": 0.011548373075376067,
                            "process_2": 0.011484265689753526,
                            "process_1": 0.011502154237360193
                        },
                        "total_energy_joules": {
                            "process_0": 41424.74416318328,
                            "process_3": 41574.14307135384,
                            "process_2": 41343.35648311269,
                            "process_1": 41407.7552544967
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 900.3047969270627,
                        "ram_power_avg": 0.7169955968856812,
                        "cpu_energy_total": 0.004726529461084284,
                        "gpu_energy_total": 0.04128896108669977,
                        "ram_energy_total": 2.6175833367754754e-05,
                        "total_energy_kwh": 0.046041666381151806,
                        "total_energy_joules": 165749.9989721465
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09884766275475665,
                        "joules_per_token": 10.116577085702302,
                        "flops_per_joule": 317578175.77850884,
                        "joules_per_flop": 3.148830984838953e-09
                    },
                    "per-process_emissions": [
                        0.004383543413601297,
                        0.004399352723064513,
                        0.004374931014511606,
                        0.004381745656722366
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0437": {
            "setup": {
                "experiment_id": "0437",
                "cycle_id": 2,
                "date_time": "April 24, 2025 at 08:18:43 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_topp_0.9_temp_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_p",
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": 0.9
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.264917113934644,
                        "average_latency_ms_per_batch": 4783.11463924183,
                        "throughput_queries_per_sec": 3.345100673258409,
                        "throughput_tokens_per_sec": 428.17288617707635
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38403047424,
                        "gpu_max_memory_reserved_bytes": 38403047424
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            28.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.3,
                        "cpu_memory_usage_bytes": 1982291968
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0437",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 704.6040826408481,
                            "process_3": 620.8719650614685,
                            "process_2": 1165.3737331396355,
                            "process_1": 1110.369406866299
                        },
                        "ram_power": {
                            "process_0": 0.690824031829834,
                            "process_3": 0.725654125213623,
                            "process_2": 0.7258772850036621,
                            "process_1": 0.7256269454956055
                        },
                        "cpu_energy": {
                            "process_0": 0.0011814184251252304,
                            "process_3": 0.0011889683410317958,
                            "process_2": 0.0011774336843154746,
                            "process_1": 0.001178709010611783
                        },
                        "gpu_energy": {
                            "process_0": 0.010319157977543014,
                            "process_3": 0.010352733559958693,
                            "process_2": 0.0103002379624062,
                            "process_1": 0.010316831586791864
                        },
                        "ram_energy": {
                            "process_0": 6.2969759937762655e-06,
                            "process_3": 6.671174385586339e-06,
                            "process_2": 6.594043031850577e-06,
                            "process_1": 6.613639956541573e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.011506873378662021,
                            "process_3": 0.011548373075376067,
                            "process_2": 0.011484265689753526,
                            "process_1": 0.011502154237360193
                        },
                        "total_energy_joules": {
                            "process_0": 41424.74416318328,
                            "process_3": 41574.14307135384,
                            "process_2": 41343.35648311269,
                            "process_1": 41407.7552544967
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 900.3047969270627,
                        "ram_power_avg": 0.7169955968856812,
                        "cpu_energy_total": 0.004726529461084284,
                        "gpu_energy_total": 0.04128896108669977,
                        "ram_energy_total": 2.6175833367754754e-05,
                        "total_energy_kwh": 0.046041666381151806,
                        "total_energy_joules": 165749.9989721465
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09884766275475665,
                        "joules_per_token": 10.116577085702302,
                        "flops_per_joule": 317578175.77850884,
                        "joules_per_flop": 3.148830984838953e-09
                    },
                    "per-process_emissions": [
                        0.004383543413601297,
                        0.004399352723064513,
                        0.004374931014511606,
                        0.004381745656722366
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0438": {
            "setup": {
                "experiment_id": "0438",
                "cycle_id": 2,
                "date_time": "April 24, 2025 at 08:20:07 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_300_temp_0.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 0.0,
                    "decoder_top_k": 300,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.085142477066256,
                        "average_latency_ms_per_batch": 4635.642809633282,
                        "throughput_queries_per_sec": 3.451517007900299,
                        "throughput_tokens_per_sec": 441.7941770112383
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38325452800,
                        "gpu_max_memory_reserved_bytes": 38325452800
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.2,
                        "cpu_memory_usage_bytes": 1966276608
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0438",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_1": 957.1615925647953,
                            "process_0": 1221.3604361752343,
                            "process_2": 605.8507138762842,
                            "process_3": 567.9295422171424
                        },
                        "ram_power": {
                            "process_1": 0.7279915809631348,
                            "process_0": 0.6845912933349609,
                            "process_2": 0.7274751663208008,
                            "process_3": 0.7278614044189453
                        },
                        "cpu_energy": {
                            "process_1": 0.0011467350358743717,
                            "process_0": 0.0011460512306275634,
                            "process_2": 0.0011461278005881466,
                            "process_3": 0.0011562875548097509
                        },
                        "gpu_energy": {
                            "process_1": 0.01009456196453229,
                            "process_0": 0.010088383070700147,
                            "process_2": 0.010074334726127177,
                            "process_3": 0.010125933100741946
                        },
                        "ram_energy": {
                            "process_1": 6.414596883466104e-06,
                            "process_0": 6.227762666266324e-06,
                            "process_2": 6.406228970903705e-06,
                            "process_3": 6.457192754407603e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.011247711597290128,
                            "process_0": 0.011240662063993968,
                            "process_2": 0.011226868755686232,
                            "process_3": 0.011288677848306103
                        },
                        "total_energy_joules": {
                            "process_1": 40491.76175024446,
                            "process_0": 40466.383430378286,
                            "process_2": 40416.727520470435,
                            "process_3": 40639.24025390197
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 838.0755712083642,
                        "ram_power_avg": 0.7169798612594604,
                        "cpu_energy_total": 0.004595201621899832,
                        "gpu_energy_total": 0.04038321286210156,
                        "ram_energy_total": 2.5505781275043735e-05,
                        "total_energy_kwh": 0.04500392026527643,
                        "total_energy_joules": 162014.11295499516
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.10112699258830127,
                        "joules_per_token": 9.888556698913279,
                        "flops_per_joule": 324901216.00385594,
                        "joules_per_flop": 3.0778585943739033e-09
                    },
                    "per-process_emissions": [
                        0.004284815732987674,
                        0.004282130213278502,
                        0.00427687565247867,
                        0.00430042182631221
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0438": {
            "setup": {
                "experiment_id": "0438",
                "cycle_id": 2,
                "date_time": "April 24, 2025 at 08:20:07 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_300_temp_0.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 0.0,
                    "decoder_top_k": 300,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.085142477066256,
                        "average_latency_ms_per_batch": 4635.642809633282,
                        "throughput_queries_per_sec": 3.451517007900299,
                        "throughput_tokens_per_sec": 441.7941770112383
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38325452800,
                        "gpu_max_memory_reserved_bytes": 38325452800
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.2,
                        "cpu_memory_usage_bytes": 1966276608
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0438",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_1": 957.1615925647953,
                            "process_0": 1221.3604361752343,
                            "process_2": 605.8507138762842,
                            "process_3": 567.9295422171424
                        },
                        "ram_power": {
                            "process_1": 0.7279915809631348,
                            "process_0": 0.6845912933349609,
                            "process_2": 0.7274751663208008,
                            "process_3": 0.7278614044189453
                        },
                        "cpu_energy": {
                            "process_1": 0.0011467350358743717,
                            "process_0": 0.0011460512306275634,
                            "process_2": 0.0011461278005881466,
                            "process_3": 0.0011562875548097509
                        },
                        "gpu_energy": {
                            "process_1": 0.01009456196453229,
                            "process_0": 0.010088383070700147,
                            "process_2": 0.010074334726127177,
                            "process_3": 0.010125933100741946
                        },
                        "ram_energy": {
                            "process_1": 6.414596883466104e-06,
                            "process_0": 6.227762666266324e-06,
                            "process_2": 6.406228970903705e-06,
                            "process_3": 6.457192754407603e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.011247711597290128,
                            "process_0": 0.011240662063993968,
                            "process_2": 0.011226868755686232,
                            "process_3": 0.011288677848306103
                        },
                        "total_energy_joules": {
                            "process_1": 40491.76175024446,
                            "process_0": 40466.383430378286,
                            "process_2": 40416.727520470435,
                            "process_3": 40639.24025390197
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 838.0755712083642,
                        "ram_power_avg": 0.7169798612594604,
                        "cpu_energy_total": 0.004595201621899832,
                        "gpu_energy_total": 0.04038321286210156,
                        "ram_energy_total": 2.5505781275043735e-05,
                        "total_energy_kwh": 0.04500392026527643,
                        "total_energy_joules": 162014.11295499516
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.10112699258830127,
                        "joules_per_token": 9.888556698913279,
                        "flops_per_joule": 324901216.00385594,
                        "joules_per_flop": 3.0778585943739033e-09
                    },
                    "per-process_emissions": [
                        0.004284815732987674,
                        0.004282130213278502,
                        0.00427687565247867,
                        0.00430042182631221
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0439": {
            "setup": {
                "experiment_id": "0439",
                "cycle_id": 2,
                "date_time": "April 24, 2025 at 08:21:29 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_5_temp_0.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 0.4,
                    "decoder_top_k": 5,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.57584161206614,
                        "average_latency_ms_per_batch": 4696.980201508268,
                        "throughput_queries_per_sec": 3.406443994561052,
                        "throughput_tokens_per_sec": 436.02483130381466
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38449184768,
                        "gpu_max_memory_reserved_bytes": 38449184768
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            27.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.2,
                        "cpu_memory_usage_bytes": 1956818944
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0439",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_1": 934.1154195125711,
                            "process_0": 1033.8226211302413,
                            "process_2": 1030.8600343169187,
                            "process_3": 892.7094979200388
                        },
                        "ram_power": {
                            "process_1": 0.7244381904602051,
                            "process_0": 0.680844783782959,
                            "process_2": 0.7320971488952637,
                            "process_3": 0.708409309387207
                        },
                        "cpu_energy": {
                            "process_1": 0.001162268761663654,
                            "process_0": 0.0011603588364068856,
                            "process_2": 0.001159915932623335,
                            "process_3": 0.00116511687025195
                        },
                        "gpu_energy": {
                            "process_1": 0.01017107730352329,
                            "process_0": 0.010163463686323482,
                            "process_2": 0.010163463686323482,
                            "process_3": 0.010193487043673954
                        },
                        "ram_energy": {
                            "process_1": 6.441206250167532e-06,
                            "process_0": 6.057537982748525e-06,
                            "process_2": 6.523211214306057e-06,
                            "process_3": 6.360581729585753e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.011339787271437113,
                            "process_0": 0.011329880060713112,
                            "process_2": 0.011329902830161127,
                            "process_3": 0.011364964495655494
                        },
                        "total_energy_joules": {
                            "process_1": 40823.2341771736,
                            "process_0": 40787.568218567205,
                            "process_2": 40787.65018858005,
                            "process_3": 40913.87218435978
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 972.8768932199424,
                        "ram_power_avg": 0.7114473581314087,
                        "cpu_energy_total": 0.004647660400945825,
                        "gpu_energy_total": 0.04069149171984421,
                        "ram_energy_total": 2.5382537176807866e-05,
                        "total_energy_kwh": 0.045364534657966846,
                        "total_energy_joules": 163312.32476868064
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.1003231080275581,
                        "joules_per_token": 9.967793259807168,
                        "flops_per_joule": 322318492.39438915,
                        "joules_per_flop": 3.1025213371139726e-09
                    },
                    "per-process_emissions": [
                        0.004319891961053968,
                        0.00431611780912866,
                        0.004316126483149881,
                        0.004329483224619961
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0439": {
            "setup": {
                "experiment_id": "0439",
                "cycle_id": 2,
                "date_time": "April 24, 2025 at 08:21:29 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_5_temp_0.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 0.4,
                    "decoder_top_k": 5,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.57584161206614,
                        "average_latency_ms_per_batch": 4696.980201508268,
                        "throughput_queries_per_sec": 3.406443994561052,
                        "throughput_tokens_per_sec": 436.02483130381466
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38449184768,
                        "gpu_max_memory_reserved_bytes": 38449184768
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            27.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.2,
                        "cpu_memory_usage_bytes": 1956818944
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0439",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_1": 934.1154195125711,
                            "process_0": 1033.8226211302413,
                            "process_2": 1030.8600343169187,
                            "process_3": 892.7094979200388
                        },
                        "ram_power": {
                            "process_1": 0.7244381904602051,
                            "process_0": 0.680844783782959,
                            "process_2": 0.7320971488952637,
                            "process_3": 0.708409309387207
                        },
                        "cpu_energy": {
                            "process_1": 0.001162268761663654,
                            "process_0": 0.0011603588364068856,
                            "process_2": 0.001159915932623335,
                            "process_3": 0.00116511687025195
                        },
                        "gpu_energy": {
                            "process_1": 0.01017107730352329,
                            "process_0": 0.010163463686323482,
                            "process_2": 0.010163463686323482,
                            "process_3": 0.010193487043673954
                        },
                        "ram_energy": {
                            "process_1": 6.441206250167532e-06,
                            "process_0": 6.057537982748525e-06,
                            "process_2": 6.523211214306057e-06,
                            "process_3": 6.360581729585753e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.011339787271437113,
                            "process_0": 0.011329880060713112,
                            "process_2": 0.011329902830161127,
                            "process_3": 0.011364964495655494
                        },
                        "total_energy_joules": {
                            "process_1": 40823.2341771736,
                            "process_0": 40787.568218567205,
                            "process_2": 40787.65018858005,
                            "process_3": 40913.87218435978
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 972.8768932199424,
                        "ram_power_avg": 0.7114473581314087,
                        "cpu_energy_total": 0.004647660400945825,
                        "gpu_energy_total": 0.04069149171984421,
                        "ram_energy_total": 2.5382537176807866e-05,
                        "total_energy_kwh": 0.045364534657966846,
                        "total_energy_joules": 163312.32476868064
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.1003231080275581,
                        "joules_per_token": 9.967793259807168,
                        "flops_per_joule": 322318492.39438915,
                        "joules_per_flop": 3.1025213371139726e-09
                    },
                    "per-process_emissions": [
                        0.004319891961053968,
                        0.00431611780912866,
                        0.004316126483149881,
                        0.004329483224619961
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0440": {
            "setup": {
                "experiment_id": "0440",
                "cycle_id": 2,
                "date_time": "April 24, 2025 at 08:22:44 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_28",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 28,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 28.679072881001048,
                        "average_latency_ms_per_batch": 5735.81457620021,
                        "throughput_queries_per_sec": 4.46318472466367,
                        "throughput_tokens_per_sec": 571.2876447569498
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 39535509504,
                        "gpu_max_memory_reserved_bytes": 39535509504
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.2,
                        "cpu_memory_usage_bytes": 1972461568
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0440",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_3": 882.5879638498308,
                            "process_1": 960.4543507294413,
                            "process_0": 1039.8751327637674,
                            "process_2": 960.2969310670311
                        },
                        "ram_power": {
                            "process_3": 0.7236270904541016,
                            "process_1": 0.7219033241271973,
                            "process_0": 0.6861906051635742,
                            "process_2": 0.716191291809082
                        },
                        "cpu_energy": {
                            "process_3": 0.0008923877564411669,
                            "process_1": 0.0008871681496912061,
                            "process_0": 0.0008830895256232904,
                            "process_2": 0.0008857363370570963
                        },
                        "gpu_energy": {
                            "process_3": 0.007882739639519798,
                            "process_1": 0.007869908518145508,
                            "process_0": 0.00783985904965867,
                            "process_2": 0.007864927403050004
                        },
                        "ram_energy": {
                            "process_3": 5.0067962272720924e-06,
                            "process_1": 4.968775947765608e-06,
                            "process_0": 4.697632845843465e-06,
                            "process_2": 4.915025350770203e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.008780134192188238,
                            "process_1": 0.008762045443784477,
                            "process_0": 0.008727646208127804,
                            "process_2": 0.008755578765457872
                        },
                        "total_energy_joules": {
                            "process_3": 31608.483091877657,
                            "process_1": 31543.36359762412,
                            "process_0": 31419.526349260093,
                            "process_2": 31520.08355564834
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 960.8035946025176,
                        "ram_power_avg": 0.7119780778884888,
                        "cpu_energy_total": 0.0035483817688127597,
                        "gpu_energy_total": 0.03145743461037398,
                        "ram_energy_total": 1.958823037165137e-05,
                        "total_energy_kwh": 0.03502540460955839,
                        "total_energy_joules": 126091.4565944102
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.1299374314685038,
                        "joules_per_token": 7.69601175502992,
                        "flops_per_joule": 417463512.0457284,
                        "joules_per_flop": 2.3954189315842804e-09
                    },
                    "per-process_emissions": [
                        0.003344792120514109,
                        0.003337901211809697,
                        0.003324796822986287,
                        0.0033354377307011768
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0440": {
            "setup": {
                "experiment_id": "0440",
                "cycle_id": 2,
                "date_time": "April 24, 2025 at 08:22:44 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_28",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 28,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 28.679072881001048,
                        "average_latency_ms_per_batch": 5735.81457620021,
                        "throughput_queries_per_sec": 4.46318472466367,
                        "throughput_tokens_per_sec": 571.2876447569498
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 39535509504,
                        "gpu_max_memory_reserved_bytes": 39535509504
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.2,
                        "cpu_memory_usage_bytes": 1972461568
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0440",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_3": 882.5879638498308,
                            "process_1": 960.4543507294413,
                            "process_0": 1039.8751327637674,
                            "process_2": 960.2969310670311
                        },
                        "ram_power": {
                            "process_3": 0.7236270904541016,
                            "process_1": 0.7219033241271973,
                            "process_0": 0.6861906051635742,
                            "process_2": 0.716191291809082
                        },
                        "cpu_energy": {
                            "process_3": 0.0008923877564411669,
                            "process_1": 0.0008871681496912061,
                            "process_0": 0.0008830895256232904,
                            "process_2": 0.0008857363370570963
                        },
                        "gpu_energy": {
                            "process_3": 0.007882739639519798,
                            "process_1": 0.007869908518145508,
                            "process_0": 0.00783985904965867,
                            "process_2": 0.007864927403050004
                        },
                        "ram_energy": {
                            "process_3": 5.0067962272720924e-06,
                            "process_1": 4.968775947765608e-06,
                            "process_0": 4.697632845843465e-06,
                            "process_2": 4.915025350770203e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.008780134192188238,
                            "process_1": 0.008762045443784477,
                            "process_0": 0.008727646208127804,
                            "process_2": 0.008755578765457872
                        },
                        "total_energy_joules": {
                            "process_3": 31608.483091877657,
                            "process_1": 31543.36359762412,
                            "process_0": 31419.526349260093,
                            "process_2": 31520.08355564834
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 960.8035946025176,
                        "ram_power_avg": 0.7119780778884888,
                        "cpu_energy_total": 0.0035483817688127597,
                        "gpu_energy_total": 0.03145743461037398,
                        "ram_energy_total": 1.958823037165137e-05,
                        "total_energy_kwh": 0.03502540460955839,
                        "total_energy_joules": 126091.4565944102
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.1299374314685038,
                        "joules_per_token": 7.69601175502992,
                        "flops_per_joule": 417463512.0457284,
                        "joules_per_flop": 2.3954189315842804e-09
                    },
                    "per-process_emissions": [
                        0.003344792120514109,
                        0.003337901211809697,
                        0.003324796822986287,
                        0.0033354377307011768
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0441": {
            "setup": {
                "experiment_id": "0441",
                "cycle_id": 2,
                "date_time": "April 24, 2025 at 08:24:13 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_topp_0.8_temp_0.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_p",
                    "decoder_temperature": 0.0,
                    "decoder_top_k": null,
                    "decoder_top_p": 0.8
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.37911775294924,
                        "average_latency_ms_per_batch": 4672.389719118655,
                        "throughput_queries_per_sec": 3.424371887158859,
                        "throughput_tokens_per_sec": 438.31960155633396
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38325452800,
                        "gpu_max_memory_reserved_bytes": 38325452800
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.2,
                        "cpu_memory_usage_bytes": 1964998656
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0441",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_1": 465.9367812654876,
                            "process_2": 756.6887884361655,
                            "process_0": 844.3152382085627,
                            "process_3": 763.6879130110615
                        },
                        "ram_power": {
                            "process_1": 0.704132080078125,
                            "process_2": 0.7278256416320801,
                            "process_0": 0.6840834617614746,
                            "process_3": 0.7280373573303223
                        },
                        "cpu_energy": {
                            "process_1": 0.0013438454093502512,
                            "process_2": 0.0011454954039018046,
                            "process_0": 0.0011528213320561918,
                            "process_3": 0.0011633507688075043
                        },
                        "gpu_energy": {
                            "process_1": 0.01088040842654081,
                            "process_2": 0.010052188597297729,
                            "process_0": 0.01009773196706476,
                            "process_3": 0.010156006735908107
                        },
                        "ram_energy": {
                            "process_1": 7.357427731241552e-06,
                            "process_2": 6.3785643705183794e-06,
                            "process_0": 6.03759246115686e-06,
                            "process_3": 6.508887934109776e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.012231611263622304,
                            "process_2": 0.01120406256557005,
                            "process_0": 0.011256590891582114,
                            "process_3": 0.011325866392649722
                        },
                        "total_energy_joules": {
                            "process_1": 44033.800549040294,
                            "process_2": 40334.62523605218,
                            "process_0": 40523.72720969561,
                            "process_3": 40773.119013539
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 707.6571802303193,
                        "ram_power_avg": 0.7110196352005005,
                        "cpu_energy_total": 0.0048055129141157515,
                        "gpu_energy_total": 0.04118633572681141,
                        "ram_energy_total": 2.6282472497026568e-05,
                        "total_energy_kwh": 0.04601813111342419,
                        "total_energy_joules": 165665.27200832707
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09889821687659721,
                        "joules_per_token": 10.111405762226994,
                        "flops_per_joule": 317740596.2682279,
                        "joules_per_flop": 3.1472213867057375e-09
                    },
                    "per-process_emissions": [
                        0.004659632310876917,
                        0.00426818763435391,
                        0.004288198300148207,
                        0.004314588802279912
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0441": {
            "setup": {
                "experiment_id": "0441",
                "cycle_id": 2,
                "date_time": "April 24, 2025 at 08:24:13 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_topp_0.8_temp_0.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_p",
                    "decoder_temperature": 0.0,
                    "decoder_top_k": null,
                    "decoder_top_p": 0.8
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.37911775294924,
                        "average_latency_ms_per_batch": 4672.389719118655,
                        "throughput_queries_per_sec": 3.424371887158859,
                        "throughput_tokens_per_sec": 438.31960155633396
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38325452800,
                        "gpu_max_memory_reserved_bytes": 38325452800
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.2,
                        "cpu_memory_usage_bytes": 1964998656
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0441",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_1": 465.9367812654876,
                            "process_2": 756.6887884361655,
                            "process_0": 844.3152382085627,
                            "process_3": 763.6879130110615
                        },
                        "ram_power": {
                            "process_1": 0.704132080078125,
                            "process_2": 0.7278256416320801,
                            "process_0": 0.6840834617614746,
                            "process_3": 0.7280373573303223
                        },
                        "cpu_energy": {
                            "process_1": 0.0013438454093502512,
                            "process_2": 0.0011454954039018046,
                            "process_0": 0.0011528213320561918,
                            "process_3": 0.0011633507688075043
                        },
                        "gpu_energy": {
                            "process_1": 0.01088040842654081,
                            "process_2": 0.010052188597297729,
                            "process_0": 0.01009773196706476,
                            "process_3": 0.010156006735908107
                        },
                        "ram_energy": {
                            "process_1": 7.357427731241552e-06,
                            "process_2": 6.3785643705183794e-06,
                            "process_0": 6.03759246115686e-06,
                            "process_3": 6.508887934109776e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.012231611263622304,
                            "process_2": 0.01120406256557005,
                            "process_0": 0.011256590891582114,
                            "process_3": 0.011325866392649722
                        },
                        "total_energy_joules": {
                            "process_1": 44033.800549040294,
                            "process_2": 40334.62523605218,
                            "process_0": 40523.72720969561,
                            "process_3": 40773.119013539
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 707.6571802303193,
                        "ram_power_avg": 0.7110196352005005,
                        "cpu_energy_total": 0.0048055129141157515,
                        "gpu_energy_total": 0.04118633572681141,
                        "ram_energy_total": 2.6282472497026568e-05,
                        "total_energy_kwh": 0.04601813111342419,
                        "total_energy_joules": 165665.27200832707
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09889821687659721,
                        "joules_per_token": 10.111405762226994,
                        "flops_per_joule": 317740596.2682279,
                        "joules_per_flop": 3.1472213867057375e-09
                    },
                    "per-process_emissions": [
                        0.004659632310876917,
                        0.00426818763435391,
                        0.004288198300148207,
                        0.004314588802279912
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0442": {
            "setup": {
                "experiment_id": "0442",
                "cycle_id": 2,
                "date_time": "April 24, 2025 at 08:25:37 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_topp_0.3_temp_1.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_p",
                    "decoder_temperature": 1.2,
                    "decoder_top_k": null,
                    "decoder_top_p": 0.3
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.70822639792459,
                        "average_latency_ms_per_batch": 4838.528299740574,
                        "throughput_queries_per_sec": 3.306790620788116,
                        "throughput_tokens_per_sec": 423.26919946087884
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38356910080,
                        "gpu_max_memory_reserved_bytes": 38356910080
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            15.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 97.6,
                        "cpu_memory_usage_bytes": 1981517824
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0442",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 982.3981795740153,
                            "process_1": 0.0,
                            "process_2": 1008.9028912137774,
                            "process_3": 379.36431485067345
                        },
                        "ram_power": {
                            "process_0": 0.690556526184082,
                            "process_1": 0.7322244644165039,
                            "process_2": 0.7319169044494629,
                            "process_3": 0.7334933280944824
                        },
                        "cpu_energy": {
                            "process_0": 0.0012020710815031635,
                            "process_1": 0.0012113025319158625,
                            "process_2": 0.001198603204125902,
                            "process_3": 0.0012143848812866053
                        },
                        "gpu_energy": {
                            "process_0": 0.010355691895655994,
                            "process_1": 0.010408112493150767,
                            "process_2": 0.010339837438529287,
                            "process_3": 0.010421291392582077
                        },
                        "ram_energy": {
                            "process_0": 6.291545637389122e-06,
                            "process_1": 6.820094772136405e-06,
                            "process_2": 6.602208553218721e-06,
                            "process_3": 6.657949953800577e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.011564054522796546,
                            "process_1": 0.01162623511983877,
                            "process_2": 0.011545042851208406,
                            "process_3": 0.011642334223822482
                        },
                        "total_energy_joules": {
                            "process_0": 41630.59628206757,
                            "process_1": 41854.44643141957,
                            "process_2": 41562.15426435026,
                            "process_3": 41912.40320576094
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 592.6663464096165,
                        "ram_power_avg": 0.7220478057861328,
                        "cpu_energy_total": 0.004826361698831533,
                        "gpu_energy_total": 0.041524933219918125,
                        "ram_energy_total": 2.6371798916544823e-05,
                        "total_energy_kwh": 0.0463776667176662,
                        "total_energy_joules": 166959.60018359835
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09813152392544792,
                        "joules_per_token": 10.190405284643454,
                        "flops_per_joule": 315277362.0144011,
                        "joules_per_flop": 3.171810350133298e-09
                    },
                    "per-process_emissions": [
                        0.004405326570459344,
                        0.004429014268902579,
                        0.004398084074167842,
                        0.004435147222565175
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0442": {
            "setup": {
                "experiment_id": "0442",
                "cycle_id": 2,
                "date_time": "April 24, 2025 at 08:25:37 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_topp_0.3_temp_1.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_p",
                    "decoder_temperature": 1.2,
                    "decoder_top_k": null,
                    "decoder_top_p": 0.3
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.70822639792459,
                        "average_latency_ms_per_batch": 4838.528299740574,
                        "throughput_queries_per_sec": 3.306790620788116,
                        "throughput_tokens_per_sec": 423.26919946087884
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38356910080,
                        "gpu_max_memory_reserved_bytes": 38356910080
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            15.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 97.6,
                        "cpu_memory_usage_bytes": 1981517824
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0442",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 982.3981795740153,
                            "process_1": 0.0,
                            "process_2": 1008.9028912137774,
                            "process_3": 379.36431485067345
                        },
                        "ram_power": {
                            "process_0": 0.690556526184082,
                            "process_1": 0.7322244644165039,
                            "process_2": 0.7319169044494629,
                            "process_3": 0.7334933280944824
                        },
                        "cpu_energy": {
                            "process_0": 0.0012020710815031635,
                            "process_1": 0.0012113025319158625,
                            "process_2": 0.001198603204125902,
                            "process_3": 0.0012143848812866053
                        },
                        "gpu_energy": {
                            "process_0": 0.010355691895655994,
                            "process_1": 0.010408112493150767,
                            "process_2": 0.010339837438529287,
                            "process_3": 0.010421291392582077
                        },
                        "ram_energy": {
                            "process_0": 6.291545637389122e-06,
                            "process_1": 6.820094772136405e-06,
                            "process_2": 6.602208553218721e-06,
                            "process_3": 6.657949953800577e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.011564054522796546,
                            "process_1": 0.01162623511983877,
                            "process_2": 0.011545042851208406,
                            "process_3": 0.011642334223822482
                        },
                        "total_energy_joules": {
                            "process_0": 41630.59628206757,
                            "process_1": 41854.44643141957,
                            "process_2": 41562.15426435026,
                            "process_3": 41912.40320576094
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 592.6663464096165,
                        "ram_power_avg": 0.7220478057861328,
                        "cpu_energy_total": 0.004826361698831533,
                        "gpu_energy_total": 0.041524933219918125,
                        "ram_energy_total": 2.6371798916544823e-05,
                        "total_energy_kwh": 0.0463776667176662,
                        "total_energy_joules": 166959.60018359835
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09813152392544792,
                        "joules_per_token": 10.190405284643454,
                        "flops_per_joule": 315277362.0144011,
                        "joules_per_flop": 3.171810350133298e-09
                    },
                    "per-process_emissions": [
                        0.004405326570459344,
                        0.004429014268902579,
                        0.004398084074167842,
                        0.004435147222565175
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0443": {
            "setup": {
                "experiment_id": "0443",
                "cycle_id": 2,
                "date_time": "April 24, 2025 at 08:27:09 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_burst_0.2_0.4_2.0_10",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.2,
                    "delay_max": 0.4,
                    "simulate_burst": true,
                    "burst_interval": 2.0,
                    "burst_size": 10
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 41.22439895005664,
                        "average_latency_ms_per_batch": 5153.04986875708,
                        "throughput_queries_per_sec": 3.104957337402833,
                        "throughput_tokens_per_sec": 397.4345391875626
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38352715776,
                        "gpu_max_memory_reserved_bytes": 38352715776
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 97.1,
                        "cpu_memory_usage_bytes": 1970741248
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0443",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_1": 920.8564872831948,
                            "process_0": 421.1560966742196,
                            "process_2": 980.0408783651495,
                            "process_3": 943.3132156143765
                        },
                        "ram_power": {
                            "process_1": 0.7056527137756348,
                            "process_0": 0.6859374046325684,
                            "process_2": 0.7295322418212891,
                            "process_3": 0.7231378555297852
                        },
                        "cpu_energy": {
                            "process_1": 0.001271607110460536,
                            "process_0": 0.0012814457997865247,
                            "process_2": 0.0012689673760669393,
                            "process_3": 0.0012700235262509522
                        },
                        "gpu_energy": {
                            "process_1": 0.0103988244301636,
                            "process_0": 0.010435384459412944,
                            "process_2": 0.010388313032866137,
                            "process_3": 0.010399859708769554
                        },
                        "ram_energy": {
                            "process_1": 6.877239347259957e-06,
                            "process_0": 6.565381751538649e-06,
                            "process_2": 6.909595461339359e-06,
                            "process_3": 6.974412377223289e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.011677308779971399,
                            "process_0": 0.01172339564095101,
                            "process_2": 0.011664190004394415,
                            "process_3": 0.011676857647397728
                        },
                        "total_energy_joules": {
                            "process_1": 42038.311607897034,
                            "process_0": 42204.22430742364,
                            "process_2": 41991.08401581989,
                            "process_3": 42036.68753063182
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 816.3416694842351,
                        "ram_power_avg": 0.7110650539398193,
                        "cpu_energy_total": 0.005092043812564952,
                        "gpu_energy_total": 0.041622381631212235,
                        "ram_energy_total": 2.7326628937361252e-05,
                        "total_energy_kwh": 0.046741752072714554,
                        "total_energy_joules": 168270.30746177238
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09736714841221833,
                        "joules_per_token": 10.270404508164818,
                        "flops_per_joule": 312821573.23461497,
                        "joules_per_flop": 3.1967104751116507e-09
                    },
                    "per-process_emissions": [
                        0.004448470779730105,
                        0.004466027569420288,
                        0.004443473182174053,
                        0.004448298920776164
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0443": {
            "setup": {
                "experiment_id": "0443",
                "cycle_id": 2,
                "date_time": "April 24, 2025 at 08:27:09 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_burst_0.2_0.4_2.0_10",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.2,
                    "delay_max": 0.4,
                    "simulate_burst": true,
                    "burst_interval": 2.0,
                    "burst_size": 10
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 41.22439895005664,
                        "average_latency_ms_per_batch": 5153.04986875708,
                        "throughput_queries_per_sec": 3.104957337402833,
                        "throughput_tokens_per_sec": 397.4345391875626
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38352715776,
                        "gpu_max_memory_reserved_bytes": 38352715776
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 97.1,
                        "cpu_memory_usage_bytes": 1970741248
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0443",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_1": 920.8564872831948,
                            "process_0": 421.1560966742196,
                            "process_2": 980.0408783651495,
                            "process_3": 943.3132156143765
                        },
                        "ram_power": {
                            "process_1": 0.7056527137756348,
                            "process_0": 0.6859374046325684,
                            "process_2": 0.7295322418212891,
                            "process_3": 0.7231378555297852
                        },
                        "cpu_energy": {
                            "process_1": 0.001271607110460536,
                            "process_0": 0.0012814457997865247,
                            "process_2": 0.0012689673760669393,
                            "process_3": 0.0012700235262509522
                        },
                        "gpu_energy": {
                            "process_1": 0.0103988244301636,
                            "process_0": 0.010435384459412944,
                            "process_2": 0.010388313032866137,
                            "process_3": 0.010399859708769554
                        },
                        "ram_energy": {
                            "process_1": 6.877239347259957e-06,
                            "process_0": 6.565381751538649e-06,
                            "process_2": 6.909595461339359e-06,
                            "process_3": 6.974412377223289e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.011677308779971399,
                            "process_0": 0.01172339564095101,
                            "process_2": 0.011664190004394415,
                            "process_3": 0.011676857647397728
                        },
                        "total_energy_joules": {
                            "process_1": 42038.311607897034,
                            "process_0": 42204.22430742364,
                            "process_2": 41991.08401581989,
                            "process_3": 42036.68753063182
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 816.3416694842351,
                        "ram_power_avg": 0.7110650539398193,
                        "cpu_energy_total": 0.005092043812564952,
                        "gpu_energy_total": 0.041622381631212235,
                        "ram_energy_total": 2.7326628937361252e-05,
                        "total_energy_kwh": 0.046741752072714554,
                        "total_energy_joules": 168270.30746177238
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09736714841221833,
                        "joules_per_token": 10.270404508164818,
                        "flops_per_joule": 312821573.23461497,
                        "joules_per_flop": 3.1967104751116507e-09
                    },
                    "per-process_emissions": [
                        0.004448470779730105,
                        0.004466027569420288,
                        0.004443473182174053,
                        0.004448298920776164
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0444": {
            "setup": {
                "experiment_id": "0444",
                "cycle_id": 2,
                "date_time": "April 24, 2025 at 08:28:33 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_48",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 48,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 34.24742917000549,
                        "average_latency_ms_per_batch": 11415.809723335164,
                        "throughput_queries_per_sec": 3.7375068173614823,
                        "throughput_tokens_per_sec": 478.40087262226973
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 40082866176,
                        "gpu_max_memory_reserved_bytes": 40082866176
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            10.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.2,
                        "cpu_memory_usage_bytes": 1953624064
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0444",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_1": 722.1508373302636,
                            "process_2": 890.1927873117719,
                            "process_0": 1059.9566118863606,
                            "process_3": 743.8411638858247
                        },
                        "ram_power": {
                            "process_1": 0.7311887741088867,
                            "process_2": 0.7232837677001953,
                            "process_0": 0.6801910400390625,
                            "process_3": 0.7313246726989746
                        },
                        "cpu_energy": {
                            "process_1": 0.001046746596784942,
                            "process_2": 0.0010302057242697628,
                            "process_0": 0.001033509848935864,
                            "process_3": 0.0010514397710630872
                        },
                        "gpu_energy": {
                            "process_1": 0.007403877867540487,
                            "process_2": 0.0073270461394088215,
                            "process_0": 0.007362015611828809,
                            "process_3": 0.007402460088629326
                        },
                        "ram_energy": {
                            "process_1": 6.142313416743379e-06,
                            "process_2": 6.178765391588522e-06,
                            "process_0": 5.630378542798504e-06,
                            "process_3": 6.189024193957679e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.008456766777742174,
                            "process_2": 0.008363430629070173,
                            "process_0": 0.008401155839307473,
                            "process_3": 0.008460088883886371
                        },
                        "total_energy_joules": {
                            "process_1": 30444.360399871824,
                            "process_2": 30108.35026465262,
                            "process_0": 30244.1610215069,
                            "process_3": 30456.31998199094
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 854.0353501035553,
                        "ram_power_avg": 0.7164970636367798,
                        "cpu_energy_total": 0.004161901941053656,
                        "gpu_energy_total": 0.029495399707407444,
                        "ram_energy_total": 2.4140481545088082e-05,
                        "total_energy_kwh": 0.03368144213000619,
                        "total_energy_joules": 121253.19166802228
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.13512221636901373,
                        "joules_per_token": 7.400707499268939,
                        "flops_per_joule": 434121210.2109656,
                        "joules_per_flop": 2.30350412852217e-09
                    },
                    "per-process_emissions": [
                        0.003221605303980881,
                        0.0031860488981442824,
                        0.003200420316984182,
                        0.0032228708603165134
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0444": {
            "setup": {
                "experiment_id": "0444",
                "cycle_id": 2,
                "date_time": "April 24, 2025 at 08:28:33 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_48",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 48,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 34.24742917000549,
                        "average_latency_ms_per_batch": 11415.809723335164,
                        "throughput_queries_per_sec": 3.7375068173614823,
                        "throughput_tokens_per_sec": 478.40087262226973
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 40082866176,
                        "gpu_max_memory_reserved_bytes": 40082866176
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            10.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.2,
                        "cpu_memory_usage_bytes": 1953624064
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0444",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_1": 722.1508373302636,
                            "process_2": 890.1927873117719,
                            "process_0": 1059.9566118863606,
                            "process_3": 743.8411638858247
                        },
                        "ram_power": {
                            "process_1": 0.7311887741088867,
                            "process_2": 0.7232837677001953,
                            "process_0": 0.6801910400390625,
                            "process_3": 0.7313246726989746
                        },
                        "cpu_energy": {
                            "process_1": 0.001046746596784942,
                            "process_2": 0.0010302057242697628,
                            "process_0": 0.001033509848935864,
                            "process_3": 0.0010514397710630872
                        },
                        "gpu_energy": {
                            "process_1": 0.007403877867540487,
                            "process_2": 0.0073270461394088215,
                            "process_0": 0.007362015611828809,
                            "process_3": 0.007402460088629326
                        },
                        "ram_energy": {
                            "process_1": 6.142313416743379e-06,
                            "process_2": 6.178765391588522e-06,
                            "process_0": 5.630378542798504e-06,
                            "process_3": 6.189024193957679e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.008456766777742174,
                            "process_2": 0.008363430629070173,
                            "process_0": 0.008401155839307473,
                            "process_3": 0.008460088883886371
                        },
                        "total_energy_joules": {
                            "process_1": 30444.360399871824,
                            "process_2": 30108.35026465262,
                            "process_0": 30244.1610215069,
                            "process_3": 30456.31998199094
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 854.0353501035553,
                        "ram_power_avg": 0.7164970636367798,
                        "cpu_energy_total": 0.004161901941053656,
                        "gpu_energy_total": 0.029495399707407444,
                        "ram_energy_total": 2.4140481545088082e-05,
                        "total_energy_kwh": 0.03368144213000619,
                        "total_energy_joules": 121253.19166802228
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.13512221636901373,
                        "joules_per_token": 7.400707499268939,
                        "flops_per_joule": 434121210.2109656,
                        "joules_per_flop": 2.30350412852217e-09
                    },
                    "per-process_emissions": [
                        0.003221605303980881,
                        0.0031860488981442824,
                        0.003200420316984182,
                        0.0032228708603165134
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0445": {
            "setup": {
                "experiment_id": "0445",
                "cycle_id": 2,
                "date_time": "April 24, 2025 at 08:29:57 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_topp_0.3_temp_0.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_p",
                    "decoder_temperature": 0.2,
                    "decoder_top_k": null,
                    "decoder_top_p": 0.3
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.34678295691265,
                        "average_latency_ms_per_batch": 4793.347869614081,
                        "throughput_queries_per_sec": 3.3379592792392474,
                        "throughput_tokens_per_sec": 427.25878774262367
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38356910080,
                        "gpu_max_memory_reserved_bytes": 38356910080
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.3,
                        "cpu_memory_usage_bytes": 1981075456
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0445",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_1": 894.2005836381694,
                            "process_2": 1099.2168084551013,
                            "process_3": 737.0795171379177,
                            "process_0": 881.113361813548
                        },
                        "ram_power": {
                            "process_1": 0.7336535453796387,
                            "process_2": 0.7335891723632812,
                            "process_3": 0.7086224555969238,
                            "process_0": 0.690666675567627
                        },
                        "cpu_energy": {
                            "process_1": 0.0011857204712559903,
                            "process_2": 0.0011792084957196495,
                            "process_3": 0.0011897046547492207,
                            "process_0": 0.0011852346095674877
                        },
                        "gpu_energy": {
                            "process_1": 0.010323976036950455,
                            "process_2": 0.01029083906599837,
                            "process_3": 0.010343313274644217,
                            "process_0": 0.01031353436193072
                        },
                        "ram_energy": {
                            "process_1": 6.681039001602453e-06,
                            "process_2": 6.6490327326454365e-06,
                            "process_3": 6.490036145742545e-06,
                            "process_0": 6.291960743825012e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.011516377547208047,
                            "process_2": 0.011476696594450664,
                            "process_3": 0.011539507965539188,
                            "process_0": 0.011505060932242033
                        },
                        "total_energy_joules": {
                            "process_1": 41458.95916994897,
                            "process_2": 41316.10774002239,
                            "process_3": 41542.22867594108,
                            "process_0": 41418.21935607132
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 902.9025677611842,
                        "ram_power_avg": 0.7166329622268677,
                        "cpu_energy_total": 0.0047398682312923485,
                        "gpu_energy_total": 0.04127166273952376,
                        "ram_energy_total": 2.6112068623815445e-05,
                        "total_energy_kwh": 0.04603764303943993,
                        "total_energy_joules": 165735.51494198374
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09885630129266665,
                        "joules_per_token": 10.115693050658187,
                        "flops_per_joule": 317605929.7084895,
                        "joules_per_flop": 3.148555824879709e-09
                    },
                    "per-process_emissions": [
                        0.004387164026608906,
                        0.004372047567655981,
                        0.004395975559472153,
                        0.004382852962137603
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0445": {
            "setup": {
                "experiment_id": "0445",
                "cycle_id": 2,
                "date_time": "April 24, 2025 at 08:29:57 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_topp_0.3_temp_0.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_p",
                    "decoder_temperature": 0.2,
                    "decoder_top_k": null,
                    "decoder_top_p": 0.3
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.34678295691265,
                        "average_latency_ms_per_batch": 4793.347869614081,
                        "throughput_queries_per_sec": 3.3379592792392474,
                        "throughput_tokens_per_sec": 427.25878774262367
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38356910080,
                        "gpu_max_memory_reserved_bytes": 38356910080
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.3,
                        "cpu_memory_usage_bytes": 1981075456
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0445",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_1": 894.2005836381694,
                            "process_2": 1099.2168084551013,
                            "process_3": 737.0795171379177,
                            "process_0": 881.113361813548
                        },
                        "ram_power": {
                            "process_1": 0.7336535453796387,
                            "process_2": 0.7335891723632812,
                            "process_3": 0.7086224555969238,
                            "process_0": 0.690666675567627
                        },
                        "cpu_energy": {
                            "process_1": 0.0011857204712559903,
                            "process_2": 0.0011792084957196495,
                            "process_3": 0.0011897046547492207,
                            "process_0": 0.0011852346095674877
                        },
                        "gpu_energy": {
                            "process_1": 0.010323976036950455,
                            "process_2": 0.01029083906599837,
                            "process_3": 0.010343313274644217,
                            "process_0": 0.01031353436193072
                        },
                        "ram_energy": {
                            "process_1": 6.681039001602453e-06,
                            "process_2": 6.6490327326454365e-06,
                            "process_3": 6.490036145742545e-06,
                            "process_0": 6.291960743825012e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.011516377547208047,
                            "process_2": 0.011476696594450664,
                            "process_3": 0.011539507965539188,
                            "process_0": 0.011505060932242033
                        },
                        "total_energy_joules": {
                            "process_1": 41458.95916994897,
                            "process_2": 41316.10774002239,
                            "process_3": 41542.22867594108,
                            "process_0": 41418.21935607132
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 902.9025677611842,
                        "ram_power_avg": 0.7166329622268677,
                        "cpu_energy_total": 0.0047398682312923485,
                        "gpu_energy_total": 0.04127166273952376,
                        "ram_energy_total": 2.6112068623815445e-05,
                        "total_energy_kwh": 0.04603764303943993,
                        "total_energy_joules": 165735.51494198374
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09885630129266665,
                        "joules_per_token": 10.115693050658187,
                        "flops_per_joule": 317605929.7084895,
                        "joules_per_flop": 3.148555824879709e-09
                    },
                    "per-process_emissions": [
                        0.004387164026608906,
                        0.004372047567655981,
                        0.004395975559472153,
                        0.004382852962137603
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0446": {
            "setup": {
                "experiment_id": "0446",
                "cycle_id": 2,
                "date_time": "April 24, 2025 at 08:31:21 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_burst_0.1_0.2_2.0_20",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.1,
                    "delay_max": 0.2,
                    "simulate_burst": true,
                    "burst_interval": 2.0,
                    "burst_size": 20
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.66219717095373,
                        "average_latency_ms_per_batch": 4832.7746463692165,
                        "throughput_queries_per_sec": 3.3107275159251497,
                        "throughput_tokens_per_sec": 423.77312203841916
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38352715776,
                        "gpu_max_memory_reserved_bytes": 38352715776
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            27.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.2,
                        "cpu_memory_usage_bytes": 1970499584
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0446",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_3": 841.9859280828947,
                            "process_2": 1058.1477900629786,
                            "process_1": 1020.2355590268539,
                            "process_0": 969.7929008226617
                        },
                        "ram_power": {
                            "process_3": 0.7292361259460449,
                            "process_2": 0.723365306854248,
                            "process_1": 0.7229790687561035,
                            "process_0": 0.6858987808227539
                        },
                        "cpu_energy": {
                            "process_3": 0.0011983443735352922,
                            "process_2": 0.0011861803346528176,
                            "process_1": 0.0011920527101283369,
                            "process_0": 0.0011926677046503757
                        },
                        "gpu_energy": {
                            "process_3": 0.01024092347051031,
                            "process_2": 0.010167119522579426,
                            "process_1": 0.010213433726297794,
                            "process_0": 0.01021631261748901
                        },
                        "ram_energy": {
                            "process_3": 6.766845349561699e-06,
                            "process_2": 6.614432059609931e-06,
                            "process_1": 6.656495719343043e-06,
                            "process_0": 6.295012716590014e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.011446034689395162,
                            "process_2": 0.011359914289291851,
                            "process_1": 0.01141214293214547,
                            "process_0": 0.011415275334855972
                        },
                        "total_energy_joules": {
                            "process_3": 41205.72488182258,
                            "process_2": 40895.69144145067,
                            "process_1": 41083.714555723695,
                            "process_0": 41094.9912054815
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 972.5405444988473,
                        "ram_power_avg": 0.7153698205947876,
                        "cpu_energy_total": 0.004769245122966823,
                        "gpu_energy_total": 0.04083778933687654,
                        "ram_energy_total": 2.6332785845104686e-05,
                        "total_energy_kwh": 0.04563336724568846,
                        "total_energy_joules": 164280.12208447844
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09973209048125,
                        "joules_per_token": 10.026862920195217,
                        "flops_per_joule": 320419668.8008026,
                        "joules_per_flop": 3.120907039641429e-09
                    },
                    "per-process_emissions": [
                        0.004360366914925087,
                        0.004327559348505731,
                        0.004347455850000817,
                        0.004348649138813382
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0446": {
            "setup": {
                "experiment_id": "0446",
                "cycle_id": 2,
                "date_time": "April 24, 2025 at 08:31:21 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_burst_0.1_0.2_2.0_20",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.1,
                    "delay_max": 0.2,
                    "simulate_burst": true,
                    "burst_interval": 2.0,
                    "burst_size": 20
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.66219717095373,
                        "average_latency_ms_per_batch": 4832.7746463692165,
                        "throughput_queries_per_sec": 3.3107275159251497,
                        "throughput_tokens_per_sec": 423.77312203841916
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38352715776,
                        "gpu_max_memory_reserved_bytes": 38352715776
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            27.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.2,
                        "cpu_memory_usage_bytes": 1970499584
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0446",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_3": 841.9859280828947,
                            "process_2": 1058.1477900629786,
                            "process_1": 1020.2355590268539,
                            "process_0": 969.7929008226617
                        },
                        "ram_power": {
                            "process_3": 0.7292361259460449,
                            "process_2": 0.723365306854248,
                            "process_1": 0.7229790687561035,
                            "process_0": 0.6858987808227539
                        },
                        "cpu_energy": {
                            "process_3": 0.0011983443735352922,
                            "process_2": 0.0011861803346528176,
                            "process_1": 0.0011920527101283369,
                            "process_0": 0.0011926677046503757
                        },
                        "gpu_energy": {
                            "process_3": 0.01024092347051031,
                            "process_2": 0.010167119522579426,
                            "process_1": 0.010213433726297794,
                            "process_0": 0.01021631261748901
                        },
                        "ram_energy": {
                            "process_3": 6.766845349561699e-06,
                            "process_2": 6.614432059609931e-06,
                            "process_1": 6.656495719343043e-06,
                            "process_0": 6.295012716590014e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.011446034689395162,
                            "process_2": 0.011359914289291851,
                            "process_1": 0.01141214293214547,
                            "process_0": 0.011415275334855972
                        },
                        "total_energy_joules": {
                            "process_3": 41205.72488182258,
                            "process_2": 40895.69144145067,
                            "process_1": 41083.714555723695,
                            "process_0": 41094.9912054815
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 972.5405444988473,
                        "ram_power_avg": 0.7153698205947876,
                        "cpu_energy_total": 0.004769245122966823,
                        "gpu_energy_total": 0.04083778933687654,
                        "ram_energy_total": 2.6332785845104686e-05,
                        "total_energy_kwh": 0.04563336724568846,
                        "total_energy_joules": 164280.12208447844
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09973209048125,
                        "joules_per_token": 10.026862920195217,
                        "flops_per_joule": 320419668.8008026,
                        "joules_per_flop": 3.120907039641429e-09
                    },
                    "per-process_emissions": [
                        0.004360366914925087,
                        0.004327559348505731,
                        0.004347455850000817,
                        0.004348649138813382
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0447": {
            "setup": {
                "experiment_id": "0447",
                "cycle_id": 2,
                "date_time": "April 24, 2025 at 08:32:37 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_40",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 40,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 29.512334658997133,
                        "average_latency_ms_per_batch": 7378.083664749283,
                        "throughput_queries_per_sec": 4.337169576009058,
                        "throughput_tokens_per_sec": 555.1577057291594
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 40011563008,
                        "gpu_max_memory_reserved_bytes": 40011563008
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.4,
                        "cpu_memory_usage_bytes": 1972506624
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0447",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 927.260941844813,
                            "process_1": 782.5914002837001,
                            "process_2": 930.1305678071021,
                            "process_3": 886.6094521902353
                        },
                        "ram_power": {
                            "process_0": 0.6861763000488281,
                            "process_1": 0.717282772064209,
                            "process_2": 0.7169251441955566,
                            "process_3": 0.7311301231384277
                        },
                        "cpu_energy": {
                            "process_0": 0.0009093609752872,
                            "process_1": 0.0009167230312741594,
                            "process_2": 0.0009131400705045961,
                            "process_3": 0.0009150050917105545
                        },
                        "gpu_energy": {
                            "process_0": 0.007970290820670911,
                            "process_1": 0.008006584183037901,
                            "process_2": 0.007980216384167349,
                            "process_3": 0.007995155007225918
                        },
                        "ram_energy": {
                            "process_0": 4.9661408647439336e-06,
                            "process_1": 5.250298041180937e-06,
                            "process_2": 5.214464362096109e-06,
                            "process_3": 5.326990746885538e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.008884617936822858,
                            "process_1": 0.008928557512353243,
                            "process_2": 0.008898570919034041,
                            "process_3": 0.00891548708968336
                        },
                        "total_energy_joules": {
                            "process_0": 31984.62457256229,
                            "process_1": 32142.807044471676,
                            "process_2": 32034.85530852255,
                            "process_3": 32095.753522860094
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 881.6480905314627,
                        "ram_power_avg": 0.7128785848617554,
                        "cpu_energy_total": 0.00365422916877651,
                        "gpu_energy_total": 0.03195224639510208,
                        "ram_energy_total": 2.075789401490652e-05,
                        "total_energy_kwh": 0.0356272334578935,
                        "total_energy_joules": 128258.0404484166
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.1277424786993326,
                        "joules_per_token": 7.8282495390879285,
                        "flops_per_joule": 410411558.79840857,
                        "joules_per_flop": 2.4365785479526257e-09
                    },
                    "per-process_emissions": [
                        0.0033845952030326677,
                        0.003401333984330968,
                        0.003389910591606018,
                        0.003396354806814876
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0447": {
            "setup": {
                "experiment_id": "0447",
                "cycle_id": 2,
                "date_time": "April 24, 2025 at 08:32:37 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_40",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 40,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 29.512334658997133,
                        "average_latency_ms_per_batch": 7378.083664749283,
                        "throughput_queries_per_sec": 4.337169576009058,
                        "throughput_tokens_per_sec": 555.1577057291594
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 40011563008,
                        "gpu_max_memory_reserved_bytes": 40011563008
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.4,
                        "cpu_memory_usage_bytes": 1972506624
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0447",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 927.260941844813,
                            "process_1": 782.5914002837001,
                            "process_2": 930.1305678071021,
                            "process_3": 886.6094521902353
                        },
                        "ram_power": {
                            "process_0": 0.6861763000488281,
                            "process_1": 0.717282772064209,
                            "process_2": 0.7169251441955566,
                            "process_3": 0.7311301231384277
                        },
                        "cpu_energy": {
                            "process_0": 0.0009093609752872,
                            "process_1": 0.0009167230312741594,
                            "process_2": 0.0009131400705045961,
                            "process_3": 0.0009150050917105545
                        },
                        "gpu_energy": {
                            "process_0": 0.007970290820670911,
                            "process_1": 0.008006584183037901,
                            "process_2": 0.007980216384167349,
                            "process_3": 0.007995155007225918
                        },
                        "ram_energy": {
                            "process_0": 4.9661408647439336e-06,
                            "process_1": 5.250298041180937e-06,
                            "process_2": 5.214464362096109e-06,
                            "process_3": 5.326990746885538e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.008884617936822858,
                            "process_1": 0.008928557512353243,
                            "process_2": 0.008898570919034041,
                            "process_3": 0.00891548708968336
                        },
                        "total_energy_joules": {
                            "process_0": 31984.62457256229,
                            "process_1": 32142.807044471676,
                            "process_2": 32034.85530852255,
                            "process_3": 32095.753522860094
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 881.6480905314627,
                        "ram_power_avg": 0.7128785848617554,
                        "cpu_energy_total": 0.00365422916877651,
                        "gpu_energy_total": 0.03195224639510208,
                        "ram_energy_total": 2.075789401490652e-05,
                        "total_energy_kwh": 0.0356272334578935,
                        "total_energy_joules": 128258.0404484166
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.1277424786993326,
                        "joules_per_token": 7.8282495390879285,
                        "flops_per_joule": 410411558.79840857,
                        "joules_per_flop": 2.4365785479526257e-09
                    },
                    "per-process_emissions": [
                        0.0033845952030326677,
                        0.003401333984330968,
                        0.003389910591606018,
                        0.003396354806814876
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0448": {
            "setup": {
                "experiment_id": "0448",
                "cycle_id": 2,
                "date_time": "April 24, 2025 at 08:34:06 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_greedy_temp_0.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "greedy",
                    "decoder_temperature": 0.2,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.380918089009356,
                        "average_latency_ms_per_batch": 4797.6147611261695,
                        "throughput_queries_per_sec": 3.334990572741242,
                        "throughput_tokens_per_sec": 426.87879331087896
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38405144576,
                        "gpu_max_memory_reserved_bytes": 38405144576
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            28.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 97.3,
                        "cpu_memory_usage_bytes": 1951846400
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0448",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_2": 959.0583971227724,
                            "process_0": 959.8009597148528,
                            "process_3": 872.2973714280702,
                            "process_1": 871.7907600401252
                        },
                        "ram_power": {
                            "process_2": 0.7226414680480957,
                            "process_0": 0.6796460151672364,
                            "process_3": 0.7222180366516113,
                            "process_1": 0.7237644195556641
                        },
                        "cpu_energy": {
                            "process_2": 0.0012003897517843145,
                            "process_0": 0.0011929723241282776,
                            "process_3": 0.001204819907596175,
                            "process_1": 0.0012048494997816306
                        },
                        "gpu_energy": {
                            "process_2": 0.010257481539310831,
                            "process_0": 0.01020014343788489,
                            "process_3": 0.010279701557088572,
                            "process_1": 0.010279701557088572
                        },
                        "ram_energy": {
                            "process_2": 6.459919380515596e-06,
                            "process_0": 6.048355200273811e-06,
                            "process_3": 6.455555417539073e-06,
                            "process_1": 6.4486534426971675e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.011464331210475659,
                            "process_0": 0.01139916411721344,
                            "process_3": 0.011490977020102284,
                            "process_1": 0.011490999710312899
                        },
                        "total_energy_joules": {
                            "process_2": 41271.592357712376,
                            "process_0": 41036.990821968386,
                            "process_3": 41367.51727236822,
                            "process_1": 41367.598957126436
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 915.736872076455,
                        "ram_power_avg": 0.7120674848556519,
                        "cpu_energy_total": 0.004803031483290398,
                        "gpu_energy_total": 0.041017028091372865,
                        "ram_energy_total": 2.541248344102565e-05,
                        "total_energy_kwh": 0.045845472058104283,
                        "total_energy_joules": 165043.69940917543
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09927067836367917,
                        "joules_per_token": 10.073467981517055,
                        "flops_per_joule": 318937242.05952704,
                        "joules_per_flop": 3.135413078581015e-09
                    },
                    "per-process_emissions": [
                        0.004367336974630702,
                        0.00434251157045246,
                        0.004377487695807965,
                        0.004377496339643699
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0448": {
            "setup": {
                "experiment_id": "0448",
                "cycle_id": 2,
                "date_time": "April 24, 2025 at 08:34:06 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_greedy_temp_0.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "greedy",
                    "decoder_temperature": 0.2,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.380918089009356,
                        "average_latency_ms_per_batch": 4797.6147611261695,
                        "throughput_queries_per_sec": 3.334990572741242,
                        "throughput_tokens_per_sec": 426.87879331087896
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38405144576,
                        "gpu_max_memory_reserved_bytes": 38405144576
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            28.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 97.3,
                        "cpu_memory_usage_bytes": 1951846400
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0448",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_2": 959.0583971227724,
                            "process_0": 959.8009597148528,
                            "process_3": 872.2973714280702,
                            "process_1": 871.7907600401252
                        },
                        "ram_power": {
                            "process_2": 0.7226414680480957,
                            "process_0": 0.6796460151672364,
                            "process_3": 0.7222180366516113,
                            "process_1": 0.7237644195556641
                        },
                        "cpu_energy": {
                            "process_2": 0.0012003897517843145,
                            "process_0": 0.0011929723241282776,
                            "process_3": 0.001204819907596175,
                            "process_1": 0.0012048494997816306
                        },
                        "gpu_energy": {
                            "process_2": 0.010257481539310831,
                            "process_0": 0.01020014343788489,
                            "process_3": 0.010279701557088572,
                            "process_1": 0.010279701557088572
                        },
                        "ram_energy": {
                            "process_2": 6.459919380515596e-06,
                            "process_0": 6.048355200273811e-06,
                            "process_3": 6.455555417539073e-06,
                            "process_1": 6.4486534426971675e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.011464331210475659,
                            "process_0": 0.01139916411721344,
                            "process_3": 0.011490977020102284,
                            "process_1": 0.011490999710312899
                        },
                        "total_energy_joules": {
                            "process_2": 41271.592357712376,
                            "process_0": 41036.990821968386,
                            "process_3": 41367.51727236822,
                            "process_1": 41367.598957126436
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 915.736872076455,
                        "ram_power_avg": 0.7120674848556519,
                        "cpu_energy_total": 0.004803031483290398,
                        "gpu_energy_total": 0.041017028091372865,
                        "ram_energy_total": 2.541248344102565e-05,
                        "total_energy_kwh": 0.045845472058104283,
                        "total_energy_joules": 165043.69940917543
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09927067836367917,
                        "joules_per_token": 10.073467981517055,
                        "flops_per_joule": 318937242.05952704,
                        "joules_per_flop": 3.135413078581015e-09
                    },
                    "per-process_emissions": [
                        0.004367336974630702,
                        0.00434251157045246,
                        0.004377487695807965,
                        0.004377496339643699
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0449": {
            "setup": {
                "experiment_id": "0449",
                "cycle_id": 2,
                "date_time": "April 24, 2025 at 08:35:38 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_400_temp_0.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 0.2,
                    "decoder_top_k": 400,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.64448536001146,
                        "average_latency_ms_per_batch": 4830.560670001432,
                        "throughput_queries_per_sec": 3.312244911726832,
                        "throughput_tokens_per_sec": 423.9673487010345
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38449184768,
                        "gpu_max_memory_reserved_bytes": 38449184768
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            27.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 96.6,
                        "cpu_memory_usage_bytes": 1956904960
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0449",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 815.025146376252,
                            "process_1": 1007.3457562716754,
                            "process_2": 921.669122323797,
                            "process_3": 805.0107531076667
                        },
                        "ram_power": {
                            "process_0": 0.6809420585632324,
                            "process_1": 0.7313647270202638,
                            "process_2": 0.7323102951049805,
                            "process_3": 0.7249488830566406
                        },
                        "cpu_energy": {
                            "process_0": 0.0011704324983729747,
                            "process_1": 0.0011789677400956865,
                            "process_2": 0.0011860747662176434,
                            "process_3": 0.0012281694941939348
                        },
                        "gpu_energy": {
                            "process_0": 0.010280047112921409,
                            "process_1": 0.010218082618903779,
                            "process_2": 0.010270155438341533,
                            "process_3": 0.010305777689060491
                        },
                        "ram_energy": {
                            "process_0": 5.85777798873615e-06,
                            "process_1": 6.325616440323842e-06,
                            "process_2": 6.339326341060476e-06,
                            "process_3": 6.4792017253788295e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.011456337389283118,
                            "process_1": 0.011403375975439787,
                            "process_2": 0.01146256953090024,
                            "process_3": 0.011540426384979803
                        },
                        "total_energy_joules": {
                            "process_0": 41242.814601419224,
                            "process_1": 41052.15351158323,
                            "process_2": 41265.25031124086,
                            "process_3": 41545.53498592729
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 887.2626945198476,
                        "ram_power_avg": 0.7173914909362793,
                        "cpu_energy_total": 0.00476364449888024,
                        "gpu_energy_total": 0.04107406285922721,
                        "ram_energy_total": 2.5001922495499297e-05,
                        "total_energy_kwh": 0.045862709280602945,
                        "total_energy_joules": 165105.7534101706
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09923336807832123,
                        "joules_per_token": 10.077255457163734,
                        "flops_per_joule": 318817371.42190605,
                        "joules_per_flop": 3.1365919477350335e-09
                    },
                    "per-process_emissions": [
                        0.004364291728447404,
                        0.004344116077843786,
                        0.004366665862796446,
                        0.004396325431358056
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0449": {
            "setup": {
                "experiment_id": "0449",
                "cycle_id": 2,
                "date_time": "April 24, 2025 at 08:35:38 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_400_temp_0.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 0.2,
                    "decoder_top_k": 400,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.64448536001146,
                        "average_latency_ms_per_batch": 4830.560670001432,
                        "throughput_queries_per_sec": 3.312244911726832,
                        "throughput_tokens_per_sec": 423.9673487010345
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38449184768,
                        "gpu_max_memory_reserved_bytes": 38449184768
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            27.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 96.6,
                        "cpu_memory_usage_bytes": 1956904960
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0449",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 815.025146376252,
                            "process_1": 1007.3457562716754,
                            "process_2": 921.669122323797,
                            "process_3": 805.0107531076667
                        },
                        "ram_power": {
                            "process_0": 0.6809420585632324,
                            "process_1": 0.7313647270202638,
                            "process_2": 0.7323102951049805,
                            "process_3": 0.7249488830566406
                        },
                        "cpu_energy": {
                            "process_0": 0.0011704324983729747,
                            "process_1": 0.0011789677400956865,
                            "process_2": 0.0011860747662176434,
                            "process_3": 0.0012281694941939348
                        },
                        "gpu_energy": {
                            "process_0": 0.010280047112921409,
                            "process_1": 0.010218082618903779,
                            "process_2": 0.010270155438341533,
                            "process_3": 0.010305777689060491
                        },
                        "ram_energy": {
                            "process_0": 5.85777798873615e-06,
                            "process_1": 6.325616440323842e-06,
                            "process_2": 6.339326341060476e-06,
                            "process_3": 6.4792017253788295e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.011456337389283118,
                            "process_1": 0.011403375975439787,
                            "process_2": 0.01146256953090024,
                            "process_3": 0.011540426384979803
                        },
                        "total_energy_joules": {
                            "process_0": 41242.814601419224,
                            "process_1": 41052.15351158323,
                            "process_2": 41265.25031124086,
                            "process_3": 41545.53498592729
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 887.2626945198476,
                        "ram_power_avg": 0.7173914909362793,
                        "cpu_energy_total": 0.00476364449888024,
                        "gpu_energy_total": 0.04107406285922721,
                        "ram_energy_total": 2.5001922495499297e-05,
                        "total_energy_kwh": 0.045862709280602945,
                        "total_energy_joules": 165105.7534101706
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09923336807832123,
                        "joules_per_token": 10.077255457163734,
                        "flops_per_joule": 318817371.42190605,
                        "joules_per_flop": 3.1365919477350335e-09
                    },
                    "per-process_emissions": [
                        0.004364291728447404,
                        0.004344116077843786,
                        0.004366665862796446,
                        0.004396325431358056
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0450": {
            "setup": {
                "experiment_id": "0450",
                "cycle_id": 2,
                "date_time": "April 24, 2025 at 08:37:05 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_20",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 20,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 35.256674187025055,
                        "average_latency_ms_per_batch": 5036.667741003579,
                        "throughput_queries_per_sec": 3.6305182763695214,
                        "throughput_tokens_per_sec": 464.70633937529874
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38738591744,
                        "gpu_max_memory_reserved_bytes": 38738591744
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            28.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 6.1,
                        "cpu_memory_usage_bytes": 1973239808
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0450",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_0": 840.5244656160396,
                            "process_3": 415.88692341337986,
                            "process_1": 845.3488341524769,
                            "process_2": 0.0
                        },
                        "ram_power": {
                            "process_0": 0.6877570152282716,
                            "process_3": 0.7076826095581055,
                            "process_1": 0.7288212776184082,
                            "process_2": 0.7215728759765625
                        },
                        "cpu_energy": {
                            "process_0": 0.001096727138561619,
                            "process_3": 0.001128766296817048,
                            "process_1": 0.0010913056744648202,
                            "process_2": 0.0011166929275605066
                        },
                        "gpu_energy": {
                            "process_0": 0.009348470534325415,
                            "process_3": 0.009491960649116393,
                            "process_1": 0.009316081897303263,
                            "process_2": 0.009441592275491573
                        },
                        "ram_energy": {
                            "process_0": 5.433472069760874e-06,
                            "process_3": 5.791454485700779e-06,
                            "process_1": 5.791939325761943e-06,
                            "process_2": 6.0730717450634485e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.010450631144956795,
                            "process_3": 0.01062651840041914,
                            "process_1": 0.01041317951109385,
                            "process_2": 0.010564358274797138
                        },
                        "total_energy_joules": {
                            "process_0": 37622.272121844464,
                            "process_3": 38255.466241508904,
                            "process_1": 37487.44623993786,
                            "process_2": 38031.689789269694
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 525.4400557954741,
                        "ram_power_avg": 0.7114584445953369,
                        "cpu_energy_total": 0.004433492037403994,
                        "gpu_energy_total": 0.037598105356236644,
                        "ram_energy_total": 2.3089937626287046e-05,
                        "total_energy_kwh": 0.042054687331266924,
                        "total_energy_joules": 151396.87439256092
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.10821887879612031,
                        "joules_per_token": 9.24053188431158,
                        "flops_per_joule": 347686057.06070286,
                        "joules_per_flop": 2.8761579007622067e-09
                    },
                    "per-process_emissions": [
                        0.003981167934671291,
                        0.004048172184639672,
                        0.003966900734751202,
                        0.00402449228478397
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0450": {
            "setup": {
                "experiment_id": "0450",
                "cycle_id": 2,
                "date_time": "April 24, 2025 at 08:37:05 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_20",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 20,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 35.256674187025055,
                        "average_latency_ms_per_batch": 5036.667741003579,
                        "throughput_queries_per_sec": 3.6305182763695214,
                        "throughput_tokens_per_sec": 464.70633937529874
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38738591744,
                        "gpu_max_memory_reserved_bytes": 38738591744
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            28.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 6.1,
                        "cpu_memory_usage_bytes": 1973239808
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0450",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_0": 840.5244656160396,
                            "process_3": 415.88692341337986,
                            "process_1": 845.3488341524769,
                            "process_2": 0.0
                        },
                        "ram_power": {
                            "process_0": 0.6877570152282716,
                            "process_3": 0.7076826095581055,
                            "process_1": 0.7288212776184082,
                            "process_2": 0.7215728759765625
                        },
                        "cpu_energy": {
                            "process_0": 0.001096727138561619,
                            "process_3": 0.001128766296817048,
                            "process_1": 0.0010913056744648202,
                            "process_2": 0.0011166929275605066
                        },
                        "gpu_energy": {
                            "process_0": 0.009348470534325415,
                            "process_3": 0.009491960649116393,
                            "process_1": 0.009316081897303263,
                            "process_2": 0.009441592275491573
                        },
                        "ram_energy": {
                            "process_0": 5.433472069760874e-06,
                            "process_3": 5.791454485700779e-06,
                            "process_1": 5.791939325761943e-06,
                            "process_2": 6.0730717450634485e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.010450631144956795,
                            "process_3": 0.01062651840041914,
                            "process_1": 0.01041317951109385,
                            "process_2": 0.010564358274797138
                        },
                        "total_energy_joules": {
                            "process_0": 37622.272121844464,
                            "process_3": 38255.466241508904,
                            "process_1": 37487.44623993786,
                            "process_2": 38031.689789269694
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 525.4400557954741,
                        "ram_power_avg": 0.7114584445953369,
                        "cpu_energy_total": 0.004433492037403994,
                        "gpu_energy_total": 0.037598105356236644,
                        "ram_energy_total": 2.3089937626287046e-05,
                        "total_energy_kwh": 0.042054687331266924,
                        "total_energy_joules": 151396.87439256092
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.10821887879612031,
                        "joules_per_token": 9.24053188431158,
                        "flops_per_joule": 347686057.06070286,
                        "joules_per_flop": 2.8761579007622067e-09
                    },
                    "per-process_emissions": [
                        0.003981167934671291,
                        0.004048172184639672,
                        0.003966900734751202,
                        0.00402449228478397
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0451": {
            "setup": {
                "experiment_id": "0451",
                "cycle_id": 2,
                "date_time": "April 24, 2025 at 08:38:33 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_burst_0.05_0.1_4.0_10",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.05,
                    "delay_max": 0.1,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 10
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 39.27384582901141,
                        "average_latency_ms_per_batch": 4909.230728626426,
                        "throughput_queries_per_sec": 3.2591664324721417,
                        "throughput_tokens_per_sec": 417.17330335643413
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38352715776,
                        "gpu_max_memory_reserved_bytes": 38352715776
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 96.4,
                        "cpu_memory_usage_bytes": 1970257920
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0451",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_2": 917.8196220064422,
                            "process_3": 813.4420888895544,
                            "process_1": 1009.8507043886071,
                            "process_0": 1084.1768911961647
                        },
                        "ram_power": {
                            "process_2": 0.7063493728637695,
                            "process_3": 0.7227702140808105,
                            "process_1": 0.7228231430053712,
                            "process_0": 0.6856327056884766
                        },
                        "cpu_energy": {
                            "process_2": 0.0012252050738989057,
                            "process_3": 0.0012272937792804444,
                            "process_1": 0.0012237095551863603,
                            "process_0": 0.0012204622637163993
                        },
                        "gpu_energy": {
                            "process_2": 0.010334679378845024,
                            "process_3": 0.010344784664708229,
                            "process_1": 0.010319017977430178,
                            "process_0": 0.010319017977430178
                        },
                        "ram_energy": {
                            "process_2": 6.2578507215962715e-06,
                            "process_3": 6.464585858291477e-06,
                            "process_1": 6.4212401342139e-06,
                            "process_0": 6.062561922339515e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.011566142303465525,
                            "process_3": 0.011578543029846958,
                            "process_1": 0.011549148772750753,
                            "process_0": 0.01154554280306892
                        },
                        "total_energy_joules": {
                            "process_2": 41638.11229247589,
                            "process_3": 41682.75490744905,
                            "process_1": 41576.93558190271,
                            "process_0": 41563.95409104811
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 956.3223266201921,
                        "ram_power_avg": 0.7093938589096069,
                        "cpu_energy_total": 0.004896670672082109,
                        "gpu_energy_total": 0.04131749999841361,
                        "ram_energy_total": 2.5206238636441163e-05,
                        "total_energy_kwh": 0.04623937690913216,
                        "total_energy_joules": 166461.75687287576
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09842500949039126,
                        "joules_per_token": 10.160019340385483,
                        "flops_per_joule": 316220273.6395679,
                        "joules_per_flop": 3.1623525857162886e-09
                    },
                    "per-process_emissions": [
                        0.004406121910505192,
                        0.004410845967220199,
                        0.0043996482249794,
                        0.004398274530829105
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0451": {
            "setup": {
                "experiment_id": "0451",
                "cycle_id": 2,
                "date_time": "April 24, 2025 at 08:38:33 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_burst_0.05_0.1_4.0_10",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.05,
                    "delay_max": 0.1,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 10
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 39.27384582901141,
                        "average_latency_ms_per_batch": 4909.230728626426,
                        "throughput_queries_per_sec": 3.2591664324721417,
                        "throughput_tokens_per_sec": 417.17330335643413
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38352715776,
                        "gpu_max_memory_reserved_bytes": 38352715776
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 96.4,
                        "cpu_memory_usage_bytes": 1970257920
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0451",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_2": 917.8196220064422,
                            "process_3": 813.4420888895544,
                            "process_1": 1009.8507043886071,
                            "process_0": 1084.1768911961647
                        },
                        "ram_power": {
                            "process_2": 0.7063493728637695,
                            "process_3": 0.7227702140808105,
                            "process_1": 0.7228231430053712,
                            "process_0": 0.6856327056884766
                        },
                        "cpu_energy": {
                            "process_2": 0.0012252050738989057,
                            "process_3": 0.0012272937792804444,
                            "process_1": 0.0012237095551863603,
                            "process_0": 0.0012204622637163993
                        },
                        "gpu_energy": {
                            "process_2": 0.010334679378845024,
                            "process_3": 0.010344784664708229,
                            "process_1": 0.010319017977430178,
                            "process_0": 0.010319017977430178
                        },
                        "ram_energy": {
                            "process_2": 6.2578507215962715e-06,
                            "process_3": 6.464585858291477e-06,
                            "process_1": 6.4212401342139e-06,
                            "process_0": 6.062561922339515e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.011566142303465525,
                            "process_3": 0.011578543029846958,
                            "process_1": 0.011549148772750753,
                            "process_0": 0.01154554280306892
                        },
                        "total_energy_joules": {
                            "process_2": 41638.11229247589,
                            "process_3": 41682.75490744905,
                            "process_1": 41576.93558190271,
                            "process_0": 41563.95409104811
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 956.3223266201921,
                        "ram_power_avg": 0.7093938589096069,
                        "cpu_energy_total": 0.004896670672082109,
                        "gpu_energy_total": 0.04131749999841361,
                        "ram_energy_total": 2.5206238636441163e-05,
                        "total_energy_kwh": 0.04623937690913216,
                        "total_energy_joules": 166461.75687287576
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09842500949039126,
                        "joules_per_token": 10.160019340385483,
                        "flops_per_joule": 316220273.6395679,
                        "joules_per_flop": 3.1623525857162886e-09
                    },
                    "per-process_emissions": [
                        0.004406121910505192,
                        0.004410845967220199,
                        0.0043996482249794,
                        0.004398274530829105
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0452": {
            "setup": {
                "experiment_id": "0452",
                "cycle_id": 2,
                "date_time": "April 24, 2025 at 08:40:08 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_burst_0.2_0.4_2.0_8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.2,
                    "delay_max": 0.4,
                    "simulate_burst": true,
                    "burst_interval": 2.0,
                    "burst_size": 8
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 43.653086053964216,
                        "average_latency_ms_per_batch": 5456.635756745527,
                        "throughput_queries_per_sec": 2.932209645883125,
                        "throughput_tokens_per_sec": 375.32283467304
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38352715776,
                        "gpu_max_memory_reserved_bytes": 38352715776
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 7.0,
                        "cpu_memory_usage_bytes": 1973989376
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0452",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_2": 889.2822776405027,
                            "process_1": 998.078361394994,
                            "process_3": 1071.7097093419839,
                            "process_0": 819.4331819027603
                        },
                        "ram_power": {
                            "process_2": 0.7292690277099609,
                            "process_1": 0.7295522689819336,
                            "process_3": 0.7075996398925781,
                            "process_0": 0.6879715919494629
                        },
                        "cpu_energy": {
                            "process_2": 0.0013494586821925622,
                            "process_1": 0.001347356245347328,
                            "process_3": 0.0013446023591313855,
                            "process_0": 0.0013539056475565306
                        },
                        "gpu_energy": {
                            "process_2": 0.010553966498720868,
                            "process_1": 0.01053993176527257,
                            "process_3": 0.010527886200080161,
                            "process_0": 0.010565808452639658
                        },
                        "ram_energy": {
                            "process_2": 7.284713958038828e-06,
                            "process_1": 7.339120480432214e-06,
                            "process_3": 7.010329050965063e-06,
                            "process_0": 6.8931561028301945e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.01191070989487147,
                            "process_1": 0.01189462713110033,
                            "process_3": 0.011879498888262511,
                            "process_0": 0.01192660725629902
                        },
                        "total_energy_joules": {
                            "process_2": 42878.55562153729,
                            "process_1": 42820.657671961184,
                            "process_3": 42766.19599774504,
                            "process_0": 42935.78612267647
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 944.6258825700602,
                        "ram_power_avg": 0.7135981321334839,
                        "cpu_energy_total": 0.005395322934227806,
                        "gpu_energy_total": 0.04218759291671326,
                        "ram_energy_total": 2.85273195922663e-05,
                        "total_energy_kwh": 0.047611443170533334,
                        "total_energy_joules": 171401.19541391998
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09558859820337874,
                        "joules_per_token": 10.46149874352539,
                        "flops_per_joule": 307107439.83871347,
                        "joules_per_flop": 3.256189431702402e-09
                    },
                    "per-process_emissions": [
                        0.0045373849344512865,
                        0.004531258205592671,
                        0.004525495101483604,
                        0.004543441034287112
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0452": {
            "setup": {
                "experiment_id": "0452",
                "cycle_id": 2,
                "date_time": "April 24, 2025 at 08:40:08 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_burst_0.2_0.4_2.0_8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.2,
                    "delay_max": 0.4,
                    "simulate_burst": true,
                    "burst_interval": 2.0,
                    "burst_size": 8
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 43.653086053964216,
                        "average_latency_ms_per_batch": 5456.635756745527,
                        "throughput_queries_per_sec": 2.932209645883125,
                        "throughput_tokens_per_sec": 375.32283467304
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38352715776,
                        "gpu_max_memory_reserved_bytes": 38352715776
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 7.0,
                        "cpu_memory_usage_bytes": 1973989376
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0452",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_2": 889.2822776405027,
                            "process_1": 998.078361394994,
                            "process_3": 1071.7097093419839,
                            "process_0": 819.4331819027603
                        },
                        "ram_power": {
                            "process_2": 0.7292690277099609,
                            "process_1": 0.7295522689819336,
                            "process_3": 0.7075996398925781,
                            "process_0": 0.6879715919494629
                        },
                        "cpu_energy": {
                            "process_2": 0.0013494586821925622,
                            "process_1": 0.001347356245347328,
                            "process_3": 0.0013446023591313855,
                            "process_0": 0.0013539056475565306
                        },
                        "gpu_energy": {
                            "process_2": 0.010553966498720868,
                            "process_1": 0.01053993176527257,
                            "process_3": 0.010527886200080161,
                            "process_0": 0.010565808452639658
                        },
                        "ram_energy": {
                            "process_2": 7.284713958038828e-06,
                            "process_1": 7.339120480432214e-06,
                            "process_3": 7.010329050965063e-06,
                            "process_0": 6.8931561028301945e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.01191070989487147,
                            "process_1": 0.01189462713110033,
                            "process_3": 0.011879498888262511,
                            "process_0": 0.01192660725629902
                        },
                        "total_energy_joules": {
                            "process_2": 42878.55562153729,
                            "process_1": 42820.657671961184,
                            "process_3": 42766.19599774504,
                            "process_0": 42935.78612267647
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 944.6258825700602,
                        "ram_power_avg": 0.7135981321334839,
                        "cpu_energy_total": 0.005395322934227806,
                        "gpu_energy_total": 0.04218759291671326,
                        "ram_energy_total": 2.85273195922663e-05,
                        "total_energy_kwh": 0.047611443170533334,
                        "total_energy_joules": 171401.19541391998
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09558859820337874,
                        "joules_per_token": 10.46149874352539,
                        "flops_per_joule": 307107439.83871347,
                        "joules_per_flop": 3.256189431702402e-09
                    },
                    "per-process_emissions": [
                        0.0045373849344512865,
                        0.004531258205592671,
                        0.004525495101483604,
                        0.004543441034287112
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0453": {
            "setup": {
                "experiment_id": "0453",
                "cycle_id": 2,
                "date_time": "April 24, 2025 at 08:41:27 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_56",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 56,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 33.96582212904468,
                        "average_latency_ms_per_batch": 11321.940709681561,
                        "throughput_queries_per_sec": 3.7684940913161435,
                        "throughput_tokens_per_sec": 482.3672436884664
                    }
                },
                "compute_metrics": {
                    "flops": 0.0,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25714405888,
                        "gpu_max_memory_allocated_bytes": 25714405888,
                        "gpu_current_memory_reserved_bytes": 40152072192,
                        "gpu_max_memory_reserved_bytes": 40152072192
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 6.5,
                        "cpu_memory_usage_bytes": 1975459840
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0453",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_3": 561.0344903643223,
                            "process_0": 0.0,
                            "process_2": 2216.0916468166283,
                            "process_1": 492.48905769290496
                        },
                        "ram_power": {
                            "process_3": 0.7249188423156738,
                            "process_0": 0.6872291564941406,
                            "process_2": 0.7060246467590332,
                            "process_1": 0.7307567596435547
                        },
                        "cpu_energy": {
                            "process_3": 0.0010400996567186665,
                            "process_0": 0.0010715520203539199,
                            "process_2": 0.0010130321273718436,
                            "process_1": 0.001043142349102709
                        },
                        "gpu_energy": {
                            "process_3": 0.007421712326253882,
                            "process_0": 0.007406290091696022,
                            "process_2": 0.00718972380733085,
                            "process_1": 0.007431099833766552
                        },
                        "ram_energy": {
                            "process_3": 5.986895343005645e-06,
                            "process_0": 5.849754556515631e-06,
                            "process_2": 5.872631869054701e-06,
                            "process_1": 6.055004380925991e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.008467798878315554,
                            "process_0": 0.008483691866606457,
                            "process_2": 0.008208628566571747,
                            "process_1": 0.008480297187250189
                        },
                        "total_energy_joules": {
                            "process_3": 30484.075961935996,
                            "process_0": 30541.290719783243,
                            "process_2": 29551.06283965829,
                            "process_1": 30529.069874100678
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 817.4037987184639,
                        "ram_power_avg": 0.7122323513031006,
                        "cpu_energy_total": 0.004167826153547139,
                        "gpu_energy_total": 0.029448826059047306,
                        "ram_energy_total": 2.3764286149501966e-05,
                        "total_energy_kwh": 0.03364041649874395,
                        "total_energy_joules": 121105.49939547821
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.13528700250429535,
                        "joules_per_token": 7.391693078337293,
                        "flops_per_joule": 0.0,
                        "joules_per_flop": 0
                    },
                    "per-process_emissions": [
                        0.0032258079826943107,
                        0.0032318624165837296,
                        0.003127077052435507,
                        0.0032305692134829596
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0453": {
            "setup": {
                "experiment_id": "0453",
                "cycle_id": 2,
                "date_time": "April 24, 2025 at 08:41:27 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_56",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 56,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 33.96582212904468,
                        "average_latency_ms_per_batch": 11321.940709681561,
                        "throughput_queries_per_sec": 3.7684940913161435,
                        "throughput_tokens_per_sec": 482.3672436884664
                    }
                },
                "compute_metrics": {
                    "flops": 0.0,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25714405888,
                        "gpu_max_memory_allocated_bytes": 25714405888,
                        "gpu_current_memory_reserved_bytes": 40152072192,
                        "gpu_max_memory_reserved_bytes": 40152072192
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 6.5,
                        "cpu_memory_usage_bytes": 1975459840
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0453",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_3": 561.0344903643223,
                            "process_0": 0.0,
                            "process_2": 2216.0916468166283,
                            "process_1": 492.48905769290496
                        },
                        "ram_power": {
                            "process_3": 0.7249188423156738,
                            "process_0": 0.6872291564941406,
                            "process_2": 0.7060246467590332,
                            "process_1": 0.7307567596435547
                        },
                        "cpu_energy": {
                            "process_3": 0.0010400996567186665,
                            "process_0": 0.0010715520203539199,
                            "process_2": 0.0010130321273718436,
                            "process_1": 0.001043142349102709
                        },
                        "gpu_energy": {
                            "process_3": 0.007421712326253882,
                            "process_0": 0.007406290091696022,
                            "process_2": 0.00718972380733085,
                            "process_1": 0.007431099833766552
                        },
                        "ram_energy": {
                            "process_3": 5.986895343005645e-06,
                            "process_0": 5.849754556515631e-06,
                            "process_2": 5.872631869054701e-06,
                            "process_1": 6.055004380925991e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.008467798878315554,
                            "process_0": 0.008483691866606457,
                            "process_2": 0.008208628566571747,
                            "process_1": 0.008480297187250189
                        },
                        "total_energy_joules": {
                            "process_3": 30484.075961935996,
                            "process_0": 30541.290719783243,
                            "process_2": 29551.06283965829,
                            "process_1": 30529.069874100678
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 817.4037987184639,
                        "ram_power_avg": 0.7122323513031006,
                        "cpu_energy_total": 0.004167826153547139,
                        "gpu_energy_total": 0.029448826059047306,
                        "ram_energy_total": 2.3764286149501966e-05,
                        "total_energy_kwh": 0.03364041649874395,
                        "total_energy_joules": 121105.49939547821
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.13528700250429535,
                        "joules_per_token": 7.391693078337293,
                        "flops_per_joule": 0.0,
                        "joules_per_flop": 0
                    },
                    "per-process_emissions": [
                        0.0032258079826943107,
                        0.0032318624165837296,
                        0.003127077052435507,
                        0.0032305692134829596
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0454": {
            "setup": {
                "experiment_id": "0454",
                "cycle_id": 2,
                "date_time": "April 24, 2025 at 08:42:51 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_20_temp_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 0.8,
                    "decoder_top_k": 20,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.638358301948756,
                        "average_latency_ms_per_batch": 4704.7947877435945,
                        "throughput_queries_per_sec": 3.4007859474936954,
                        "throughput_tokens_per_sec": 435.300601279193
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38449184768,
                        "gpu_max_memory_reserved_bytes": 38449184768
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 6.2,
                        "cpu_memory_usage_bytes": 1975746560
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0454",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 1010.948547125438,
                            "process_2": 1077.1319977781538,
                            "process_1": 998.2177834854402,
                            "process_3": 880.6071079318831
                        },
                        "ram_power": {
                            "process_0": 0.6883535385131836,
                            "process_2": 0.717165470123291,
                            "process_1": 0.7318196296691895,
                            "process_3": 0.7245268821716309
                        },
                        "cpu_energy": {
                            "process_0": 0.0011624153006905542,
                            "process_2": 0.0011614717505963195,
                            "process_1": 0.001163532626809683,
                            "process_3": 0.0011697085555661033
                        },
                        "gpu_energy": {
                            "process_0": 0.010160460628362955,
                            "process_2": 0.010160460628362955,
                            "process_1": 0.01016466090949919,
                            "process_3": 0.010190719541457227
                        },
                        "ram_energy": {
                            "process_0": 6.0627569488880496e-06,
                            "process_2": 6.314984366886349e-06,
                            "process_1": 6.443643734286949e-06,
                            "process_3": 6.417900770399026e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.011328938686002394,
                            "process_2": 0.011328247363326164,
                            "process_1": 0.011334637180043154,
                            "process_3": 0.011366845997793732
                        },
                        "total_energy_joules": {
                            "process_0": 40784.17926960862,
                            "process_2": 40781.69050797419,
                            "process_1": 40804.69384815535,
                            "process_3": 40920.64559205744
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 991.7263590802288,
                        "ram_power_avg": 0.7154663801193237,
                        "cpu_energy_total": 0.00465712823366266,
                        "gpu_energy_total": 0.04067630170768233,
                        "ram_energy_total": 2.5239285820460375e-05,
                        "total_energy_kwh": 0.045358669227165443,
                        "total_energy_joules": 163291.2092177956
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.10033608103267361,
                        "joules_per_token": 9.966504468859595,
                        "flops_per_joule": 322360172.1183617,
                        "joules_per_flop": 3.102120195024675e-09
                    },
                    "per-process_emissions": [
                        0.004315759192432612,
                        0.0043154958330591025,
                        0.004317930033737439,
                        0.004330199982859522
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0454": {
            "setup": {
                "experiment_id": "0454",
                "cycle_id": 2,
                "date_time": "April 24, 2025 at 08:42:51 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_20_temp_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 0.8,
                    "decoder_top_k": 20,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.638358301948756,
                        "average_latency_ms_per_batch": 4704.7947877435945,
                        "throughput_queries_per_sec": 3.4007859474936954,
                        "throughput_tokens_per_sec": 435.300601279193
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38449184768,
                        "gpu_max_memory_reserved_bytes": 38449184768
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 6.2,
                        "cpu_memory_usage_bytes": 1975746560
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0454",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 1010.948547125438,
                            "process_2": 1077.1319977781538,
                            "process_1": 998.2177834854402,
                            "process_3": 880.6071079318831
                        },
                        "ram_power": {
                            "process_0": 0.6883535385131836,
                            "process_2": 0.717165470123291,
                            "process_1": 0.7318196296691895,
                            "process_3": 0.7245268821716309
                        },
                        "cpu_energy": {
                            "process_0": 0.0011624153006905542,
                            "process_2": 0.0011614717505963195,
                            "process_1": 0.001163532626809683,
                            "process_3": 0.0011697085555661033
                        },
                        "gpu_energy": {
                            "process_0": 0.010160460628362955,
                            "process_2": 0.010160460628362955,
                            "process_1": 0.01016466090949919,
                            "process_3": 0.010190719541457227
                        },
                        "ram_energy": {
                            "process_0": 6.0627569488880496e-06,
                            "process_2": 6.314984366886349e-06,
                            "process_1": 6.443643734286949e-06,
                            "process_3": 6.417900770399026e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.011328938686002394,
                            "process_2": 0.011328247363326164,
                            "process_1": 0.011334637180043154,
                            "process_3": 0.011366845997793732
                        },
                        "total_energy_joules": {
                            "process_0": 40784.17926960862,
                            "process_2": 40781.69050797419,
                            "process_1": 40804.69384815535,
                            "process_3": 40920.64559205744
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 991.7263590802288,
                        "ram_power_avg": 0.7154663801193237,
                        "cpu_energy_total": 0.00465712823366266,
                        "gpu_energy_total": 0.04067630170768233,
                        "ram_energy_total": 2.5239285820460375e-05,
                        "total_energy_kwh": 0.045358669227165443,
                        "total_energy_joules": 163291.2092177956
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.10033608103267361,
                        "joules_per_token": 9.966504468859595,
                        "flops_per_joule": 322360172.1183617,
                        "joules_per_flop": 3.102120195024675e-09
                    },
                    "per-process_emissions": [
                        0.004315759192432612,
                        0.0043154958330591025,
                        0.004317930033737439,
                        0.004330199982859522
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0455": {
            "setup": {
                "experiment_id": "0455",
                "cycle_id": 2,
                "date_time": "April 24, 2025 at 08:44:15 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_20_temp_0.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 0.2,
                    "decoder_top_k": 20,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.78254205104895,
                        "average_latency_ms_per_batch": 4722.817756381119,
                        "throughput_queries_per_sec": 3.3878080470884133,
                        "throughput_tokens_per_sec": 433.6394300273169
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38449184768,
                        "gpu_max_memory_reserved_bytes": 38449184768
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 6.6,
                        "cpu_memory_usage_bytes": 1977237504
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0455",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_0": 913.8715836355339,
                            "process_1": 974.7668423670831,
                            "process_3": 704.9252476302784,
                            "process_2": 956.3880464742771
                        },
                        "ram_power": {
                            "process_0": 0.6885709762573242,
                            "process_1": 0.7322402000427247,
                            "process_3": 0.7244153022766113,
                            "process_2": 0.7243123054504395
                        },
                        "cpu_energy": {
                            "process_0": 0.0011651786193706357,
                            "process_1": 0.0011639682461845953,
                            "process_3": 0.0011753021366548639,
                            "process_2": 0.0011608601574353088
                        },
                        "gpu_energy": {
                            "process_0": 0.01018109786709509,
                            "process_1": 0.01017327341638996,
                            "process_3": 0.010212884559187785,
                            "process_2": 0.010144692837972258
                        },
                        "ram_energy": {
                            "process_0": 6.061333250690522e-06,
                            "process_1": 6.457551635881391e-06,
                            "process_3": 6.609149957080699e-06,
                            "process_2": 6.3603625808555375e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.011352337819716415,
                            "process_1": 0.011343699214210438,
                            "process_3": 0.011394795845799728,
                            "process_2": 0.011311913357988418
                        },
                        "total_energy_joules": {
                            "process_0": 40868.416150979094,
                            "process_1": 40837.31717115758,
                            "process_3": 41021.26504487902,
                            "process_2": 40722.88808875831
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 887.487930026793,
                        "ram_power_avg": 0.7173846960067749,
                        "cpu_energy_total": 0.004665309159645404,
                        "gpu_energy_total": 0.040711948680645094,
                        "ram_energy_total": 2.548839742450815e-05,
                        "total_energy_kwh": 0.045402746237715,
                        "total_energy_joules": 163449.88645577402
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.10023867471105986,
                        "joules_per_token": 9.976189358872926,
                        "flops_per_joule": 322047224.6893047,
                        "joules_per_flop": 3.105134661429704e-09
                    },
                    "per-process_emissions": [
                        0.004324673092420969,
                        0.004321382215653467,
                        0.004340847477457406,
                        0.004309273393725688
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0455": {
            "setup": {
                "experiment_id": "0455",
                "cycle_id": 2,
                "date_time": "April 24, 2025 at 08:44:15 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_20_temp_0.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 0.2,
                    "decoder_top_k": 20,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.78254205104895,
                        "average_latency_ms_per_batch": 4722.817756381119,
                        "throughput_queries_per_sec": 3.3878080470884133,
                        "throughput_tokens_per_sec": 433.6394300273169
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38449184768,
                        "gpu_max_memory_reserved_bytes": 38449184768
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 6.6,
                        "cpu_memory_usage_bytes": 1977237504
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0455",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_0": 913.8715836355339,
                            "process_1": 974.7668423670831,
                            "process_3": 704.9252476302784,
                            "process_2": 956.3880464742771
                        },
                        "ram_power": {
                            "process_0": 0.6885709762573242,
                            "process_1": 0.7322402000427247,
                            "process_3": 0.7244153022766113,
                            "process_2": 0.7243123054504395
                        },
                        "cpu_energy": {
                            "process_0": 0.0011651786193706357,
                            "process_1": 0.0011639682461845953,
                            "process_3": 0.0011753021366548639,
                            "process_2": 0.0011608601574353088
                        },
                        "gpu_energy": {
                            "process_0": 0.01018109786709509,
                            "process_1": 0.01017327341638996,
                            "process_3": 0.010212884559187785,
                            "process_2": 0.010144692837972258
                        },
                        "ram_energy": {
                            "process_0": 6.061333250690522e-06,
                            "process_1": 6.457551635881391e-06,
                            "process_3": 6.609149957080699e-06,
                            "process_2": 6.3603625808555375e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.011352337819716415,
                            "process_1": 0.011343699214210438,
                            "process_3": 0.011394795845799728,
                            "process_2": 0.011311913357988418
                        },
                        "total_energy_joules": {
                            "process_0": 40868.416150979094,
                            "process_1": 40837.31717115758,
                            "process_3": 41021.26504487902,
                            "process_2": 40722.88808875831
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 887.487930026793,
                        "ram_power_avg": 0.7173846960067749,
                        "cpu_energy_total": 0.004665309159645404,
                        "gpu_energy_total": 0.040711948680645094,
                        "ram_energy_total": 2.548839742450815e-05,
                        "total_energy_kwh": 0.045402746237715,
                        "total_energy_joules": 163449.88645577402
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.10023867471105986,
                        "joules_per_token": 9.976189358872926,
                        "flops_per_joule": 322047224.6893047,
                        "joules_per_flop": 3.105134661429704e-09
                    },
                    "per-process_emissions": [
                        0.004324673092420969,
                        0.004321382215653467,
                        0.004340847477457406,
                        0.004309273393725688
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0456": {
            "setup": {
                "experiment_id": "0456",
                "cycle_id": 2,
                "date_time": "April 24, 2025 at 08:45:39 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_topp_0.8_temp_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_p",
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": 0.8
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.35990455391584,
                        "average_latency_ms_per_batch": 4794.98806923948,
                        "throughput_queries_per_sec": 3.3368174787841998,
                        "throughput_tokens_per_sec": 427.11263728437757
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38403047424,
                        "gpu_max_memory_reserved_bytes": 38403047424
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 6.6,
                        "cpu_memory_usage_bytes": 1980444672
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0456",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 991.1666508035472,
                            "process_1": 1027.3672344297986,
                            "process_2": 986.2865442249304,
                            "process_3": 743.2842645847286
                        },
                        "ram_power": {
                            "process_0": 0.6890759468078613,
                            "process_1": 0.7274937629699707,
                            "process_2": 0.733788013458252,
                            "process_3": 0.7335405349731445
                        },
                        "cpu_energy": {
                            "process_0": 0.0011847479823100004,
                            "process_1": 0.0011834485145991495,
                            "process_2": 0.0011815251599655314,
                            "process_3": 0.0011919603987553273
                        },
                        "gpu_energy": {
                            "process_0": 0.010335845213115036,
                            "process_1": 0.010325296871343426,
                            "process_2": 0.010320888812263007,
                            "process_3": 0.010352260504022937
                        },
                        "ram_energy": {
                            "process_0": 6.227919824067485e-06,
                            "process_1": 6.563261311710129e-06,
                            "process_2": 6.602788983262135e-06,
                            "process_3": 6.68414917476699e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.0115268211152491,
                            "process_1": 0.011515308647254289,
                            "process_2": 0.011509016761211802,
                            "process_3": 0.01155090505195303
                        },
                        "total_energy_joules": {
                            "process_0": 41496.556014896756,
                            "process_1": 41455.11113011544,
                            "process_2": 41432.460340362486,
                            "process_3": 41583.25818703091
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 937.0261735107512,
                        "ram_power_avg": 0.7209745645523071,
                        "cpu_energy_total": 0.004741682055630008,
                        "gpu_energy_total": 0.041334291400744405,
                        "ram_energy_total": 2.607811929380674e-05,
                        "total_energy_kwh": 0.04610205157566822,
                        "total_energy_joules": 165967.3856724056
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09871819052653831,
                        "joules_per_token": 10.129845316919287,
                        "flops_per_joule": 317162206.8733707,
                        "joules_per_flop": 3.1529607826170076e-09
                    },
                    "per-process_emissions": [
                        0.004391142503854145,
                        0.004386756829171522,
                        0.004384359935183636,
                        0.004400317279541507
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0456": {
            "setup": {
                "experiment_id": "0456",
                "cycle_id": 2,
                "date_time": "April 24, 2025 at 08:45:39 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_topp_0.8_temp_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_p",
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": 0.8
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.35990455391584,
                        "average_latency_ms_per_batch": 4794.98806923948,
                        "throughput_queries_per_sec": 3.3368174787841998,
                        "throughput_tokens_per_sec": 427.11263728437757
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38403047424,
                        "gpu_max_memory_reserved_bytes": 38403047424
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 6.6,
                        "cpu_memory_usage_bytes": 1980444672
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0456",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 991.1666508035472,
                            "process_1": 1027.3672344297986,
                            "process_2": 986.2865442249304,
                            "process_3": 743.2842645847286
                        },
                        "ram_power": {
                            "process_0": 0.6890759468078613,
                            "process_1": 0.7274937629699707,
                            "process_2": 0.733788013458252,
                            "process_3": 0.7335405349731445
                        },
                        "cpu_energy": {
                            "process_0": 0.0011847479823100004,
                            "process_1": 0.0011834485145991495,
                            "process_2": 0.0011815251599655314,
                            "process_3": 0.0011919603987553273
                        },
                        "gpu_energy": {
                            "process_0": 0.010335845213115036,
                            "process_1": 0.010325296871343426,
                            "process_2": 0.010320888812263007,
                            "process_3": 0.010352260504022937
                        },
                        "ram_energy": {
                            "process_0": 6.227919824067485e-06,
                            "process_1": 6.563261311710129e-06,
                            "process_2": 6.602788983262135e-06,
                            "process_3": 6.68414917476699e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.0115268211152491,
                            "process_1": 0.011515308647254289,
                            "process_2": 0.011509016761211802,
                            "process_3": 0.01155090505195303
                        },
                        "total_energy_joules": {
                            "process_0": 41496.556014896756,
                            "process_1": 41455.11113011544,
                            "process_2": 41432.460340362486,
                            "process_3": 41583.25818703091
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 937.0261735107512,
                        "ram_power_avg": 0.7209745645523071,
                        "cpu_energy_total": 0.004741682055630008,
                        "gpu_energy_total": 0.041334291400744405,
                        "ram_energy_total": 2.607811929380674e-05,
                        "total_energy_kwh": 0.04610205157566822,
                        "total_energy_joules": 165967.3856724056
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09871819052653831,
                        "joules_per_token": 10.129845316919287,
                        "flops_per_joule": 317162206.8733707,
                        "joules_per_flop": 3.1529607826170076e-09
                    },
                    "per-process_emissions": [
                        0.004391142503854145,
                        0.004386756829171522,
                        0.004384359935183636,
                        0.004400317279541507
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0457": {
            "setup": {
                "experiment_id": "0457",
                "cycle_id": 2,
                "date_time": "April 24, 2025 at 08:46:51 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_32",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 32,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 25.569964621041436,
                        "average_latency_ms_per_batch": 6392.491155260359,
                        "throughput_queries_per_sec": 5.005873175697288,
                        "throughput_tokens_per_sec": 640.7517664892529
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 39950745600,
                        "gpu_max_memory_reserved_bytes": 39950745600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            28.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.2,
                        "cpu_memory_usage_bytes": 1972142080
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0457",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_3": 844.4540231053456,
                            "process_0": 1089.8582632788155,
                            "process_1": 0.0,
                            "process_2": 1004.0173030646282
                        },
                        "ram_power": {
                            "process_3": 0.7236270904541016,
                            "process_0": 0.6867828369140625,
                            "process_1": 0.7234039306640625,
                            "process_2": 0.730074405670166
                        },
                        "cpu_energy": {
                            "process_3": 0.0007974285105683521,
                            "process_0": 0.0007902813577129561,
                            "process_1": 0.0008375648279052257,
                            "process_2": 0.0007902979661648715
                        },
                        "gpu_energy": {
                            "process_3": 0.007039921187487863,
                            "process_0": 0.007005614215595202,
                            "process_1": 0.007070817045534028,
                            "process_2": 0.007005614215595202
                        },
                        "ram_energy": {
                            "process_3": 4.468267887232472e-06,
                            "process_0": 4.1889418138856456e-06,
                            "process_1": 4.653904983726567e-06,
                            "process_2": 4.434947629850009e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.007841817965943448,
                            "process_0": 0.007800084515122044,
                            "process_1": 0.007913035778422981,
                            "process_2": 0.007800347129389923
                        },
                        "total_energy_joules": {
                            "process_3": 28230.54467739641,
                            "process_0": 28080.304254439357,
                            "process_1": 28486.92880232273,
                            "process_2": 28081.249665803723
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 734.5823973621973,
                        "ram_power_avg": 0.7159720659255981,
                        "cpu_energy_total": 0.0032155726623514054,
                        "gpu_energy_total": 0.028121966664212295,
                        "ram_energy_total": 1.7746062314694694e-05,
                        "total_energy_kwh": 0.0313552853888784,
                        "total_energy_joules": 112879.02739996223
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.14514653764642096,
                        "joules_per_token": 6.889589074704725,
                        "flops_per_joule": 466327390.67063946,
                        "joules_per_flop": 2.1444161762873717e-09
                    },
                    "per-process_emissions": [
                        0.0029873405541261566,
                        0.0029714421960357427,
                        0.0030144709797902346,
                        0.0029715422389410915
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0457": {
            "setup": {
                "experiment_id": "0457",
                "cycle_id": 2,
                "date_time": "April 24, 2025 at 08:46:51 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_32",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 32,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 25.569964621041436,
                        "average_latency_ms_per_batch": 6392.491155260359,
                        "throughput_queries_per_sec": 5.005873175697288,
                        "throughput_tokens_per_sec": 640.7517664892529
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 39950745600,
                        "gpu_max_memory_reserved_bytes": 39950745600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            28.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.2,
                        "cpu_memory_usage_bytes": 1972142080
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0457",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_3": 844.4540231053456,
                            "process_0": 1089.8582632788155,
                            "process_1": 0.0,
                            "process_2": 1004.0173030646282
                        },
                        "ram_power": {
                            "process_3": 0.7236270904541016,
                            "process_0": 0.6867828369140625,
                            "process_1": 0.7234039306640625,
                            "process_2": 0.730074405670166
                        },
                        "cpu_energy": {
                            "process_3": 0.0007974285105683521,
                            "process_0": 0.0007902813577129561,
                            "process_1": 0.0008375648279052257,
                            "process_2": 0.0007902979661648715
                        },
                        "gpu_energy": {
                            "process_3": 0.007039921187487863,
                            "process_0": 0.007005614215595202,
                            "process_1": 0.007070817045534028,
                            "process_2": 0.007005614215595202
                        },
                        "ram_energy": {
                            "process_3": 4.468267887232472e-06,
                            "process_0": 4.1889418138856456e-06,
                            "process_1": 4.653904983726567e-06,
                            "process_2": 4.434947629850009e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.007841817965943448,
                            "process_0": 0.007800084515122044,
                            "process_1": 0.007913035778422981,
                            "process_2": 0.007800347129389923
                        },
                        "total_energy_joules": {
                            "process_3": 28230.54467739641,
                            "process_0": 28080.304254439357,
                            "process_1": 28486.92880232273,
                            "process_2": 28081.249665803723
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 734.5823973621973,
                        "ram_power_avg": 0.7159720659255981,
                        "cpu_energy_total": 0.0032155726623514054,
                        "gpu_energy_total": 0.028121966664212295,
                        "ram_energy_total": 1.7746062314694694e-05,
                        "total_energy_kwh": 0.0313552853888784,
                        "total_energy_joules": 112879.02739996223
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.14514653764642096,
                        "joules_per_token": 6.889589074704725,
                        "flops_per_joule": 466327390.67063946,
                        "joules_per_flop": 2.1444161762873717e-09
                    },
                    "per-process_emissions": [
                        0.0029873405541261566,
                        0.0029714421960357427,
                        0.0030144709797902346,
                        0.0029715422389410915
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0458": {
            "setup": {
                "experiment_id": "0458",
                "cycle_id": 2,
                "date_time": "April 24, 2025 at 08:48:22 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_burst_0.4_0.5_4.0_8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.4,
                    "delay_max": 0.5,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 8
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 45.08045972400578,
                        "average_latency_ms_per_batch": 5635.057465500722,
                        "throughput_queries_per_sec": 2.839367672460509,
                        "throughput_tokens_per_sec": 363.4390620749451
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38352715776,
                        "gpu_max_memory_reserved_bytes": 38352715776
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.3,
                        "cpu_memory_usage_bytes": 1973919744
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0458",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_3": 511.4543930883864,
                            "process_2": 1021.6682962965682,
                            "process_0": 1206.1193920641083,
                            "process_1": 1202.25728630294
                        },
                        "ram_power": {
                            "process_3": 0.7061676979064941,
                            "process_2": 0.7290916442871094,
                            "process_0": 0.6876082420349121,
                            "process_1": 0.7313346862792969
                        },
                        "cpu_energy": {
                            "process_3": 0.0013935603725549301,
                            "process_2": 0.0013832316332754997,
                            "process_0": 0.0013881634193730862,
                            "process_1": 0.0013874537503415925
                        },
                        "gpu_energy": {
                            "process_3": 0.010545978158997826,
                            "process_2": 0.01049297144992778,
                            "process_0": 0.010529732312669182,
                            "process_1": 0.010529732312669182
                        },
                        "ram_energy": {
                            "process_3": 7.588739156062417e-06,
                            "process_2": 7.78777245668734e-06,
                            "process_0": 7.538749336662546e-06,
                            "process_1": 7.991977665419144e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.01194712727070882,
                            "process_2": 0.011883990855659967,
                            "process_0": 0.011925434481378928,
                            "process_1": 0.011925178040676195
                        },
                        "total_energy_joules": {
                            "process_3": 43009.658174551754,
                            "process_2": 42782.36708037588,
                            "process_0": 42931.564132964144,
                            "process_1": 42930.6409464343
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 985.3748419380007,
                        "ram_power_avg": 0.7135505676269531,
                        "cpu_energy_total": 0.005552409175545109,
                        "gpu_energy_total": 0.04209841423426397,
                        "ram_energy_total": 3.090723861483145e-05,
                        "total_energy_kwh": 0.04768173064842391,
                        "total_energy_joules": 171654.2303343261
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09544769137404505,
                        "joules_per_token": 10.476942769429083,
                        "flops_per_joule": 306654733.8002758,
                        "joules_per_flop": 3.2609964555489297e-09
                    },
                    "per-process_emissions": [
                        0.004551258133776525,
                        0.004527206316463665,
                        0.004542994265681303,
                        0.004542896574595596
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0458": {
            "setup": {
                "experiment_id": "0458",
                "cycle_id": 2,
                "date_time": "April 24, 2025 at 08:48:22 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_burst_0.4_0.5_4.0_8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.4,
                    "delay_max": 0.5,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 8
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 45.08045972400578,
                        "average_latency_ms_per_batch": 5635.057465500722,
                        "throughput_queries_per_sec": 2.839367672460509,
                        "throughput_tokens_per_sec": 363.4390620749451
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38352715776,
                        "gpu_max_memory_reserved_bytes": 38352715776
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.3,
                        "cpu_memory_usage_bytes": 1973919744
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0458",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_3": 511.4543930883864,
                            "process_2": 1021.6682962965682,
                            "process_0": 1206.1193920641083,
                            "process_1": 1202.25728630294
                        },
                        "ram_power": {
                            "process_3": 0.7061676979064941,
                            "process_2": 0.7290916442871094,
                            "process_0": 0.6876082420349121,
                            "process_1": 0.7313346862792969
                        },
                        "cpu_energy": {
                            "process_3": 0.0013935603725549301,
                            "process_2": 0.0013832316332754997,
                            "process_0": 0.0013881634193730862,
                            "process_1": 0.0013874537503415925
                        },
                        "gpu_energy": {
                            "process_3": 0.010545978158997826,
                            "process_2": 0.01049297144992778,
                            "process_0": 0.010529732312669182,
                            "process_1": 0.010529732312669182
                        },
                        "ram_energy": {
                            "process_3": 7.588739156062417e-06,
                            "process_2": 7.78777245668734e-06,
                            "process_0": 7.538749336662546e-06,
                            "process_1": 7.991977665419144e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.01194712727070882,
                            "process_2": 0.011883990855659967,
                            "process_0": 0.011925434481378928,
                            "process_1": 0.011925178040676195
                        },
                        "total_energy_joules": {
                            "process_3": 43009.658174551754,
                            "process_2": 42782.36708037588,
                            "process_0": 42931.564132964144,
                            "process_1": 42930.6409464343
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 985.3748419380007,
                        "ram_power_avg": 0.7135505676269531,
                        "cpu_energy_total": 0.005552409175545109,
                        "gpu_energy_total": 0.04209841423426397,
                        "ram_energy_total": 3.090723861483145e-05,
                        "total_energy_kwh": 0.04768173064842391,
                        "total_energy_joules": 171654.2303343261
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09544769137404505,
                        "joules_per_token": 10.476942769429083,
                        "flops_per_joule": 306654733.8002758,
                        "joules_per_flop": 3.2609964555489297e-09
                    },
                    "per-process_emissions": [
                        0.004551258133776525,
                        0.004527206316463665,
                        0.004542994265681303,
                        0.004542896574595596
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0459": {
            "setup": {
                "experiment_id": "0459",
                "cycle_id": 2,
                "date_time": "April 24, 2025 at 08:49:46 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_topp_0.7_temp_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_p",
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": 0.7
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.459357612999156,
                        "average_latency_ms_per_batch": 4807.4197016248945,
                        "throughput_queries_per_sec": 3.328188715162948,
                        "throughput_tokens_per_sec": 426.00815554085733
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38403047424,
                        "gpu_max_memory_reserved_bytes": 38403047424
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.2,
                        "cpu_memory_usage_bytes": 1985130496
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0459",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 922.1453167649756,
                            "process_3": 781.503166326535,
                            "process_2": 957.440145371045,
                            "process_1": 981.9705358588524
                        },
                        "ram_power": {
                            "process_0": 0.6919312477111816,
                            "process_3": 0.7338237762451172,
                            "process_2": 0.7195572853088379,
                            "process_1": 0.7336635589599609
                        },
                        "cpu_energy": {
                            "process_0": 0.0011390451022762137,
                            "process_3": 0.001145544570123093,
                            "process_2": 0.0011372349460289117,
                            "process_1": 0.0011386956835249295
                        },
                        "gpu_energy": {
                            "process_0": 0.010326110205324923,
                            "process_3": 0.010350848836228721,
                            "process_2": 0.010308434080071649,
                            "process_1": 0.010318631032673053
                        },
                        "ram_energy": {
                            "process_0": 6.255272568106228e-06,
                            "process_3": 6.704479811533495e-06,
                            "process_2": 6.524235399798127e-06,
                            "process_1": 6.678520489521371e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.01147141058016924,
                            "process_3": 0.011503097886163344,
                            "process_2": 0.011452193261500355,
                            "process_1": 0.011464005236687502
                        },
                        "total_energy_joules": {
                            "process_0": 41297.07808860927,
                            "process_3": 41411.15239018804,
                            "process_2": 41227.89574140128,
                            "process_1": 41270.41885207501
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 910.7647910803521,
                        "ram_power_avg": 0.7197439670562744,
                        "cpu_energy_total": 0.004560520301953148,
                        "gpu_energy_total": 0.04130402415429835,
                        "ram_energy_total": 2.6162508268959223e-05,
                        "total_energy_kwh": 0.04589070696452044,
                        "total_energy_joules": 165206.54507227358
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09917282631164778,
                        "joules_per_token": 10.083407292008886,
                        "flops_per_joule": 318622862.5859586,
                        "joules_per_flop": 3.1385067345260526e-09
                    },
                    "per-process_emissions": [
                        0.004370033860515472,
                        0.0043821051397339265,
                        0.004362713022968561,
                        0.004367212794916104
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0459": {
            "setup": {
                "experiment_id": "0459",
                "cycle_id": 2,
                "date_time": "April 24, 2025 at 08:49:46 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_topp_0.7_temp_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_p",
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": 0.7
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.459357612999156,
                        "average_latency_ms_per_batch": 4807.4197016248945,
                        "throughput_queries_per_sec": 3.328188715162948,
                        "throughput_tokens_per_sec": 426.00815554085733
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38403047424,
                        "gpu_max_memory_reserved_bytes": 38403047424
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.2,
                        "cpu_memory_usage_bytes": 1985130496
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0459",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 922.1453167649756,
                            "process_3": 781.503166326535,
                            "process_2": 957.440145371045,
                            "process_1": 981.9705358588524
                        },
                        "ram_power": {
                            "process_0": 0.6919312477111816,
                            "process_3": 0.7338237762451172,
                            "process_2": 0.7195572853088379,
                            "process_1": 0.7336635589599609
                        },
                        "cpu_energy": {
                            "process_0": 0.0011390451022762137,
                            "process_3": 0.001145544570123093,
                            "process_2": 0.0011372349460289117,
                            "process_1": 0.0011386956835249295
                        },
                        "gpu_energy": {
                            "process_0": 0.010326110205324923,
                            "process_3": 0.010350848836228721,
                            "process_2": 0.010308434080071649,
                            "process_1": 0.010318631032673053
                        },
                        "ram_energy": {
                            "process_0": 6.255272568106228e-06,
                            "process_3": 6.704479811533495e-06,
                            "process_2": 6.524235399798127e-06,
                            "process_1": 6.678520489521371e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.01147141058016924,
                            "process_3": 0.011503097886163344,
                            "process_2": 0.011452193261500355,
                            "process_1": 0.011464005236687502
                        },
                        "total_energy_joules": {
                            "process_0": 41297.07808860927,
                            "process_3": 41411.15239018804,
                            "process_2": 41227.89574140128,
                            "process_1": 41270.41885207501
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 910.7647910803521,
                        "ram_power_avg": 0.7197439670562744,
                        "cpu_energy_total": 0.004560520301953148,
                        "gpu_energy_total": 0.04130402415429835,
                        "ram_energy_total": 2.6162508268959223e-05,
                        "total_energy_kwh": 0.04589070696452044,
                        "total_energy_joules": 165206.54507227358
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09917282631164778,
                        "joules_per_token": 10.083407292008886,
                        "flops_per_joule": 318622862.5859586,
                        "joules_per_flop": 3.1385067345260526e-09
                    },
                    "per-process_emissions": [
                        0.004370033860515472,
                        0.0043821051397339265,
                        0.004362713022968561,
                        0.004367212794916104
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0460": {
            "setup": {
                "experiment_id": "0460",
                "cycle_id": 2,
                "date_time": "April 24, 2025 at 08:51:14 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_burst_0.2_0.4_2.0_5",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.2,
                    "delay_max": 0.4,
                    "simulate_burst": true,
                    "burst_interval": 2.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 41.78801634901902,
                        "average_latency_ms_per_batch": 5223.502043627377,
                        "throughput_queries_per_sec": 3.063079111746467,
                        "throughput_tokens_per_sec": 392.07412630354776
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38352715776,
                        "gpu_max_memory_reserved_bytes": 38352715776
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 7.2,
                        "cpu_memory_usage_bytes": 1950412800
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0460",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_3": 456.01138472210425,
                            "process_1": 30.037936432035703,
                            "process_0": 988.3904976800633,
                            "process_2": 995.9307058537775
                        },
                        "ram_power": {
                            "process_3": 0.7303805351257324,
                            "process_1": 0.7308382987976074,
                            "process_0": 0.6793441772460938,
                            "process_2": 0.7291574478149414
                        },
                        "cpu_energy": {
                            "process_3": 0.0013022051663738245,
                            "process_1": 0.0013301918193646999,
                            "process_0": 0.0012892224867227923,
                            "process_2": 0.001286122230152614
                        },
                        "gpu_energy": {
                            "process_3": 0.010424366395040963,
                            "process_1": 0.010410838328665406,
                            "process_0": 0.01035667356311265,
                            "process_2": 0.01034756633360523
                        },
                        "ram_energy": {
                            "process_3": 7.224551551241308e-06,
                            "process_1": 7.43224235092786e-06,
                            "process_0": 6.691368945728659e-06,
                            "process_2": 7.156687943979782e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.011733796112966028,
                            "process_1": 0.011748462390381038,
                            "process_0": 0.011652587418781175,
                            "process_2": 0.011640845251701821
                        },
                        "total_energy_joules": {
                            "process_3": 42241.666006677704,
                            "process_1": 42294.46460537174,
                            "process_0": 41949.31470761223,
                            "process_2": 41907.04290612655
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 617.5926311719952,
                        "ram_power_avg": 0.7174301147460938,
                        "cpu_energy_total": 0.00520774170261393,
                        "gpu_energy_total": 0.04153944462042425,
                        "ram_energy_total": 2.8504850791877606e-05,
                        "total_energy_kwh": 0.04677569117383006,
                        "total_energy_joules": 168392.48822578823
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09729650159947512,
                        "joules_per_token": 10.27786183018727,
                        "flops_per_joule": 312594598.86526424,
                        "joules_per_flop": 3.199031600769993e-09
                    },
                    "per-process_emissions": [
                        0.004469989629234408,
                        0.004475576747615656,
                        0.004439053177184689,
                        0.004434579998635809
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0460": {
            "setup": {
                "experiment_id": "0460",
                "cycle_id": 2,
                "date_time": "April 24, 2025 at 08:51:14 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_burst_0.2_0.4_2.0_5",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.2,
                    "delay_max": 0.4,
                    "simulate_burst": true,
                    "burst_interval": 2.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 41.78801634901902,
                        "average_latency_ms_per_batch": 5223.502043627377,
                        "throughput_queries_per_sec": 3.063079111746467,
                        "throughput_tokens_per_sec": 392.07412630354776
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38352715776,
                        "gpu_max_memory_reserved_bytes": 38352715776
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 7.2,
                        "cpu_memory_usage_bytes": 1950412800
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0460",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_3": 456.01138472210425,
                            "process_1": 30.037936432035703,
                            "process_0": 988.3904976800633,
                            "process_2": 995.9307058537775
                        },
                        "ram_power": {
                            "process_3": 0.7303805351257324,
                            "process_1": 0.7308382987976074,
                            "process_0": 0.6793441772460938,
                            "process_2": 0.7291574478149414
                        },
                        "cpu_energy": {
                            "process_3": 0.0013022051663738245,
                            "process_1": 0.0013301918193646999,
                            "process_0": 0.0012892224867227923,
                            "process_2": 0.001286122230152614
                        },
                        "gpu_energy": {
                            "process_3": 0.010424366395040963,
                            "process_1": 0.010410838328665406,
                            "process_0": 0.01035667356311265,
                            "process_2": 0.01034756633360523
                        },
                        "ram_energy": {
                            "process_3": 7.224551551241308e-06,
                            "process_1": 7.43224235092786e-06,
                            "process_0": 6.691368945728659e-06,
                            "process_2": 7.156687943979782e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.011733796112966028,
                            "process_1": 0.011748462390381038,
                            "process_0": 0.011652587418781175,
                            "process_2": 0.011640845251701821
                        },
                        "total_energy_joules": {
                            "process_3": 42241.666006677704,
                            "process_1": 42294.46460537174,
                            "process_0": 41949.31470761223,
                            "process_2": 41907.04290612655
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 617.5926311719952,
                        "ram_power_avg": 0.7174301147460938,
                        "cpu_energy_total": 0.00520774170261393,
                        "gpu_energy_total": 0.04153944462042425,
                        "ram_energy_total": 2.8504850791877606e-05,
                        "total_energy_kwh": 0.04677569117383006,
                        "total_energy_joules": 168392.48822578823
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09729650159947512,
                        "joules_per_token": 10.27786183018727,
                        "flops_per_joule": 312594598.86526424,
                        "joules_per_flop": 3.199031600769993e-09
                    },
                    "per-process_emissions": [
                        0.004469989629234408,
                        0.004475576747615656,
                        0.004439053177184689,
                        0.004434579998635809
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0461": {
            "setup": {
                "experiment_id": "0461",
                "cycle_id": 2,
                "date_time": "April 24, 2025 at 08:52:38 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_topp_0.7_temp_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_p",
                    "decoder_temperature": 0.8,
                    "decoder_top_k": null,
                    "decoder_top_p": 0.7
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.304307358921506,
                        "average_latency_ms_per_batch": 4788.038419865188,
                        "throughput_queries_per_sec": 3.3416607380628527,
                        "throughput_tokens_per_sec": 427.73257447204514
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38403047424,
                        "gpu_max_memory_reserved_bytes": 38403047424
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 5.6,
                        "cpu_memory_usage_bytes": 1980219392
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0461",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_2": 1085.3834743463385,
                            "process_1": 785.4256153501705,
                            "process_0": 1006.4816995895916,
                            "process_3": 750.2685253181571
                        },
                        "ram_power": {
                            "process_2": 0.7324762344360352,
                            "process_1": 0.7320928573608398,
                            "process_0": 0.6903748512268068,
                            "process_3": 0.7323460578918458
                        },
                        "cpu_energy": {
                            "process_2": 0.0011805720096799634,
                            "process_1": 0.0011923800219374244,
                            "process_0": 0.0011815608725573839,
                            "process_3": 0.001195559941284955
                        },
                        "gpu_energy": {
                            "process_2": 0.010296104070210532,
                            "process_1": 0.010347829944924669,
                            "process_0": 0.010296104070210532,
                            "process_3": 0.010363316068424666
                        },
                        "ram_energy": {
                            "process_2": 6.535364144582588e-06,
                            "process_1": 6.579310755549898e-06,
                            "process_0": 6.163259464661282e-06,
                            "process_3": 6.620165971957847e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.011483211444035078,
                            "process_1": 0.011546789277617642,
                            "process_0": 0.011483828202232578,
                            "process_3": 0.011565496175681582
                        },
                        "total_energy_joules": {
                            "process_2": 41339.56119852628,
                            "process_1": 41568.441399423515,
                            "process_0": 41341.78152803728,
                            "process_3": 41635.7862324537
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 906.8898286510645,
                        "ram_power_avg": 0.7218225002288818,
                        "cpu_energy_total": 0.004750072845459727,
                        "gpu_energy_total": 0.0413033541537704,
                        "ram_energy_total": 2.5898100336751614e-05,
                        "total_energy_kwh": 0.04607932509956688,
                        "total_energy_joules": 165885.57035844075
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09876687866580515,
                        "joules_per_token": 10.1248517064478,
                        "flops_per_joule": 317318632.3266338,
                        "joules_per_flop": 3.1514064984707363e-09
                    },
                    "per-process_emissions": [
                        0.0043745293996051635,
                        0.004398749375308441,
                        0.0043747643536405,
                        0.004405875768125899
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0461": {
            "setup": {
                "experiment_id": "0461",
                "cycle_id": 2,
                "date_time": "April 24, 2025 at 08:52:38 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_topp_0.7_temp_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_p",
                    "decoder_temperature": 0.8,
                    "decoder_top_k": null,
                    "decoder_top_p": 0.7
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.304307358921506,
                        "average_latency_ms_per_batch": 4788.038419865188,
                        "throughput_queries_per_sec": 3.3416607380628527,
                        "throughput_tokens_per_sec": 427.73257447204514
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38403047424,
                        "gpu_max_memory_reserved_bytes": 38403047424
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 5.6,
                        "cpu_memory_usage_bytes": 1980219392
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0461",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_2": 1085.3834743463385,
                            "process_1": 785.4256153501705,
                            "process_0": 1006.4816995895916,
                            "process_3": 750.2685253181571
                        },
                        "ram_power": {
                            "process_2": 0.7324762344360352,
                            "process_1": 0.7320928573608398,
                            "process_0": 0.6903748512268068,
                            "process_3": 0.7323460578918458
                        },
                        "cpu_energy": {
                            "process_2": 0.0011805720096799634,
                            "process_1": 0.0011923800219374244,
                            "process_0": 0.0011815608725573839,
                            "process_3": 0.001195559941284955
                        },
                        "gpu_energy": {
                            "process_2": 0.010296104070210532,
                            "process_1": 0.010347829944924669,
                            "process_0": 0.010296104070210532,
                            "process_3": 0.010363316068424666
                        },
                        "ram_energy": {
                            "process_2": 6.535364144582588e-06,
                            "process_1": 6.579310755549898e-06,
                            "process_0": 6.163259464661282e-06,
                            "process_3": 6.620165971957847e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.011483211444035078,
                            "process_1": 0.011546789277617642,
                            "process_0": 0.011483828202232578,
                            "process_3": 0.011565496175681582
                        },
                        "total_energy_joules": {
                            "process_2": 41339.56119852628,
                            "process_1": 41568.441399423515,
                            "process_0": 41341.78152803728,
                            "process_3": 41635.7862324537
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 906.8898286510645,
                        "ram_power_avg": 0.7218225002288818,
                        "cpu_energy_total": 0.004750072845459727,
                        "gpu_energy_total": 0.0413033541537704,
                        "ram_energy_total": 2.5898100336751614e-05,
                        "total_energy_kwh": 0.04607932509956688,
                        "total_energy_joules": 165885.57035844075
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09876687866580515,
                        "joules_per_token": 10.1248517064478,
                        "flops_per_joule": 317318632.3266338,
                        "joules_per_flop": 3.1514064984707363e-09
                    },
                    "per-process_emissions": [
                        0.0043745293996051635,
                        0.004398749375308441,
                        0.0043747643536405,
                        0.004405875768125899
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0463": {
            "setup": {
                "experiment_id": "0463",
                "cycle_id": 2,
                "date_time": "April 25, 2025 at 10:41:16 AM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "precis_float16_quant_False_quant_False_quant_False",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 30.16925900510978,
                        "average_latency_ms_per_batch": 3771.1573756387224,
                        "throughput_queries_per_sec": 4.242729328496951,
                        "throughput_tokens_per_sec": 543.0693540476097
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 12870114304,
                        "gpu_max_memory_allocated_bytes": 12870114304,
                        "gpu_current_memory_reserved_bytes": 19576913920,
                        "gpu_max_memory_reserved_bytes": 19576913920
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            7.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.0,
                        "cpu_memory_usage_bytes": 2663632896
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0463",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_1": 342.15213816710815,
                            "process_2": 133.09135179663497,
                            "process_3": 136.7227352542395,
                            "process_0": 377.40199557008515
                        },
                        "ram_power": {
                            "process_1": 0.97100830078125,
                            "process_2": 0.9647183418273926,
                            "process_3": 0.9645652770996095,
                            "process_0": 0.9288954734802246
                        },
                        "cpu_energy": {
                            "process_1": 0.0008672407287176612,
                            "process_2": 0.000897735578028005,
                            "process_3": 0.0008455892643778497,
                            "process_0": 0.0008556695055085584
                        },
                        "gpu_energy": {
                            "process_1": 0.004590398672313256,
                            "process_2": 0.004519355837706129,
                            "process_3": 0.0045317555698467515,
                            "process_0": 0.004552940586796339
                        },
                        "ram_energy": {
                            "process_1": 7.316590265314934e-06,
                            "process_2": 7.043580399254153e-06,
                            "process_3": 7.1152056660473094e-06,
                            "process_0": 6.886376536704744e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.005464955991296232,
                            "process_2": 0.005424134996133389,
                            "process_3": 0.005384460039890648,
                            "process_0": 0.0054154964688416015
                        },
                        "total_energy_joules": {
                            "process_1": 19673.841568666438,
                            "process_2": 19526.8859860802,
                            "process_3": 19384.056143606333,
                            "process_0": 19495.787287829764
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 247.34205519701692,
                        "ram_power_avg": 0.9572968482971191,
                        "cpu_energy_total": 0.0034662350766320746,
                        "gpu_energy_total": 0.018194450666662476,
                        "ram_energy_total": 2.836175286732114e-05,
                        "total_energy_kwh": 0.02168904749616187,
                        "total_energy_joules": 78080.57098618273
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20983453108839764,
                        "joules_per_token": 4.76565985023088,
                        "flops_per_joule": 674157240.9630433,
                        "joules_per_flop": 1.4833334706477166e-09
                    },
                    "per-process_emissions": [
                        0.0020818749848842996,
                        0.0020663242267770147,
                        0.0020512100521963422,
                        0.0020630333798052083
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0463": {
            "setup": {
                "experiment_id": "0463",
                "cycle_id": 2,
                "date_time": "April 25, 2025 at 10:41:16 AM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "precis_float16_quant_False_quant_False_quant_False",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 30.16925900510978,
                        "average_latency_ms_per_batch": 3771.1573756387224,
                        "throughput_queries_per_sec": 4.242729328496951,
                        "throughput_tokens_per_sec": 543.0693540476097
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 12870114304,
                        "gpu_max_memory_allocated_bytes": 12870114304,
                        "gpu_current_memory_reserved_bytes": 19576913920,
                        "gpu_max_memory_reserved_bytes": 19576913920
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            7.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.0,
                        "cpu_memory_usage_bytes": 2663632896
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0463",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_1": 342.15213816710815,
                            "process_2": 133.09135179663497,
                            "process_3": 136.7227352542395,
                            "process_0": 377.40199557008515
                        },
                        "ram_power": {
                            "process_1": 0.97100830078125,
                            "process_2": 0.9647183418273926,
                            "process_3": 0.9645652770996095,
                            "process_0": 0.9288954734802246
                        },
                        "cpu_energy": {
                            "process_1": 0.0008672407287176612,
                            "process_2": 0.000897735578028005,
                            "process_3": 0.0008455892643778497,
                            "process_0": 0.0008556695055085584
                        },
                        "gpu_energy": {
                            "process_1": 0.004590398672313256,
                            "process_2": 0.004519355837706129,
                            "process_3": 0.0045317555698467515,
                            "process_0": 0.004552940586796339
                        },
                        "ram_energy": {
                            "process_1": 7.316590265314934e-06,
                            "process_2": 7.043580399254153e-06,
                            "process_3": 7.1152056660473094e-06,
                            "process_0": 6.886376536704744e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.005464955991296232,
                            "process_2": 0.005424134996133389,
                            "process_3": 0.005384460039890648,
                            "process_0": 0.0054154964688416015
                        },
                        "total_energy_joules": {
                            "process_1": 19673.841568666438,
                            "process_2": 19526.8859860802,
                            "process_3": 19384.056143606333,
                            "process_0": 19495.787287829764
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 247.34205519701692,
                        "ram_power_avg": 0.9572968482971191,
                        "cpu_energy_total": 0.0034662350766320746,
                        "gpu_energy_total": 0.018194450666662476,
                        "ram_energy_total": 2.836175286732114e-05,
                        "total_energy_kwh": 0.02168904749616187,
                        "total_energy_joules": 78080.57098618273
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20983453108839764,
                        "joules_per_token": 4.76565985023088,
                        "flops_per_joule": 674157240.9630433,
                        "joules_per_flop": 1.4833334706477166e-09
                    },
                    "per-process_emissions": [
                        0.0020818749848842996,
                        0.0020663242267770147,
                        0.0020512100521963422,
                        0.0020630333798052083
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0464": {
            "setup": {
                "experiment_id": "0464",
                "cycle_id": 2,
                "date_time": "April 25, 2025 at 10:44:43 AM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_3",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 3,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 157.34693902806612,
                        "average_latency_ms_per_batch": 3659.231140187584,
                        "throughput_queries_per_sec": 0.8134889740509571,
                        "throughput_tokens_per_sec": 104.12658867852251
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38042337280,
                        "gpu_max_memory_reserved_bytes": 38042337280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.8,
                        "cpu_memory_usage_bytes": 1942470656
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0464",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_2": 1353.971546976407,
                            "process_1": 939.5168775529195,
                            "process_0": 845.5026802043863,
                            "process_3": 666.759453171169
                        },
                        "ram_power": {
                            "process_2": 0.7288470268249512,
                            "process_1": 0.7210721969604492,
                            "process_0": 0.6772069931030275,
                            "process_3": 0.7215571403503418
                        },
                        "cpu_energy": {
                            "process_2": 0.0045259918569772704,
                            "process_1": 0.0046666138065302246,
                            "process_0": 0.004704962065470679,
                            "process_3": 0.004612267166479796
                        },
                        "gpu_energy": {
                            "process_2": 0.032244745518001494,
                            "process_1": 0.03257742383969564,
                            "process_0": 0.03278737872988202,
                            "process_3": 0.03248060209557124
                        },
                        "ram_energy": {
                            "process_2": 2.8435234984412853e-05,
                            "process_1": 2.840232612075307e-05,
                            "process_0": 2.7285770023745857e-05,
                            "process_3": 2.8505920118873623e-05
                        },
                        "total_energy_kwh": {
                            "process_2": 0.03679917260996314,
                            "process_1": 0.03727243997234661,
                            "process_0": 0.037519626565376434,
                            "process_3": 0.0371213751821699
                        },
                        "total_energy_joules": {
                            "process_2": 132477.0213958673,
                            "process_1": 134180.7839004478,
                            "process_0": 135070.65563535516,
                            "process_3": 133636.95065581164
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 951.4376394762204,
                        "ram_power_avg": 0.7121708393096924,
                        "cpu_energy_total": 0.01850983489545797,
                        "gpu_energy_total": 0.1300901501831504,
                        "ram_energy_total": 0.0001126292512477854,
                        "total_energy_kwh": 0.14871261432985608,
                        "total_energy_joules": 535365.4115874819
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.03060339656874295,
                        "joules_per_token": 32.676111547087515,
                        "flops_per_joule": 98322717.8475697,
                        "joules_per_flop": 1.0170589482181585e-08
                    },
                    "per-process_emissions": [
                        0.014018644805765458,
                        0.014198936007465442,
                        0.014293101740080152,
                        0.014141387875647624
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0464": {
            "setup": {
                "experiment_id": "0464",
                "cycle_id": 2,
                "date_time": "April 25, 2025 at 10:44:43 AM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_3",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 3,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 157.34693902806612,
                        "average_latency_ms_per_batch": 3659.231140187584,
                        "throughput_queries_per_sec": 0.8134889740509571,
                        "throughput_tokens_per_sec": 104.12658867852251
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38042337280,
                        "gpu_max_memory_reserved_bytes": 38042337280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.8,
                        "cpu_memory_usage_bytes": 1942470656
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0464",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_2": 1353.971546976407,
                            "process_1": 939.5168775529195,
                            "process_0": 845.5026802043863,
                            "process_3": 666.759453171169
                        },
                        "ram_power": {
                            "process_2": 0.7288470268249512,
                            "process_1": 0.7210721969604492,
                            "process_0": 0.6772069931030275,
                            "process_3": 0.7215571403503418
                        },
                        "cpu_energy": {
                            "process_2": 0.0045259918569772704,
                            "process_1": 0.0046666138065302246,
                            "process_0": 0.004704962065470679,
                            "process_3": 0.004612267166479796
                        },
                        "gpu_energy": {
                            "process_2": 0.032244745518001494,
                            "process_1": 0.03257742383969564,
                            "process_0": 0.03278737872988202,
                            "process_3": 0.03248060209557124
                        },
                        "ram_energy": {
                            "process_2": 2.8435234984412853e-05,
                            "process_1": 2.840232612075307e-05,
                            "process_0": 2.7285770023745857e-05,
                            "process_3": 2.8505920118873623e-05
                        },
                        "total_energy_kwh": {
                            "process_2": 0.03679917260996314,
                            "process_1": 0.03727243997234661,
                            "process_0": 0.037519626565376434,
                            "process_3": 0.0371213751821699
                        },
                        "total_energy_joules": {
                            "process_2": 132477.0213958673,
                            "process_1": 134180.7839004478,
                            "process_0": 135070.65563535516,
                            "process_3": 133636.95065581164
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 951.4376394762204,
                        "ram_power_avg": 0.7121708393096924,
                        "cpu_energy_total": 0.01850983489545797,
                        "gpu_energy_total": 0.1300901501831504,
                        "ram_energy_total": 0.0001126292512477854,
                        "total_energy_kwh": 0.14871261432985608,
                        "total_energy_joules": 535365.4115874819
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.03060339656874295,
                        "joules_per_token": 32.676111547087515,
                        "flops_per_joule": 98322717.8475697,
                        "joules_per_flop": 1.0170589482181585e-08
                    },
                    "per-process_emissions": [
                        0.014018644805765458,
                        0.014198936007465442,
                        0.014293101740080152,
                        0.014141387875647624
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0465": {
            "setup": {
                "experiment_id": "0465",
                "cycle_id": 2,
                "date_time": "April 25, 2025 at 10:46:14 AM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_5_temp_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 5,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.26999272510875,
                        "average_latency_ms_per_batch": 4783.749090638594,
                        "throughput_queries_per_sec": 3.344657024615002,
                        "throughput_tokens_per_sec": 428.11609915072023
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38451281920,
                        "gpu_max_memory_reserved_bytes": 38451281920
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            27.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.5,
                        "cpu_memory_usage_bytes": 1981136896
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0465",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_2": 176.5625320553874,
                            "process_1": 154.89979248980646,
                            "process_0": 170.98064887467842,
                            "process_3": 14585.388668347336
                        },
                        "ram_power": {
                            "process_2": 0.7176847457885742,
                            "process_1": 0.7245168685913086,
                            "process_0": 0.6907238960266113,
                            "process_3": 0.7249760627746582
                        },
                        "cpu_energy": {
                            "process_2": 0.001228932224123128,
                            "process_1": 0.0012675289697817792,
                            "process_0": 0.00122887720556173,
                            "process_3": 0.0012023426677060345
                        },
                        "gpu_energy": {
                            "process_2": 0.01025499487065673,
                            "process_1": 0.01025790348409572,
                            "process_0": 0.01025790348409572,
                            "process_3": 0.01025499487065673
                        },
                        "ram_energy": {
                            "process_2": 6.807231237007232e-06,
                            "process_1": 7.0838549077713735e-06,
                            "process_0": 6.543760306175642e-06,
                            "process_3": 6.974990990126875e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.011490734326016858,
                            "process_1": 0.011532516308785273,
                            "process_0": 0.011493324449963623,
                            "process_3": 0.011464312529352885
                        },
                        "total_energy_joules": {
                            "process_2": 41366.64357366069,
                            "process_1": 41517.05871162698,
                            "process_0": 41375.96801986904,
                            "process_3": 41271.525105670386
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 3771.957910441802,
                        "ram_power_avg": 0.7144753932952881,
                        "cpu_energy_total": 0.004927681067172672,
                        "gpu_energy_total": 0.0410257967095049,
                        "ram_energy_total": 2.7409837441081122e-05,
                        "total_energy_kwh": 0.045980887614118636,
                        "total_energy_joules": 165531.1954108271
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09897832223912251,
                        "joules_per_token": 10.103222376149114,
                        "flops_per_joule": 317997959.1050607,
                        "joules_per_flop": 3.1446742702824026e-09
                    },
                    "per-process_emissions": [
                        0.004377395241496123,
                        0.00439331208783175,
                        0.004378381949213642,
                        0.004367329858056982
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0465": {
            "setup": {
                "experiment_id": "0465",
                "cycle_id": 2,
                "date_time": "April 25, 2025 at 10:46:14 AM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_5_temp_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 5,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.26999272510875,
                        "average_latency_ms_per_batch": 4783.749090638594,
                        "throughput_queries_per_sec": 3.344657024615002,
                        "throughput_tokens_per_sec": 428.11609915072023
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38451281920,
                        "gpu_max_memory_reserved_bytes": 38451281920
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            27.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.5,
                        "cpu_memory_usage_bytes": 1981136896
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0465",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_2": 176.5625320553874,
                            "process_1": 154.89979248980646,
                            "process_0": 170.98064887467842,
                            "process_3": 14585.388668347336
                        },
                        "ram_power": {
                            "process_2": 0.7176847457885742,
                            "process_1": 0.7245168685913086,
                            "process_0": 0.6907238960266113,
                            "process_3": 0.7249760627746582
                        },
                        "cpu_energy": {
                            "process_2": 0.001228932224123128,
                            "process_1": 0.0012675289697817792,
                            "process_0": 0.00122887720556173,
                            "process_3": 0.0012023426677060345
                        },
                        "gpu_energy": {
                            "process_2": 0.01025499487065673,
                            "process_1": 0.01025790348409572,
                            "process_0": 0.01025790348409572,
                            "process_3": 0.01025499487065673
                        },
                        "ram_energy": {
                            "process_2": 6.807231237007232e-06,
                            "process_1": 7.0838549077713735e-06,
                            "process_0": 6.543760306175642e-06,
                            "process_3": 6.974990990126875e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.011490734326016858,
                            "process_1": 0.011532516308785273,
                            "process_0": 0.011493324449963623,
                            "process_3": 0.011464312529352885
                        },
                        "total_energy_joules": {
                            "process_2": 41366.64357366069,
                            "process_1": 41517.05871162698,
                            "process_0": 41375.96801986904,
                            "process_3": 41271.525105670386
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 3771.957910441802,
                        "ram_power_avg": 0.7144753932952881,
                        "cpu_energy_total": 0.004927681067172672,
                        "gpu_energy_total": 0.0410257967095049,
                        "ram_energy_total": 2.7409837441081122e-05,
                        "total_energy_kwh": 0.045980887614118636,
                        "total_energy_joules": 165531.1954108271
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09897832223912251,
                        "joules_per_token": 10.103222376149114,
                        "flops_per_joule": 317997959.1050607,
                        "joules_per_flop": 3.1446742702824026e-09
                    },
                    "per-process_emissions": [
                        0.004377395241496123,
                        0.00439331208783175,
                        0.004378381949213642,
                        0.004367329858056982
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0466": {
            "setup": {
                "experiment_id": "0466",
                "cycle_id": 2,
                "date_time": "April 25, 2025 at 10:47:43 AM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_topp_0.1_temp_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_p",
                    "decoder_temperature": 0.8,
                    "decoder_top_k": null,
                    "decoder_top_p": 0.1
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.916232665011194,
                        "average_latency_ms_per_batch": 4864.529083126399,
                        "throughput_queries_per_sec": 3.289115909595284,
                        "throughput_tokens_per_sec": 421.0068364281964
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 32331792384,
                        "gpu_max_memory_reserved_bytes": 32331792384
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.8,
                        "cpu_memory_usage_bytes": 1967845376
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0466",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_2": 2306.7514429014636,
                            "process_0": 2837.650088618102,
                            "process_3": 529.9728794047198,
                            "process_1": 2820.9228120752064
                        },
                        "ram_power": {
                            "process_2": 0.7261762619018555,
                            "process_0": 0.6861076354980469,
                            "process_3": 0.734410285949707,
                            "process_1": 0.7086725234985352
                        },
                        "cpu_energy": {
                            "process_2": 0.0011923136601544684,
                            "process_0": 0.0012055905318411532,
                            "process_3": 0.0011527387165897384,
                            "process_1": 0.0011575015754369815
                        },
                        "gpu_energy": {
                            "process_2": 0.010380588304466087,
                            "process_0": 0.010373547743276035,
                            "process_3": 0.010404704712646762,
                            "process_1": 0.01037199190869842
                        },
                        "ram_energy": {
                            "process_2": 7.06180133192088e-06,
                            "process_0": 6.769878821261189e-06,
                            "process_3": 7.230292687435395e-06,
                            "process_1": 7.0470194455231925e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.011579963765952477,
                            "process_0": 0.011585908153938448,
                            "process_3": 0.011564673721923933,
                            "process_1": 0.011536540503580923
                        },
                        "total_energy_joules": {
                            "process_2": 41687.869557428916,
                            "process_0": 41709.26935417841,
                            "process_3": 41632.82539892616,
                            "process_1": 41531.54581289132
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 2123.824305749873,
                        "ram_power_avg": 0.7138416767120361,
                        "cpu_energy_total": 0.004708144484022342,
                        "gpu_energy_total": 0.0415308326690873,
                        "ram_energy_total": 2.8108992286140658e-05,
                        "total_energy_kwh": 0.046267086145395785,
                        "total_energy_joules": 166561.5101234248
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09836606301095126,
                        "joules_per_token": 10.16610779561919,
                        "flops_per_joule": 316030890.1489783,
                        "joules_per_flop": 3.1642476453127597e-09
                    },
                    "per-process_emissions": [
                        0.004411387196639596,
                        0.004413651711242852,
                        0.004405562454366923,
                        0.004394845104839153
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0466": {
            "setup": {
                "experiment_id": "0466",
                "cycle_id": 2,
                "date_time": "April 25, 2025 at 10:47:43 AM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_topp_0.1_temp_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_p",
                    "decoder_temperature": 0.8,
                    "decoder_top_k": null,
                    "decoder_top_p": 0.1
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.916232665011194,
                        "average_latency_ms_per_batch": 4864.529083126399,
                        "throughput_queries_per_sec": 3.289115909595284,
                        "throughput_tokens_per_sec": 421.0068364281964
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 32331792384,
                        "gpu_max_memory_reserved_bytes": 32331792384
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.8,
                        "cpu_memory_usage_bytes": 1967845376
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0466",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_2": 2306.7514429014636,
                            "process_0": 2837.650088618102,
                            "process_3": 529.9728794047198,
                            "process_1": 2820.9228120752064
                        },
                        "ram_power": {
                            "process_2": 0.7261762619018555,
                            "process_0": 0.6861076354980469,
                            "process_3": 0.734410285949707,
                            "process_1": 0.7086725234985352
                        },
                        "cpu_energy": {
                            "process_2": 0.0011923136601544684,
                            "process_0": 0.0012055905318411532,
                            "process_3": 0.0011527387165897384,
                            "process_1": 0.0011575015754369815
                        },
                        "gpu_energy": {
                            "process_2": 0.010380588304466087,
                            "process_0": 0.010373547743276035,
                            "process_3": 0.010404704712646762,
                            "process_1": 0.01037199190869842
                        },
                        "ram_energy": {
                            "process_2": 7.06180133192088e-06,
                            "process_0": 6.769878821261189e-06,
                            "process_3": 7.230292687435395e-06,
                            "process_1": 7.0470194455231925e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.011579963765952477,
                            "process_0": 0.011585908153938448,
                            "process_3": 0.011564673721923933,
                            "process_1": 0.011536540503580923
                        },
                        "total_energy_joules": {
                            "process_2": 41687.869557428916,
                            "process_0": 41709.26935417841,
                            "process_3": 41632.82539892616,
                            "process_1": 41531.54581289132
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 2123.824305749873,
                        "ram_power_avg": 0.7138416767120361,
                        "cpu_energy_total": 0.004708144484022342,
                        "gpu_energy_total": 0.0415308326690873,
                        "ram_energy_total": 2.8108992286140658e-05,
                        "total_energy_kwh": 0.046267086145395785,
                        "total_energy_joules": 166561.5101234248
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09836606301095126,
                        "joules_per_token": 10.16610779561919,
                        "flops_per_joule": 316030890.1489783,
                        "joules_per_flop": 3.1642476453127597e-09
                    },
                    "per-process_emissions": [
                        0.004411387196639596,
                        0.004413651711242852,
                        0.004405562454366923,
                        0.004394845104839153
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0467": {
            "setup": {
                "experiment_id": "0467",
                "cycle_id": 2,
                "date_time": "April 25, 2025 at 10:49:14 AM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_20_temp_1.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 1.4,
                    "decoder_top_k": 20,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.36708507902222,
                        "average_latency_ms_per_batch": 4795.885634877777,
                        "throughput_queries_per_sec": 3.3361929825100507,
                        "throughput_tokens_per_sec": 427.0327017612865
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38449184768,
                        "gpu_max_memory_reserved_bytes": 38449184768
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.5,
                        "cpu_memory_usage_bytes": 1976037376
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0467",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_2": 196.57534967379794,
                            "process_0": 170.33460278661724,
                            "process_3": 14005.54784982647,
                            "process_1": 168.9031156345602
                        },
                        "ram_power": {
                            "process_2": 0.7327680587768555,
                            "process_0": 0.6889915466308594,
                            "process_3": 0.730865478515625,
                            "process_1": 0.7325291633605958
                        },
                        "cpu_energy": {
                            "process_2": 0.001102500780318223,
                            "process_0": 0.0011088489740614022,
                            "process_3": 0.0010690501551853236,
                            "process_1": 0.001103778619721197
                        },
                        "gpu_energy": {
                            "process_2": 0.010246978475354496,
                            "process_0": 0.010246978475354496,
                            "process_3": 0.010250864311798225,
                            "process_1": 0.010246978475354496
                        },
                        "ram_energy": {
                            "process_2": 6.157067545899149e-06,
                            "process_0": 5.777468814734596e-06,
                            "process_3": 6.199765180553043e-06,
                            "process_1": 6.137018627436091e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.011355636323218623,
                            "process_0": 0.011361604918230631,
                            "process_3": 0.0113261142321641,
                            "process_1": 0.011356894113703128
                        },
                        "total_energy_joules": {
                            "process_2": 40880.29076358704,
                            "process_0": 40901.77770563027,
                            "process_3": 40774.011235790764,
                            "process_1": 40884.818809331264
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 3635.3402294803614,
                        "ram_power_avg": 0.7212885618209839,
                        "cpu_energy_total": 0.004384178529286146,
                        "gpu_energy_total": 0.04099179973786171,
                        "ram_energy_total": 2.427132016862288e-05,
                        "total_energy_kwh": 0.045400249587316484,
                        "total_energy_joules": 163440.89851433935
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.10024418703597963,
                        "joules_per_token": 9.975640778463095,
                        "flops_per_joule": 322064934.70938545,
                        "joules_per_flop": 3.1049639132628568e-09
                    },
                    "per-process_emissions": [
                        0.004325929657330134,
                        0.004328203393599959,
                        0.004314683216742914,
                        0.004326408812615206
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0467": {
            "setup": {
                "experiment_id": "0467",
                "cycle_id": 2,
                "date_time": "April 25, 2025 at 10:49:14 AM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_20_temp_1.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 1.4,
                    "decoder_top_k": 20,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.36708507902222,
                        "average_latency_ms_per_batch": 4795.885634877777,
                        "throughput_queries_per_sec": 3.3361929825100507,
                        "throughput_tokens_per_sec": 427.0327017612865
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38449184768,
                        "gpu_max_memory_reserved_bytes": 38449184768
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.5,
                        "cpu_memory_usage_bytes": 1976037376
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0467",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_2": 196.57534967379794,
                            "process_0": 170.33460278661724,
                            "process_3": 14005.54784982647,
                            "process_1": 168.9031156345602
                        },
                        "ram_power": {
                            "process_2": 0.7327680587768555,
                            "process_0": 0.6889915466308594,
                            "process_3": 0.730865478515625,
                            "process_1": 0.7325291633605958
                        },
                        "cpu_energy": {
                            "process_2": 0.001102500780318223,
                            "process_0": 0.0011088489740614022,
                            "process_3": 0.0010690501551853236,
                            "process_1": 0.001103778619721197
                        },
                        "gpu_energy": {
                            "process_2": 0.010246978475354496,
                            "process_0": 0.010246978475354496,
                            "process_3": 0.010250864311798225,
                            "process_1": 0.010246978475354496
                        },
                        "ram_energy": {
                            "process_2": 6.157067545899149e-06,
                            "process_0": 5.777468814734596e-06,
                            "process_3": 6.199765180553043e-06,
                            "process_1": 6.137018627436091e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.011355636323218623,
                            "process_0": 0.011361604918230631,
                            "process_3": 0.0113261142321641,
                            "process_1": 0.011356894113703128
                        },
                        "total_energy_joules": {
                            "process_2": 40880.29076358704,
                            "process_0": 40901.77770563027,
                            "process_3": 40774.011235790764,
                            "process_1": 40884.818809331264
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 3635.3402294803614,
                        "ram_power_avg": 0.7212885618209839,
                        "cpu_energy_total": 0.004384178529286146,
                        "gpu_energy_total": 0.04099179973786171,
                        "ram_energy_total": 2.427132016862288e-05,
                        "total_energy_kwh": 0.045400249587316484,
                        "total_energy_joules": 163440.89851433935
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.10024418703597963,
                        "joules_per_token": 9.975640778463095,
                        "flops_per_joule": 322064934.70938545,
                        "joules_per_flop": 3.1049639132628568e-09
                    },
                    "per-process_emissions": [
                        0.004325929657330134,
                        0.004328203393599959,
                        0.004314683216742914,
                        0.004326408812615206
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0468": {
            "setup": {
                "experiment_id": "0468",
                "cycle_id": 2,
                "date_time": "April 25, 2025 at 10:50:40 AM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_24",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 24,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 32.194837200047914,
                        "average_latency_ms_per_batch": 5365.806200007985,
                        "throughput_queries_per_sec": 3.975792739831264,
                        "throughput_tokens_per_sec": 508.9014706984018
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 39118176256,
                        "gpu_max_memory_reserved_bytes": 39118176256
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            28.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.7,
                        "cpu_memory_usage_bytes": 1972928512
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0468",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_1": 653.1731355416641,
                            "process_0": 759.4295493795973,
                            "process_2": 2769.346973481519,
                            "process_3": 987.9821346536859
                        },
                        "ram_power": {
                            "process_1": 0.7232651710510254,
                            "process_0": 0.6866197586059571,
                            "process_2": 0.7310099601745605,
                            "process_3": 0.7301959991455078
                        },
                        "cpu_energy": {
                            "process_1": 0.0010126678713149888,
                            "process_0": 0.000993143602661803,
                            "process_2": 0.0010082836572200906,
                            "process_3": 0.0009648367850695649
                        },
                        "gpu_energy": {
                            "process_1": 0.008596822155226391,
                            "process_0": 0.008592623818536538,
                            "process_2": 0.008562133238587677,
                            "process_3": 0.008644631637919176
                        },
                        "ram_energy": {
                            "process_1": 6.0583160951791235e-06,
                            "process_0": 5.973309262709843e-06,
                            "process_2": 6.294283891682105e-06,
                            "process_3": 6.204488023637787e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.009615548342636558,
                            "process_0": 0.009591740730461052,
                            "process_2": 0.009576711179699446,
                            "process_3": 0.009615672911012377
                        },
                        "total_energy_joules": {
                            "process_1": 34615.974033491606,
                            "process_0": 34530.26662965979,
                            "process_2": 34476.16024691801,
                            "process_3": 34616.422479644556
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1292.4829482641164,
                        "ram_power_avg": 0.7177727222442627,
                        "cpu_energy_total": 0.003978931916266447,
                        "gpu_energy_total": 0.03439621085026978,
                        "ram_energy_total": 2.453039727320886e-05,
                        "total_energy_kwh": 0.038399673163809434,
                        "total_energy_joules": 138238.82338971397
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.11851952728077904,
                        "joules_per_token": 8.437428185407347,
                        "flops_per_joule": 380780022.7036706,
                        "joules_per_flop": 2.6261881936443307e-09
                    },
                    "per-process_emissions": [
                        0.0036630431411273966,
                        0.003653973631269138,
                        0.003648248123906504,
                        0.0036630905954501652
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0468": {
            "setup": {
                "experiment_id": "0468",
                "cycle_id": 2,
                "date_time": "April 25, 2025 at 10:50:40 AM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_24",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 24,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 32.194837200047914,
                        "average_latency_ms_per_batch": 5365.806200007985,
                        "throughput_queries_per_sec": 3.975792739831264,
                        "throughput_tokens_per_sec": 508.9014706984018
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 39118176256,
                        "gpu_max_memory_reserved_bytes": 39118176256
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            28.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.7,
                        "cpu_memory_usage_bytes": 1972928512
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0468",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_1": 653.1731355416641,
                            "process_0": 759.4295493795973,
                            "process_2": 2769.346973481519,
                            "process_3": 987.9821346536859
                        },
                        "ram_power": {
                            "process_1": 0.7232651710510254,
                            "process_0": 0.6866197586059571,
                            "process_2": 0.7310099601745605,
                            "process_3": 0.7301959991455078
                        },
                        "cpu_energy": {
                            "process_1": 0.0010126678713149888,
                            "process_0": 0.000993143602661803,
                            "process_2": 0.0010082836572200906,
                            "process_3": 0.0009648367850695649
                        },
                        "gpu_energy": {
                            "process_1": 0.008596822155226391,
                            "process_0": 0.008592623818536538,
                            "process_2": 0.008562133238587677,
                            "process_3": 0.008644631637919176
                        },
                        "ram_energy": {
                            "process_1": 6.0583160951791235e-06,
                            "process_0": 5.973309262709843e-06,
                            "process_2": 6.294283891682105e-06,
                            "process_3": 6.204488023637787e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.009615548342636558,
                            "process_0": 0.009591740730461052,
                            "process_2": 0.009576711179699446,
                            "process_3": 0.009615672911012377
                        },
                        "total_energy_joules": {
                            "process_1": 34615.974033491606,
                            "process_0": 34530.26662965979,
                            "process_2": 34476.16024691801,
                            "process_3": 34616.422479644556
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1292.4829482641164,
                        "ram_power_avg": 0.7177727222442627,
                        "cpu_energy_total": 0.003978931916266447,
                        "gpu_energy_total": 0.03439621085026978,
                        "ram_energy_total": 2.453039727320886e-05,
                        "total_energy_kwh": 0.038399673163809434,
                        "total_energy_joules": 138238.82338971397
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.11851952728077904,
                        "joules_per_token": 8.437428185407347,
                        "flops_per_joule": 380780022.7036706,
                        "joules_per_flop": 2.6261881936443307e-09
                    },
                    "per-process_emissions": [
                        0.0036630431411273966,
                        0.003653973631269138,
                        0.003648248123906504,
                        0.0036630905954501652
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0469": {
            "setup": {
                "experiment_id": "0469",
                "cycle_id": 2,
                "date_time": "April 25, 2025 at 10:52:14 AM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_burst_0.1_0.2_2.0_5",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.1,
                    "delay_max": 0.2,
                    "simulate_burst": true,
                    "burst_interval": 2.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 41.278059623902664,
                        "average_latency_ms_per_batch": 5159.757452987833,
                        "throughput_queries_per_sec": 3.1009209533163165,
                        "throughput_tokens_per_sec": 396.9178820244885
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38352715776,
                        "gpu_max_memory_reserved_bytes": 38352715776
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            4.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.2,
                        "cpu_memory_usage_bytes": 1972019200
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0469",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_3": 901.8157076780151,
                            "process_2": 954.9026048964262,
                            "process_1": 846.4955506375096,
                            "process_0": 1077.1741103805714
                        },
                        "ram_power": {
                            "process_3": 0.7234969139099122,
                            "process_2": 0.7238988876342773,
                            "process_1": 0.7233953475952148,
                            "process_0": 0.6863451004028321
                        },
                        "cpu_energy": {
                            "process_3": 0.0013359653172519761,
                            "process_2": 0.0012425488681492424,
                            "process_1": 0.001320508455624804,
                            "process_0": 0.0012761997297184283
                        },
                        "gpu_energy": {
                            "process_3": 0.010405728324574426,
                            "process_2": 0.010381542194117799,
                            "process_1": 0.010408808049261253,
                            "process_0": 0.010387164976391361
                        },
                        "ram_energy": {
                            "process_3": 7.534778569931451e-06,
                            "process_2": 6.938871679359133e-06,
                            "process_1": 7.426517709948667e-06,
                            "process_0": 6.800536679692186e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.011749228420396331,
                            "process_2": 0.011631029933946397,
                            "process_1": 0.011736743022596008,
                            "process_0": 0.011670165242789481
                        },
                        "total_energy_joules": {
                            "process_3": 42297.22231342679,
                            "process_2": 41871.70776220703,
                            "process_1": 42252.27488134563,
                            "process_0": 42012.59487404213
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 945.0969933981305,
                        "ram_power_avg": 0.7142840623855591,
                        "cpu_energy_total": 0.005175222370744451,
                        "gpu_energy_total": 0.04158324354434484,
                        "ram_energy_total": 2.8700704638931435e-05,
                        "total_energy_kwh": 0.04678716661972822,
                        "total_energy_joules": 168433.79983102158
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09727263777482296,
                        "joules_per_token": 10.280383290467626,
                        "flops_per_joule": 312517929.07167554,
                        "joules_per_flop": 3.1998164168387643e-09
                    },
                    "per-process_emissions": [
                        0.004475868566749982,
                        0.0044308408533368804,
                        0.004471112254457949,
                        0.004445749449240653
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0469": {
            "setup": {
                "experiment_id": "0469",
                "cycle_id": 2,
                "date_time": "April 25, 2025 at 10:52:14 AM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_burst_0.1_0.2_2.0_5",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.1,
                    "delay_max": 0.2,
                    "simulate_burst": true,
                    "burst_interval": 2.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 41.278059623902664,
                        "average_latency_ms_per_batch": 5159.757452987833,
                        "throughput_queries_per_sec": 3.1009209533163165,
                        "throughput_tokens_per_sec": 396.9178820244885
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38352715776,
                        "gpu_max_memory_reserved_bytes": 38352715776
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            4.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.2,
                        "cpu_memory_usage_bytes": 1972019200
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0469",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_3": 901.8157076780151,
                            "process_2": 954.9026048964262,
                            "process_1": 846.4955506375096,
                            "process_0": 1077.1741103805714
                        },
                        "ram_power": {
                            "process_3": 0.7234969139099122,
                            "process_2": 0.7238988876342773,
                            "process_1": 0.7233953475952148,
                            "process_0": 0.6863451004028321
                        },
                        "cpu_energy": {
                            "process_3": 0.0013359653172519761,
                            "process_2": 0.0012425488681492424,
                            "process_1": 0.001320508455624804,
                            "process_0": 0.0012761997297184283
                        },
                        "gpu_energy": {
                            "process_3": 0.010405728324574426,
                            "process_2": 0.010381542194117799,
                            "process_1": 0.010408808049261253,
                            "process_0": 0.010387164976391361
                        },
                        "ram_energy": {
                            "process_3": 7.534778569931451e-06,
                            "process_2": 6.938871679359133e-06,
                            "process_1": 7.426517709948667e-06,
                            "process_0": 6.800536679692186e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.011749228420396331,
                            "process_2": 0.011631029933946397,
                            "process_1": 0.011736743022596008,
                            "process_0": 0.011670165242789481
                        },
                        "total_energy_joules": {
                            "process_3": 42297.22231342679,
                            "process_2": 41871.70776220703,
                            "process_1": 42252.27488134563,
                            "process_0": 42012.59487404213
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 945.0969933981305,
                        "ram_power_avg": 0.7142840623855591,
                        "cpu_energy_total": 0.005175222370744451,
                        "gpu_energy_total": 0.04158324354434484,
                        "ram_energy_total": 2.8700704638931435e-05,
                        "total_energy_kwh": 0.04678716661972822,
                        "total_energy_joules": 168433.79983102158
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09727263777482296,
                        "joules_per_token": 10.280383290467626,
                        "flops_per_joule": 312517929.07167554,
                        "joules_per_flop": 3.1998164168387643e-09
                    },
                    "per-process_emissions": [
                        0.004475868566749982,
                        0.0044308408533368804,
                        0.004471112254457949,
                        0.004445749449240653
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0470": {
            "setup": {
                "experiment_id": "0470",
                "cycle_id": 2,
                "date_time": "April 25, 2025 at 10:53:46 AM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_topp_0.5_temp_0.6",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_p",
                    "decoder_temperature": 0.6,
                    "decoder_top_k": null,
                    "decoder_top_p": 0.5
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 39.076951410854235,
                        "average_latency_ms_per_batch": 4884.618926356779,
                        "throughput_queries_per_sec": 3.275588176114629,
                        "throughput_tokens_per_sec": 419.2752865426725
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38403047424,
                        "gpu_max_memory_reserved_bytes": 38403047424
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            27.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.8,
                        "cpu_memory_usage_bytes": 1979899904
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0470",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_1": 451.94942694510837,
                            "process_3": 2366.5407381332475,
                            "process_0": 249.55126398282178,
                            "process_2": 0.0
                        },
                        "ram_power": {
                            "process_1": 0.7268900871276855,
                            "process_3": 0.7264037132263185,
                            "process_0": 0.6890602111816406,
                            "process_2": 0.7119741439819336
                        },
                        "cpu_energy": {
                            "process_1": 0.001132448961685441,
                            "process_3": 0.001125453856184322,
                            "process_0": 0.0012324281351466194,
                            "process_2": 0.0010974134044099628
                        },
                        "gpu_energy": {
                            "process_1": 0.01038912136684722,
                            "process_3": 0.010387374421005902,
                            "process_0": 0.010376448578929498,
                            "process_2": 0.010387374421005902
                        },
                        "ram_energy": {
                            "process_1": 7.45465601560989e-06,
                            "process_3": 7.433167902271241e-06,
                            "process_0": 7.000548734074898e-06,
                            "process_2": 7.130894842647037e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.011529024984548273,
                            "process_3": 0.0115202614450925,
                            "process_0": 0.01161587726281019,
                            "process_2": 0.011491918720258508
                        },
                        "total_energy_joules": {
                            "process_1": 41504.48994437378,
                            "process_3": 41472.941202333,
                            "process_0": 41817.158146116686,
                            "process_2": 41370.907392930625
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 767.0103572652944,
                        "ram_power_avg": 0.7135820388793945,
                        "cpu_energy_total": 0.004587744357426345,
                        "gpu_energy_total": 0.04154031878778852,
                        "ram_energy_total": 2.9019267494603068e-05,
                        "total_energy_kwh": 0.04615708241270947,
                        "total_energy_joules": 166165.49668575407
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09860049364510855,
                        "joules_per_token": 10.141937053573857,
                        "flops_per_joule": 316784069.8506267,
                        "joules_per_flop": 3.1567243910703284e-09
                    },
                    "per-process_emissions": [
                        0.004391982067863665,
                        0.004388643597507988,
                        0.004425068443267542,
                        0.004377846436482479
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0470": {
            "setup": {
                "experiment_id": "0470",
                "cycle_id": 2,
                "date_time": "April 25, 2025 at 10:53:46 AM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_topp_0.5_temp_0.6",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_p",
                    "decoder_temperature": 0.6,
                    "decoder_top_k": null,
                    "decoder_top_p": 0.5
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 39.076951410854235,
                        "average_latency_ms_per_batch": 4884.618926356779,
                        "throughput_queries_per_sec": 3.275588176114629,
                        "throughput_tokens_per_sec": 419.2752865426725
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38403047424,
                        "gpu_max_memory_reserved_bytes": 38403047424
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            27.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.8,
                        "cpu_memory_usage_bytes": 1979899904
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0470",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_1": 451.94942694510837,
                            "process_3": 2366.5407381332475,
                            "process_0": 249.55126398282178,
                            "process_2": 0.0
                        },
                        "ram_power": {
                            "process_1": 0.7268900871276855,
                            "process_3": 0.7264037132263185,
                            "process_0": 0.6890602111816406,
                            "process_2": 0.7119741439819336
                        },
                        "cpu_energy": {
                            "process_1": 0.001132448961685441,
                            "process_3": 0.001125453856184322,
                            "process_0": 0.0012324281351466194,
                            "process_2": 0.0010974134044099628
                        },
                        "gpu_energy": {
                            "process_1": 0.01038912136684722,
                            "process_3": 0.010387374421005902,
                            "process_0": 0.010376448578929498,
                            "process_2": 0.010387374421005902
                        },
                        "ram_energy": {
                            "process_1": 7.45465601560989e-06,
                            "process_3": 7.433167902271241e-06,
                            "process_0": 7.000548734074898e-06,
                            "process_2": 7.130894842647037e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.011529024984548273,
                            "process_3": 0.0115202614450925,
                            "process_0": 0.01161587726281019,
                            "process_2": 0.011491918720258508
                        },
                        "total_energy_joules": {
                            "process_1": 41504.48994437378,
                            "process_3": 41472.941202333,
                            "process_0": 41817.158146116686,
                            "process_2": 41370.907392930625
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 767.0103572652944,
                        "ram_power_avg": 0.7135820388793945,
                        "cpu_energy_total": 0.004587744357426345,
                        "gpu_energy_total": 0.04154031878778852,
                        "ram_energy_total": 2.9019267494603068e-05,
                        "total_energy_kwh": 0.04615708241270947,
                        "total_energy_joules": 166165.49668575407
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09860049364510855,
                        "joules_per_token": 10.141937053573857,
                        "flops_per_joule": 316784069.8506267,
                        "joules_per_flop": 3.1567243910703284e-09
                    },
                    "per-process_emissions": [
                        0.004391982067863665,
                        0.004388643597507988,
                        0.004425068443267542,
                        0.004377846436482479
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0471": {
            "setup": {
                "experiment_id": "0471",
                "cycle_id": 2,
                "date_time": "April 25, 2025 at 10:55:17 AM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_300_temp_1.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 1.4,
                    "decoder_top_k": 300,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.40164277999429,
                        "average_latency_ms_per_batch": 4800.205347499286,
                        "throughput_queries_per_sec": 3.3331907370036484,
                        "throughput_tokens_per_sec": 426.648414336467
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38449184768,
                        "gpu_max_memory_reserved_bytes": 38449184768
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.7,
                        "cpu_memory_usage_bytes": 1975406592
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0471",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_3": 157.37249616116813,
                            "process_2": 195.00628392790813,
                            "process_1": 208.85067058256982,
                            "process_0": 198.37173951470547
                        },
                        "ram_power": {
                            "process_3": 0.7250361442565918,
                            "process_2": 0.7250046730041504,
                            "process_1": 0.7182168960571289,
                            "process_0": 0.688725471496582
                        },
                        "cpu_energy": {
                            "process_3": 0.0011153932687811901,
                            "process_2": 0.0011057282288657005,
                            "process_1": 0.0010931457813148882,
                            "process_0": 0.00111136163584888
                        },
                        "gpu_energy": {
                            "process_3": 0.010255106259632996,
                            "process_2": 0.010256226260528578,
                            "process_1": 0.010258657095807422,
                            "process_0": 0.010256226260528578
                        },
                        "ram_energy": {
                            "process_3": 6.544201147575281e-06,
                            "process_2": 6.460948904884481e-06,
                            "process_1": 7.1013722100517445e-06,
                            "process_0": 6.168034480445387e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.01137704372956176,
                            "process_2": 0.01136841543829916,
                            "process_1": 0.011358904249332363,
                            "process_0": 0.01137375593085791
                        },
                        "total_energy_joules": {
                            "process_3": 40957.357426422335,
                            "process_2": 40926.295577876976,
                            "process_1": 40892.05529759651,
                            "process_0": 40945.52135108847
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 189.9002975465879,
                        "ram_power_avg": 0.7142457962036133,
                        "cpu_energy_total": 0.004425628914810659,
                        "gpu_energy_total": 0.041026215876497574,
                        "ram_energy_total": 2.627455674295689e-05,
                        "total_energy_kwh": 0.045478119348051196,
                        "total_energy_joules": 163721.2296529843
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.10007254425541968,
                        "joules_per_token": 9.99275083331203,
                        "flops_per_joule": 321513480.0809536,
                        "joules_per_flop": 3.1102894962544363e-09
                    },
                    "per-process_emissions": [
                        0.004334084808776553,
                        0.004330797861220066,
                        0.004327174573783164,
                        0.0043328323218603205
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0471": {
            "setup": {
                "experiment_id": "0471",
                "cycle_id": 2,
                "date_time": "April 25, 2025 at 10:55:17 AM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_300_temp_1.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 1.4,
                    "decoder_top_k": 300,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.40164277999429,
                        "average_latency_ms_per_batch": 4800.205347499286,
                        "throughput_queries_per_sec": 3.3331907370036484,
                        "throughput_tokens_per_sec": 426.648414336467
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38449184768,
                        "gpu_max_memory_reserved_bytes": 38449184768
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.7,
                        "cpu_memory_usage_bytes": 1975406592
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0471",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_3": 157.37249616116813,
                            "process_2": 195.00628392790813,
                            "process_1": 208.85067058256982,
                            "process_0": 198.37173951470547
                        },
                        "ram_power": {
                            "process_3": 0.7250361442565918,
                            "process_2": 0.7250046730041504,
                            "process_1": 0.7182168960571289,
                            "process_0": 0.688725471496582
                        },
                        "cpu_energy": {
                            "process_3": 0.0011153932687811901,
                            "process_2": 0.0011057282288657005,
                            "process_1": 0.0010931457813148882,
                            "process_0": 0.00111136163584888
                        },
                        "gpu_energy": {
                            "process_3": 0.010255106259632996,
                            "process_2": 0.010256226260528578,
                            "process_1": 0.010258657095807422,
                            "process_0": 0.010256226260528578
                        },
                        "ram_energy": {
                            "process_3": 6.544201147575281e-06,
                            "process_2": 6.460948904884481e-06,
                            "process_1": 7.1013722100517445e-06,
                            "process_0": 6.168034480445387e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.01137704372956176,
                            "process_2": 0.01136841543829916,
                            "process_1": 0.011358904249332363,
                            "process_0": 0.01137375593085791
                        },
                        "total_energy_joules": {
                            "process_3": 40957.357426422335,
                            "process_2": 40926.295577876976,
                            "process_1": 40892.05529759651,
                            "process_0": 40945.52135108847
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 189.9002975465879,
                        "ram_power_avg": 0.7142457962036133,
                        "cpu_energy_total": 0.004425628914810659,
                        "gpu_energy_total": 0.041026215876497574,
                        "ram_energy_total": 2.627455674295689e-05,
                        "total_energy_kwh": 0.045478119348051196,
                        "total_energy_joules": 163721.2296529843
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.10007254425541968,
                        "joules_per_token": 9.99275083331203,
                        "flops_per_joule": 321513480.0809536,
                        "joules_per_flop": 3.1102894962544363e-09
                    },
                    "per-process_emissions": [
                        0.004334084808776553,
                        0.004330797861220066,
                        0.004327174573783164,
                        0.0043328323218603205
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0472": {
            "setup": {
                "experiment_id": "0472",
                "cycle_id": 2,
                "date_time": "April 25, 2025 at 10:56:49 AM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_64",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 64,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 41.03913248406025,
                        "average_latency_ms_per_batch": 20519.566242030123,
                        "throughput_queries_per_sec": 3.118974311889163,
                        "throughput_tokens_per_sec": 399.2287119218129
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 40087060480,
                        "gpu_max_memory_reserved_bytes": 40087060480
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.7,
                        "cpu_memory_usage_bytes": 1973301248
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0472",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_1": 448.784347075323,
                            "process_2": 1039.9315178296483,
                            "process_3": 983.0883834106937,
                            "process_0": 1238.5202103015365
                        },
                        "ram_power": {
                            "process_1": 0.7308268547058105,
                            "process_2": 0.7229576110839844,
                            "process_3": 0.73077392578125,
                            "process_0": 0.687751293182373
                        },
                        "cpu_energy": {
                            "process_1": 0.0010580009683144455,
                            "process_2": 0.0010051408273429843,
                            "process_3": 0.0009966710584685644,
                            "process_0": 0.0010105514403385314
                        },
                        "gpu_energy": {
                            "process_1": 0.007460720968571977,
                            "process_2": 0.007260826919765151,
                            "process_3": 0.007250362466951543,
                            "process_0": 0.00736327894617439
                        },
                        "ram_energy": {
                            "process_1": 6.568706340195348e-06,
                            "process_2": 6.180242030006291e-06,
                            "process_3": 6.184740028822109e-06,
                            "process_0": 6.0896001050084635e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.008525290643226616,
                            "process_2": 0.00827214798913814,
                            "process_3": 0.00825321826544893,
                            "process_0": 0.00837991998661793
                        },
                        "total_energy_joules": {
                            "process_1": 30691.046315615815,
                            "process_2": 29779.732760897303,
                            "process_3": 29711.585755616146,
                            "process_0": 30167.711951824545
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 927.5811146543003,
                        "ram_power_avg": 0.7180774211883545,
                        "cpu_energy_total": 0.004070364294464526,
                        "gpu_energy_total": 0.02933518930146306,
                        "ram_energy_total": 2.5023288504032212e-05,
                        "total_energy_kwh": 0.033430576884431615,
                        "total_energy_joules": 120350.07678395382
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.13613618235916627,
                        "joules_per_token": 7.345585741208119,
                        "flops_per_joule": 437378884.29733235,
                        "joules_per_flop": 2.286347228688331e-09
                    },
                    "per-process_emissions": [
                        0.003247709470537179,
                        0.0031512747764621744,
                        0.00314406349822277,
                        0.0031923305189021003
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0472": {
            "setup": {
                "experiment_id": "0472",
                "cycle_id": 2,
                "date_time": "April 25, 2025 at 10:56:49 AM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_64",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 64,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 41.03913248406025,
                        "average_latency_ms_per_batch": 20519.566242030123,
                        "throughput_queries_per_sec": 3.118974311889163,
                        "throughput_tokens_per_sec": 399.2287119218129
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 40087060480,
                        "gpu_max_memory_reserved_bytes": 40087060480
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.7,
                        "cpu_memory_usage_bytes": 1973301248
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0472",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_1": 448.784347075323,
                            "process_2": 1039.9315178296483,
                            "process_3": 983.0883834106937,
                            "process_0": 1238.5202103015365
                        },
                        "ram_power": {
                            "process_1": 0.7308268547058105,
                            "process_2": 0.7229576110839844,
                            "process_3": 0.73077392578125,
                            "process_0": 0.687751293182373
                        },
                        "cpu_energy": {
                            "process_1": 0.0010580009683144455,
                            "process_2": 0.0010051408273429843,
                            "process_3": 0.0009966710584685644,
                            "process_0": 0.0010105514403385314
                        },
                        "gpu_energy": {
                            "process_1": 0.007460720968571977,
                            "process_2": 0.007260826919765151,
                            "process_3": 0.007250362466951543,
                            "process_0": 0.00736327894617439
                        },
                        "ram_energy": {
                            "process_1": 6.568706340195348e-06,
                            "process_2": 6.180242030006291e-06,
                            "process_3": 6.184740028822109e-06,
                            "process_0": 6.0896001050084635e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.008525290643226616,
                            "process_2": 0.00827214798913814,
                            "process_3": 0.00825321826544893,
                            "process_0": 0.00837991998661793
                        },
                        "total_energy_joules": {
                            "process_1": 30691.046315615815,
                            "process_2": 29779.732760897303,
                            "process_3": 29711.585755616146,
                            "process_0": 30167.711951824545
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 927.5811146543003,
                        "ram_power_avg": 0.7180774211883545,
                        "cpu_energy_total": 0.004070364294464526,
                        "gpu_energy_total": 0.02933518930146306,
                        "ram_energy_total": 2.5023288504032212e-05,
                        "total_energy_kwh": 0.033430576884431615,
                        "total_energy_joules": 120350.07678395382
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.13613618235916627,
                        "joules_per_token": 7.345585741208119,
                        "flops_per_joule": 437378884.29733235,
                        "joules_per_flop": 2.286347228688331e-09
                    },
                    "per-process_emissions": [
                        0.003247709470537179,
                        0.0031512747764621744,
                        0.00314406349822277,
                        0.0031923305189021003
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0473": {
            "setup": {
                "experiment_id": "0473",
                "cycle_id": 2,
                "date_time": "April 25, 2025 at 10:58:17 AM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_1_temp_1.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 1.4,
                    "decoder_top_k": 1,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.267022325017024,
                        "average_latency_ms_per_batch": 4783.377790627128,
                        "throughput_queries_per_sec": 3.3449166468413756,
                        "throughput_tokens_per_sec": 428.1493307956961
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38449184768,
                        "gpu_max_memory_reserved_bytes": 38449184768
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            27.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.6,
                        "cpu_memory_usage_bytes": 1976614912
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0473",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_2": 5.860075688566076,
                            "process_3": 965.4631181436447,
                            "process_0": 30.0206239390712,
                            "process_1": 29.312943401062245
                        },
                        "ram_power": {
                            "process_2": 0.7313947677612305,
                            "process_3": 0.7236328125,
                            "process_0": 0.6891603469848633,
                            "process_1": 0.7309784889221191
                        },
                        "cpu_energy": {
                            "process_2": 0.0012200035836867754,
                            "process_3": 0.0011343924333432366,
                            "process_0": 0.0012649392049843304,
                            "process_1": 0.0011727080904402106
                        },
                        "gpu_energy": {
                            "process_2": 0.010223895679111195,
                            "process_3": 0.010237070967429318,
                            "process_0": 0.010223895679111195,
                            "process_1": 0.010225550402655159
                        },
                        "ram_energy": {
                            "process_2": 6.873936244942335e-06,
                            "process_3": 6.630591113033601e-06,
                            "process_0": 6.752269270229888e-06,
                            "process_1": 6.985818613417141e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.01145077319904291,
                            "process_3": 0.011378093991885586,
                            "process_0": 0.011495587153365756,
                            "process_1": 0.011405244311708784
                        },
                        "total_energy_joules": {
                            "process_2": 41222.783516554475,
                            "process_3": 40961.13837078811,
                            "process_0": 41384.11375211672,
                            "process_1": 41058.87952215162
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 257.66419029308605,
                        "ram_power_avg": 0.7187916040420532,
                        "cpu_energy_total": 0.004792043312454553,
                        "gpu_energy_total": 0.040910412728306866,
                        "ram_energy_total": 2.7242615241622965e-05,
                        "total_energy_kwh": 0.045729698656003036,
                        "total_energy_joules": 164626.91516161093
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09952200090681501,
                        "joules_per_token": 10.048029489844417,
                        "flops_per_joule": 319744692.155531,
                        "joules_per_flop": 3.1274952314566575e-09
                    },
                    "per-process_emissions": [
                        0.004362172050175397,
                        0.004334484906208814,
                        0.0043792439260746845,
                        0.004344827820545461
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0473": {
            "setup": {
                "experiment_id": "0473",
                "cycle_id": 2,
                "date_time": "April 25, 2025 at 10:58:17 AM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_1_temp_1.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 1.4,
                    "decoder_top_k": 1,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.267022325017024,
                        "average_latency_ms_per_batch": 4783.377790627128,
                        "throughput_queries_per_sec": 3.3449166468413756,
                        "throughput_tokens_per_sec": 428.1493307956961
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38449184768,
                        "gpu_max_memory_reserved_bytes": 38449184768
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            27.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.6,
                        "cpu_memory_usage_bytes": 1976614912
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0473",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_2": 5.860075688566076,
                            "process_3": 965.4631181436447,
                            "process_0": 30.0206239390712,
                            "process_1": 29.312943401062245
                        },
                        "ram_power": {
                            "process_2": 0.7313947677612305,
                            "process_3": 0.7236328125,
                            "process_0": 0.6891603469848633,
                            "process_1": 0.7309784889221191
                        },
                        "cpu_energy": {
                            "process_2": 0.0012200035836867754,
                            "process_3": 0.0011343924333432366,
                            "process_0": 0.0012649392049843304,
                            "process_1": 0.0011727080904402106
                        },
                        "gpu_energy": {
                            "process_2": 0.010223895679111195,
                            "process_3": 0.010237070967429318,
                            "process_0": 0.010223895679111195,
                            "process_1": 0.010225550402655159
                        },
                        "ram_energy": {
                            "process_2": 6.873936244942335e-06,
                            "process_3": 6.630591113033601e-06,
                            "process_0": 6.752269270229888e-06,
                            "process_1": 6.985818613417141e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.01145077319904291,
                            "process_3": 0.011378093991885586,
                            "process_0": 0.011495587153365756,
                            "process_1": 0.011405244311708784
                        },
                        "total_energy_joules": {
                            "process_2": 41222.783516554475,
                            "process_3": 40961.13837078811,
                            "process_0": 41384.11375211672,
                            "process_1": 41058.87952215162
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 257.66419029308605,
                        "ram_power_avg": 0.7187916040420532,
                        "cpu_energy_total": 0.004792043312454553,
                        "gpu_energy_total": 0.040910412728306866,
                        "ram_energy_total": 2.7242615241622965e-05,
                        "total_energy_kwh": 0.045729698656003036,
                        "total_energy_joules": 164626.91516161093
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09952200090681501,
                        "joules_per_token": 10.048029489844417,
                        "flops_per_joule": 319744692.155531,
                        "joules_per_flop": 3.1274952314566575e-09
                    },
                    "per-process_emissions": [
                        0.004362172050175397,
                        0.004334484906208814,
                        0.0043792439260746845,
                        0.004344827820545461
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0474": {
            "setup": {
                "experiment_id": "0474",
                "cycle_id": 2,
                "date_time": "April 25, 2025 at 10:59:48 AM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_off",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.133687741006725,
                        "average_latency_ms_per_batch": 4766.710967625841,
                        "throughput_queries_per_sec": 3.356612160600359,
                        "throughput_tokens_per_sec": 429.64635655684594
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38352715776,
                        "gpu_max_memory_reserved_bytes": 38352715776
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            27.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.5,
                        "cpu_memory_usage_bytes": 1975218176
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0474",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_1": 90.69348530803377,
                            "process_0": 90.85889554365231,
                            "process_2": 95.87328586999142,
                            "process_3": 80.41579805010615
                        },
                        "ram_power": {
                            "process_1": 0.7294549942016602,
                            "process_0": 0.6873993873596193,
                            "process_2": 0.731414794921875,
                            "process_3": 0.7294507026672363
                        },
                        "cpu_energy": {
                            "process_1": 0.0011204058500879908,
                            "process_0": 0.0010845273536469904,
                            "process_2": 0.001084466526444885,
                            "process_3": 0.001099210612528623
                        },
                        "gpu_energy": {
                            "process_1": 0.01021757206294005,
                            "process_0": 0.01021757206294005,
                            "process_2": 0.010219060397465896,
                            "process_3": 0.01022231956673636
                        },
                        "ram_energy": {
                            "process_1": 6.676775600011031e-06,
                            "process_0": 6.413786682755987e-06,
                            "process_2": 7.5595259031532645e-06,
                            "process_3": 6.858045372788981e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.01134465468862805,
                            "process_0": 0.011308513203269793,
                            "process_2": 0.011311086449813932,
                            "process_3": 0.01132838822463777
                        },
                        "total_energy_joules": {
                            "process_1": 40840.75687906098,
                            "process_0": 40710.647531771254,
                            "process_2": 40719.911219330155,
                            "process_3": 40782.19760869597
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 89.46036619294591,
                        "ram_power_avg": 0.7194299697875977,
                        "cpu_energy_total": 0.0043886103427084894,
                        "gpu_energy_total": 0.040876524090082356,
                        "ram_energy_total": 2.750813355870926e-05,
                        "total_energy_kwh": 0.045292642566349545,
                        "total_energy_joules": 163053.51323885837
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.10048234885929105,
                        "joules_per_token": 9.951996657645164,
                        "flops_per_joule": 322830101.99083126,
                        "joules_per_flop": 3.0976045722910968e-09
                    },
                    "per-process_emissions": [
                        0.004321746203632856,
                        0.004307978104785628,
                        0.0043089583830566176,
                        0.004315549494175759
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0474": {
            "setup": {
                "experiment_id": "0474",
                "cycle_id": 2,
                "date_time": "April 25, 2025 at 10:59:48 AM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_off",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.133687741006725,
                        "average_latency_ms_per_batch": 4766.710967625841,
                        "throughput_queries_per_sec": 3.356612160600359,
                        "throughput_tokens_per_sec": 429.64635655684594
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38352715776,
                        "gpu_max_memory_reserved_bytes": 38352715776
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            27.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.5,
                        "cpu_memory_usage_bytes": 1975218176
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0474",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_1": 90.69348530803377,
                            "process_0": 90.85889554365231,
                            "process_2": 95.87328586999142,
                            "process_3": 80.41579805010615
                        },
                        "ram_power": {
                            "process_1": 0.7294549942016602,
                            "process_0": 0.6873993873596193,
                            "process_2": 0.731414794921875,
                            "process_3": 0.7294507026672363
                        },
                        "cpu_energy": {
                            "process_1": 0.0011204058500879908,
                            "process_0": 0.0010845273536469904,
                            "process_2": 0.001084466526444885,
                            "process_3": 0.001099210612528623
                        },
                        "gpu_energy": {
                            "process_1": 0.01021757206294005,
                            "process_0": 0.01021757206294005,
                            "process_2": 0.010219060397465896,
                            "process_3": 0.01022231956673636
                        },
                        "ram_energy": {
                            "process_1": 6.676775600011031e-06,
                            "process_0": 6.413786682755987e-06,
                            "process_2": 7.5595259031532645e-06,
                            "process_3": 6.858045372788981e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.01134465468862805,
                            "process_0": 0.011308513203269793,
                            "process_2": 0.011311086449813932,
                            "process_3": 0.01132838822463777
                        },
                        "total_energy_joules": {
                            "process_1": 40840.75687906098,
                            "process_0": 40710.647531771254,
                            "process_2": 40719.911219330155,
                            "process_3": 40782.19760869597
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 89.46036619294591,
                        "ram_power_avg": 0.7194299697875977,
                        "cpu_energy_total": 0.0043886103427084894,
                        "gpu_energy_total": 0.040876524090082356,
                        "ram_energy_total": 2.750813355870926e-05,
                        "total_energy_kwh": 0.045292642566349545,
                        "total_energy_joules": 163053.51323885837
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.10048234885929105,
                        "joules_per_token": 9.951996657645164,
                        "flops_per_joule": 322830101.99083126,
                        "joules_per_flop": 3.0976045722910968e-09
                    },
                    "per-process_emissions": [
                        0.004321746203632856,
                        0.004307978104785628,
                        0.0043089583830566176,
                        0.004315549494175759
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0475": {
            "setup": {
                "experiment_id": "0475",
                "cycle_id": 2,
                "date_time": "April 25, 2025 at 11:01:19 AM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_20_temp_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 20,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.39196158503182,
                        "average_latency_ms_per_batch": 4798.995198128978,
                        "throughput_queries_per_sec": 3.334031258509708,
                        "throughput_tokens_per_sec": 426.7560010892426
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38451281920,
                        "gpu_max_memory_reserved_bytes": 38451281920
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.6,
                        "cpu_memory_usage_bytes": 1976127488
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0475",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 194.60088227137322,
                            "process_2": 209.0774810545294,
                            "process_3": 6346.36879727485,
                            "process_1": 173.4723531716388
                        },
                        "ram_power": {
                            "process_0": 0.6890072822570801,
                            "process_2": 0.7247729301452637,
                            "process_3": 0.7252535820007324,
                            "process_1": 0.7327151298522949
                        },
                        "cpu_energy": {
                            "process_0": 0.0012015104206820973,
                            "process_2": 0.001189251631527441,
                            "process_3": 0.001135094435318024,
                            "process_1": 0.0011479811347890065
                        },
                        "gpu_energy": {
                            "process_0": 0.010258345984448525,
                            "process_2": 0.010258345984448525,
                            "process_3": 0.010260152097005815,
                            "process_1": 0.010259787374490514
                        },
                        "ram_energy": {
                            "process_0": 6.70826515564511e-06,
                            "process_2": 6.980796560532442e-06,
                            "process_3": 6.9204113852852e-06,
                            "process_1": 7.17135256640197e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.011466564670286268,
                            "process_2": 0.011454578412536496,
                            "process_3": 0.011402166943709123,
                            "process_1": 0.011414939861845927
                        },
                        "total_energy_joules": {
                            "process_0": 41279.63281303056,
                            "process_2": 41236.48228513139,
                            "process_3": 41047.800997352846,
                            "process_1": 41093.78350264534
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1730.879878443098,
                        "ram_power_avg": 0.7179372310638428,
                        "cpu_energy_total": 0.004673837622316568,
                        "gpu_energy_total": 0.04103663144039338,
                        "ram_energy_total": 2.778082566786472e-05,
                        "total_energy_kwh": 0.04573824988837781,
                        "total_energy_joules": 164657.6995981601
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09950339425356017,
                        "joules_per_token": 10.049908422739264,
                        "flops_per_joule": 319684912.623741,
                        "joules_per_flop": 3.1280800579318186e-09
                    },
                    "per-process_emissions": [
                        0.0043681878111455534,
                        0.004363621646255779,
                        0.004343655497205991,
                        0.004348521340370206
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0475": {
            "setup": {
                "experiment_id": "0475",
                "cycle_id": 2,
                "date_time": "April 25, 2025 at 11:01:19 AM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_20_temp_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 20,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.39196158503182,
                        "average_latency_ms_per_batch": 4798.995198128978,
                        "throughput_queries_per_sec": 3.334031258509708,
                        "throughput_tokens_per_sec": 426.7560010892426
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38451281920,
                        "gpu_max_memory_reserved_bytes": 38451281920
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.6,
                        "cpu_memory_usage_bytes": 1976127488
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0475",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 194.60088227137322,
                            "process_2": 209.0774810545294,
                            "process_3": 6346.36879727485,
                            "process_1": 173.4723531716388
                        },
                        "ram_power": {
                            "process_0": 0.6890072822570801,
                            "process_2": 0.7247729301452637,
                            "process_3": 0.7252535820007324,
                            "process_1": 0.7327151298522949
                        },
                        "cpu_energy": {
                            "process_0": 0.0012015104206820973,
                            "process_2": 0.001189251631527441,
                            "process_3": 0.001135094435318024,
                            "process_1": 0.0011479811347890065
                        },
                        "gpu_energy": {
                            "process_0": 0.010258345984448525,
                            "process_2": 0.010258345984448525,
                            "process_3": 0.010260152097005815,
                            "process_1": 0.010259787374490514
                        },
                        "ram_energy": {
                            "process_0": 6.70826515564511e-06,
                            "process_2": 6.980796560532442e-06,
                            "process_3": 6.9204113852852e-06,
                            "process_1": 7.17135256640197e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.011466564670286268,
                            "process_2": 0.011454578412536496,
                            "process_3": 0.011402166943709123,
                            "process_1": 0.011414939861845927
                        },
                        "total_energy_joules": {
                            "process_0": 41279.63281303056,
                            "process_2": 41236.48228513139,
                            "process_3": 41047.800997352846,
                            "process_1": 41093.78350264534
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1730.879878443098,
                        "ram_power_avg": 0.7179372310638428,
                        "cpu_energy_total": 0.004673837622316568,
                        "gpu_energy_total": 0.04103663144039338,
                        "ram_energy_total": 2.778082566786472e-05,
                        "total_energy_kwh": 0.04573824988837781,
                        "total_energy_joules": 164657.6995981601
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09950339425356017,
                        "joules_per_token": 10.049908422739264,
                        "flops_per_joule": 319684912.623741,
                        "joules_per_flop": 3.1280800579318186e-09
                    },
                    "per-process_emissions": [
                        0.0043681878111455534,
                        0.004363621646255779,
                        0.004343655497205991,
                        0.004348521340370206
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0476": {
            "setup": {
                "experiment_id": "0476",
                "cycle_id": 2,
                "date_time": "April 25, 2025 at 11:02:16 AM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "A5_Parallel_Overdrive",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "greedy",
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 64,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 9.642941593017895,
                        "average_latency_ms_per_batch": 4821.470796508947,
                        "throughput_queries_per_sec": 13.27395782347994,
                        "throughput_tokens_per_sec": 1699.0666014054323
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 12870114304,
                        "gpu_max_memory_allocated_bytes": 12870114304,
                        "gpu_current_memory_reserved_bytes": 22057844736,
                        "gpu_max_memory_reserved_bytes": 22057844736
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            7.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.7,
                        "cpu_memory_usage_bytes": 2664914944
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0476",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 104.73868759329973,
                            "process_1": 105.51179300144517,
                            "process_2": 115.66290090005452,
                            "process_3": 104.94855947881321
                        },
                        "ram_power": {
                            "process_0": 0.929004192352295,
                            "process_1": 0.9645309448242188,
                            "process_2": 0.9723916053771973,
                            "process_3": 0.9716205596923828
                        },
                        "cpu_energy": {
                            "process_0": 0.0002855642200647708,
                            "process_1": 0.0002873504928793409,
                            "process_2": 0.00029045512372067613,
                            "process_3": 0.0002870008334411977
                        },
                        "gpu_energy": {
                            "process_0": 0.002357403830364646,
                            "process_1": 0.0023588907759961586,
                            "process_2": 0.002363095779362112,
                            "process_3": 0.002360108832526464
                        },
                        "ram_energy": {
                            "process_0": 2.4882572991942934e-06,
                            "process_1": 2.594540081660655e-06,
                            "process_2": 2.6327555938761537e-06,
                            "process_3": 2.6204022582372496e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.0026454563077286113,
                            "process_1": 0.0026488358089571604,
                            "process_2": 0.0026561836586766648,
                            "process_3": 0.0026497300682258996
                        },
                        "total_energy_joules": {
                            "process_0": 9523.642707823,
                            "process_1": 9535.808912245777,
                            "process_2": 9562.261171235994,
                            "process_3": 9539.028245613239
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 107.71548524340315,
                        "ram_power_avg": 0.9593868255615234,
                        "cpu_energy_total": 0.0011503706701059854,
                        "gpu_energy_total": 0.009439499218249381,
                        "ram_energy_total": 1.0335955232968353e-05,
                        "total_energy_kwh": 0.010600205843588336,
                        "total_energy_joules": 38160.741036918014
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.4293417673453866,
                        "joules_per_token": 2.329146791804078,
                        "flops_per_joule": 1379390988.711137,
                        "joules_per_flop": 7.249576140368809e-10
                    },
                    "per-process_emissions": [
                        0.0010077865804292146,
                        0.0010090740014222302,
                        0.0010118731647728754,
                        0.0010094146694906564
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0476": {
            "setup": {
                "experiment_id": "0476",
                "cycle_id": 2,
                "date_time": "April 25, 2025 at 11:02:16 AM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "A5_Parallel_Overdrive",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "greedy",
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 64,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 9.642941593017895,
                        "average_latency_ms_per_batch": 4821.470796508947,
                        "throughput_queries_per_sec": 13.27395782347994,
                        "throughput_tokens_per_sec": 1699.0666014054323
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 12870114304,
                        "gpu_max_memory_allocated_bytes": 12870114304,
                        "gpu_current_memory_reserved_bytes": 22057844736,
                        "gpu_max_memory_reserved_bytes": 22057844736
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            7.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.7,
                        "cpu_memory_usage_bytes": 2664914944
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0476",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 104.73868759329973,
                            "process_1": 105.51179300144517,
                            "process_2": 115.66290090005452,
                            "process_3": 104.94855947881321
                        },
                        "ram_power": {
                            "process_0": 0.929004192352295,
                            "process_1": 0.9645309448242188,
                            "process_2": 0.9723916053771973,
                            "process_3": 0.9716205596923828
                        },
                        "cpu_energy": {
                            "process_0": 0.0002855642200647708,
                            "process_1": 0.0002873504928793409,
                            "process_2": 0.00029045512372067613,
                            "process_3": 0.0002870008334411977
                        },
                        "gpu_energy": {
                            "process_0": 0.002357403830364646,
                            "process_1": 0.0023588907759961586,
                            "process_2": 0.002363095779362112,
                            "process_3": 0.002360108832526464
                        },
                        "ram_energy": {
                            "process_0": 2.4882572991942934e-06,
                            "process_1": 2.594540081660655e-06,
                            "process_2": 2.6327555938761537e-06,
                            "process_3": 2.6204022582372496e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.0026454563077286113,
                            "process_1": 0.0026488358089571604,
                            "process_2": 0.0026561836586766648,
                            "process_3": 0.0026497300682258996
                        },
                        "total_energy_joules": {
                            "process_0": 9523.642707823,
                            "process_1": 9535.808912245777,
                            "process_2": 9562.261171235994,
                            "process_3": 9539.028245613239
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 107.71548524340315,
                        "ram_power_avg": 0.9593868255615234,
                        "cpu_energy_total": 0.0011503706701059854,
                        "gpu_energy_total": 0.009439499218249381,
                        "ram_energy_total": 1.0335955232968353e-05,
                        "total_energy_kwh": 0.010600205843588336,
                        "total_energy_joules": 38160.741036918014
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.4293417673453866,
                        "joules_per_token": 2.329146791804078,
                        "flops_per_joule": 1379390988.711137,
                        "joules_per_flop": 7.249576140368809e-10
                    },
                    "per-process_emissions": [
                        0.0010077865804292146,
                        0.0010090740014222302,
                        0.0010118731647728754,
                        0.0010094146694906564
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0477": {
            "setup": {
                "experiment_id": "0477",
                "cycle_id": 2,
                "date_time": "April 25, 2025 at 11:04:11 AM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "R3_Balanced_Enterprise_Service",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.5,
                    "delay_max": 1.5,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 32,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 53.37918160302797,
                        "average_latency_ms_per_batch": 13344.795400756993,
                        "throughput_queries_per_sec": 2.3979385999567127,
                        "throughput_tokens_per_sec": 306.9361407944592
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4411281408,
                        "gpu_max_memory_allocated_bytes": 4411281408,
                        "gpu_current_memory_reserved_bytes": 8789164032,
                        "gpu_max_memory_reserved_bytes": 8789164032
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.2,
                        "cpu_memory_usage_bytes": 2352611328
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0477",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_3": 389.04669729358295,
                            "process_0": 59.7221823503077,
                            "process_2": 374.10822517944774,
                            "process_1": 281.38433311190624
                        },
                        "ram_power": {
                            "process_3": 0.8685407638549805,
                            "process_0": 0.82086181640625,
                            "process_2": 0.8593025207519531,
                            "process_1": 0.8700714111328125
                        },
                        "cpu_energy": {
                            "process_3": 0.0017248007308771779,
                            "process_0": 0.0016992986424738776,
                            "process_2": 0.0016962308284710168,
                            "process_1": 0.0017072946430016604
                        },
                        "gpu_energy": {
                            "process_3": 0.005161984407363818,
                            "process_0": 0.005112433534387506,
                            "process_2": 0.005161984407363818,
                            "process_1": 0.005267667825240352
                        },
                        "ram_energy": {
                            "process_3": 1.0790640706377445e-05,
                            "process_0": 1.0471348801816916e-05,
                            "process_2": 1.0982238154846022e-05,
                            "process_1": 1.1222860614896856e-05
                        },
                        "total_energy_kwh": {
                            "process_3": 0.006897575778947374,
                            "process_0": 0.006822203525663201,
                            "process_2": 0.006869197473989678,
                            "process_1": 0.006986185328856906
                        },
                        "total_energy_joules": {
                            "process_3": 24831.27280421055,
                            "process_0": 24559.932692387523,
                            "process_2": 24729.11090636284,
                            "process_1": 25150.267183884862
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 276.06535948381116,
                        "ram_power_avg": 0.854694128036499,
                        "cpu_energy_total": 0.006827624844823733,
                        "gpu_energy_total": 0.020704070174355493,
                        "ram_energy_total": 4.3467088277937236e-05,
                        "total_energy_kwh": 0.027575162107457158,
                        "total_energy_joules": 99270.58358684578
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.1650438569817275,
                        "joules_per_token": 6.058995580251818,
                        "flops_per_joule": 530253579.7305323,
                        "joules_per_flop": 1.88589014431206e-09
                    },
                    "per-process_emissions": [
                        0.002627631492990002,
                        0.0025989184331013967,
                        0.002616820777716368,
                        0.0026613873010280383
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0477": {
            "setup": {
                "experiment_id": "0477",
                "cycle_id": 2,
                "date_time": "April 25, 2025 at 11:04:11 AM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "R3_Balanced_Enterprise_Service",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.5,
                    "delay_max": 1.5,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 32,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 53.37918160302797,
                        "average_latency_ms_per_batch": 13344.795400756993,
                        "throughput_queries_per_sec": 2.3979385999567127,
                        "throughput_tokens_per_sec": 306.9361407944592
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4411281408,
                        "gpu_max_memory_allocated_bytes": 4411281408,
                        "gpu_current_memory_reserved_bytes": 8789164032,
                        "gpu_max_memory_reserved_bytes": 8789164032
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.2,
                        "cpu_memory_usage_bytes": 2352611328
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0477",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_3": 389.04669729358295,
                            "process_0": 59.7221823503077,
                            "process_2": 374.10822517944774,
                            "process_1": 281.38433311190624
                        },
                        "ram_power": {
                            "process_3": 0.8685407638549805,
                            "process_0": 0.82086181640625,
                            "process_2": 0.8593025207519531,
                            "process_1": 0.8700714111328125
                        },
                        "cpu_energy": {
                            "process_3": 0.0017248007308771779,
                            "process_0": 0.0016992986424738776,
                            "process_2": 0.0016962308284710168,
                            "process_1": 0.0017072946430016604
                        },
                        "gpu_energy": {
                            "process_3": 0.005161984407363818,
                            "process_0": 0.005112433534387506,
                            "process_2": 0.005161984407363818,
                            "process_1": 0.005267667825240352
                        },
                        "ram_energy": {
                            "process_3": 1.0790640706377445e-05,
                            "process_0": 1.0471348801816916e-05,
                            "process_2": 1.0982238154846022e-05,
                            "process_1": 1.1222860614896856e-05
                        },
                        "total_energy_kwh": {
                            "process_3": 0.006897575778947374,
                            "process_0": 0.006822203525663201,
                            "process_2": 0.006869197473989678,
                            "process_1": 0.006986185328856906
                        },
                        "total_energy_joules": {
                            "process_3": 24831.27280421055,
                            "process_0": 24559.932692387523,
                            "process_2": 24729.11090636284,
                            "process_1": 25150.267183884862
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 276.06535948381116,
                        "ram_power_avg": 0.854694128036499,
                        "cpu_energy_total": 0.006827624844823733,
                        "gpu_energy_total": 0.020704070174355493,
                        "ram_energy_total": 4.3467088277937236e-05,
                        "total_energy_kwh": 0.027575162107457158,
                        "total_energy_joules": 99270.58358684578
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.1650438569817275,
                        "joules_per_token": 6.058995580251818,
                        "flops_per_joule": 530253579.7305323,
                        "joules_per_flop": 1.88589014431206e-09
                    },
                    "per-process_emissions": [
                        0.002627631492990002,
                        0.0025989184331013967,
                        0.002616820777716368,
                        0.0026613873010280383
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0478": {
            "setup": {
                "experiment_id": "0478",
                "cycle_id": 2,
                "date_time": "April 25, 2025 at 11:25:47 AM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "R5_Real_Time_Mobile_Inference",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_p",
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": 0.9
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.2,
                    "delay_max": 0.6,
                    "simulate_burst": true,
                    "burst_interval": 5.0,
                    "burst_size": 8
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 1,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 1252.0654953388148,
                        "average_latency_ms_per_batch": 9781.76168233449,
                        "throughput_queries_per_sec": 0.10223107375494171,
                        "throughput_tokens_per_sec": 13.08557744063254
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4410810880,
                        "gpu_max_memory_allocated_bytes": 4410810880,
                        "gpu_current_memory_reserved_bytes": 7098859520,
                        "gpu_max_memory_reserved_bytes": 7098859520
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 0.8,
                        "cpu_memory_usage_bytes": 2325884928
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0478",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_0": 60.67091244626632
                        },
                        "ram_power": {
                            "process_0": 0.8123059272766113
                        },
                        "cpu_energy": {
                            "process_0": 0.03774509508984742
                        },
                        "gpu_energy": {
                            "process_0": 0.02097228594448275
                        },
                        "ram_energy": {
                            "process_0": 0.0002493571029519287
                        },
                        "total_energy_kwh": {
                            "process_0": 0.05896673813728229
                        },
                        "total_energy_joules": {
                            "process_0": 212280.25729421622
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 60.67091244626632,
                        "ram_power_avg": 0.8123059272766113,
                        "cpu_energy_total": 0.03774509508984742,
                        "gpu_energy_total": 0.02097228594448275,
                        "ram_energy_total": 0.0002493571029519287,
                        "total_energy_kwh": 0.05896673813728229,
                        "total_energy_joules": 212280.25729421622
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.07718098804304774,
                        "joules_per_token": 12.95655867274269,
                        "flops_per_joule": 247967394.5180308,
                        "joules_per_flop": 4.032788270182375e-09
                    },
                    "per-process_emissions": [
                        0.02246337889339769
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0478": {
            "setup": {
                "experiment_id": "0478",
                "cycle_id": 2,
                "date_time": "April 25, 2025 at 11:25:47 AM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "R5_Real_Time_Mobile_Inference",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_p",
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": 0.9
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.2,
                    "delay_max": 0.6,
                    "simulate_burst": true,
                    "burst_interval": 5.0,
                    "burst_size": 8
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 1,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 1252.0654953388148,
                        "average_latency_ms_per_batch": 9781.76168233449,
                        "throughput_queries_per_sec": 0.10223107375494171,
                        "throughput_tokens_per_sec": 13.08557744063254
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4410810880,
                        "gpu_max_memory_allocated_bytes": 4410810880,
                        "gpu_current_memory_reserved_bytes": 7098859520,
                        "gpu_max_memory_reserved_bytes": 7098859520
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 0.8,
                        "cpu_memory_usage_bytes": 2325884928
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0478",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_0": 60.67091244626632
                        },
                        "ram_power": {
                            "process_0": 0.8123059272766113
                        },
                        "cpu_energy": {
                            "process_0": 0.03774509508984742
                        },
                        "gpu_energy": {
                            "process_0": 0.02097228594448275
                        },
                        "ram_energy": {
                            "process_0": 0.0002493571029519287
                        },
                        "total_energy_kwh": {
                            "process_0": 0.05896673813728229
                        },
                        "total_energy_joules": {
                            "process_0": 212280.25729421622
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 60.67091244626632,
                        "ram_power_avg": 0.8123059272766113,
                        "cpu_energy_total": 0.03774509508984742,
                        "gpu_energy_total": 0.02097228594448275,
                        "ram_energy_total": 0.0002493571029519287,
                        "total_energy_kwh": 0.05896673813728229,
                        "total_energy_joules": 212280.25729421622
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.07718098804304774,
                        "joules_per_token": 12.95655867274269,
                        "flops_per_joule": 247967394.5180308,
                        "joules_per_flop": 4.032788270182375e-09
                    },
                    "per-process_emissions": [
                        0.02246337889339769
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0479": {
            "setup": {
                "experiment_id": "0479",
                "cycle_id": 2,
                "date_time": "April 25, 2025 at 11:27:32 AM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "R1_Standard_Production_Config",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "greedy",
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.5,
                    "delay_max": 1.5,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 49.86667745996965,
                        "average_latency_ms_per_batch": 6233.334682496206,
                        "throughput_queries_per_sec": 2.5668443642099814,
                        "throughput_tokens_per_sec": 328.5560786188776
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38352715776,
                        "gpu_max_memory_reserved_bytes": 38352715776
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            28.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.5,
                        "cpu_memory_usage_bytes": 1949630464
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0479",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_0": 1056.8699082619396,
                            "process_3": 1068.273134387189,
                            "process_1": 104.92863741429747,
                            "process_2": 86.16339257683919
                        },
                        "ram_power": {
                            "process_0": 0.6797318458557129,
                            "process_3": 0.7231063842773438,
                            "process_1": 0.7302660942077637,
                            "process_2": 0.7299928665161133
                        },
                        "cpu_energy": {
                            "process_0": 0.0013106290565556267,
                            "process_3": 0.0013333053200058206,
                            "process_1": 0.0014769165410853016,
                            "process_2": 0.001482420861098944
                        },
                        "gpu_energy": {
                            "process_0": 0.010633816840378785,
                            "process_3": 0.010875579256008905,
                            "process_1": 0.010920871514466413,
                            "process_2": 0.010925947629639055
                        },
                        "ram_energy": {
                            "process_0": 8.989102846789846e-06,
                            "process_3": 8.535629502965761e-06,
                            "process_1": 1.0324311862905129e-05,
                            "process_2": 1.035438256865598e-05
                        },
                        "total_energy_kwh": {
                            "process_0": 0.011953434999781198,
                            "process_3": 0.012217420205517693,
                            "process_1": 0.01240811236741462,
                            "process_2": 0.012418722873306651
                        },
                        "total_energy_joules": {
                            "process_0": 43032.365999212314,
                            "process_3": 43982.712739863695,
                            "process_1": 44669.204522692635,
                            "process_2": 44707.402343903945
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 579.0587681600664,
                        "ram_power_avg": 0.7157742977142334,
                        "cpu_energy_total": 0.005603271778745693,
                        "gpu_energy_total": 0.04335621524049316,
                        "ram_energy_total": 3.820342678131672e-05,
                        "total_energy_kwh": 0.04899769044602016,
                        "total_energy_joules": 176391.6856056726
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.0928841965750403,
                        "joules_per_token": 10.766094092143103,
                        "flops_per_joule": 298418727.2099587,
                        "joules_per_flop": 3.3509961299996745e-09
                    },
                    "per-process_emissions": [
                        0.004553661063166648,
                        0.004654226227291965,
                        0.004726870406366599,
                        0.004730912478586169
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0479": {
            "setup": {
                "experiment_id": "0479",
                "cycle_id": 2,
                "date_time": "April 25, 2025 at 11:27:32 AM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "R1_Standard_Production_Config",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "greedy",
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.5,
                    "delay_max": 1.5,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 49.86667745996965,
                        "average_latency_ms_per_batch": 6233.334682496206,
                        "throughput_queries_per_sec": 2.5668443642099814,
                        "throughput_tokens_per_sec": 328.5560786188776
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38352715776,
                        "gpu_max_memory_reserved_bytes": 38352715776
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            28.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.5,
                        "cpu_memory_usage_bytes": 1949630464
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0479",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_0": 1056.8699082619396,
                            "process_3": 1068.273134387189,
                            "process_1": 104.92863741429747,
                            "process_2": 86.16339257683919
                        },
                        "ram_power": {
                            "process_0": 0.6797318458557129,
                            "process_3": 0.7231063842773438,
                            "process_1": 0.7302660942077637,
                            "process_2": 0.7299928665161133
                        },
                        "cpu_energy": {
                            "process_0": 0.0013106290565556267,
                            "process_3": 0.0013333053200058206,
                            "process_1": 0.0014769165410853016,
                            "process_2": 0.001482420861098944
                        },
                        "gpu_energy": {
                            "process_0": 0.010633816840378785,
                            "process_3": 0.010875579256008905,
                            "process_1": 0.010920871514466413,
                            "process_2": 0.010925947629639055
                        },
                        "ram_energy": {
                            "process_0": 8.989102846789846e-06,
                            "process_3": 8.535629502965761e-06,
                            "process_1": 1.0324311862905129e-05,
                            "process_2": 1.035438256865598e-05
                        },
                        "total_energy_kwh": {
                            "process_0": 0.011953434999781198,
                            "process_3": 0.012217420205517693,
                            "process_1": 0.01240811236741462,
                            "process_2": 0.012418722873306651
                        },
                        "total_energy_joules": {
                            "process_0": 43032.365999212314,
                            "process_3": 43982.712739863695,
                            "process_1": 44669.204522692635,
                            "process_2": 44707.402343903945
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 579.0587681600664,
                        "ram_power_avg": 0.7157742977142334,
                        "cpu_energy_total": 0.005603271778745693,
                        "gpu_energy_total": 0.04335621524049316,
                        "ram_energy_total": 3.820342678131672e-05,
                        "total_energy_kwh": 0.04899769044602016,
                        "total_energy_joules": 176391.6856056726
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.0928841965750403,
                        "joules_per_token": 10.766094092143103,
                        "flops_per_joule": 298418727.2099587,
                        "joules_per_flop": 3.3509961299996745e-09
                    },
                    "per-process_emissions": [
                        0.004553661063166648,
                        0.004654226227291965,
                        0.004726870406366599,
                        0.004730912478586169
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0481": {
            "setup": {
                "experiment_id": "0481",
                "cycle_id": 2,
                "date_time": "April 25, 2025 at 11:28:51 AM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "A3_Quantisation_Gaming",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 64,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1803463680,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 15.245880879985634,
                        "average_latency_ms_per_batch": 7622.940439992817,
                        "throughput_queries_per_sec": 8.3957103566272,
                        "throughput_tokens_per_sec": 1074.6509256482816
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 3194348544,
                        "gpu_max_memory_allocated_bytes": 3194348544,
                        "gpu_current_memory_reserved_bytes": 7407140864,
                        "gpu_max_memory_reserved_bytes": 7407140864
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 1.4,
                        "cpu_memory_usage_bytes": 2270965760
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0481",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_0": 153.74392792793213
                        },
                        "ram_power": {
                            "process_0": 0.7923746109008789
                        },
                        "cpu_energy": {
                            "process_0": 0.00043530689021645225
                        },
                        "gpu_energy": {
                            "process_0": 0.0008273181618534409
                        },
                        "ram_energy": {
                            "process_0": 2.8374785301338907e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.0012654625306000268
                        },
                        "total_energy_joules": {
                            "process_0": 4555.665110160096
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 153.74392792793213,
                        "ram_power_avg": 0.7923746109008789,
                        "cpu_energy_total": 0.00043530689021645225,
                        "gpu_energy_total": 0.0008273181618534409,
                        "ram_energy_total": 2.8374785301338907e-06,
                        "total_energy_kwh": 0.0012654625306000268,
                        "total_energy_joules": 4555.665110160096
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 3.596401316563023,
                        "joules_per_token": 0.27805573182129495,
                        "flops_per_joule": 11554532880.713472,
                        "joules_per_flop": 8.65461209313943e-11
                    },
                    "per-process_emissions": [
                        0.0004820779510320802
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0481": {
            "setup": {
                "experiment_id": "0481",
                "cycle_id": 2,
                "date_time": "April 25, 2025 at 11:28:51 AM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "A3_Quantisation_Gaming",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 64,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1803463680,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 15.245880879985634,
                        "average_latency_ms_per_batch": 7622.940439992817,
                        "throughput_queries_per_sec": 8.3957103566272,
                        "throughput_tokens_per_sec": 1074.6509256482816
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 3194348544,
                        "gpu_max_memory_allocated_bytes": 3194348544,
                        "gpu_current_memory_reserved_bytes": 7407140864,
                        "gpu_max_memory_reserved_bytes": 7407140864
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 1.4,
                        "cpu_memory_usage_bytes": 2270965760
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0481",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_0": 153.74392792793213
                        },
                        "ram_power": {
                            "process_0": 0.7923746109008789
                        },
                        "cpu_energy": {
                            "process_0": 0.00043530689021645225
                        },
                        "gpu_energy": {
                            "process_0": 0.0008273181618534409
                        },
                        "ram_energy": {
                            "process_0": 2.8374785301338907e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.0012654625306000268
                        },
                        "total_energy_joules": {
                            "process_0": 4555.665110160096
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 153.74392792793213,
                        "ram_power_avg": 0.7923746109008789,
                        "cpu_energy_total": 0.00043530689021645225,
                        "gpu_energy_total": 0.0008273181618534409,
                        "ram_energy_total": 2.8374785301338907e-06,
                        "total_energy_kwh": 0.0012654625306000268,
                        "total_energy_joules": 4555.665110160096
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 3.596401316563023,
                        "joules_per_token": 0.27805573182129495,
                        "flops_per_joule": 11554532880.713472,
                        "joules_per_flop": 8.65461209313943e-11
                    },
                    "per-process_emissions": [
                        0.0004820779510320802
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0483": {
            "setup": {
                "experiment_id": "0483",
                "cycle_id": 2,
                "date_time": "April 25, 2025 at 11:52:37 AM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "R4_High_Load_Cloud_API_Deployment",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "greedy",
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.05,
                    "delay_max": 0.2,
                    "simulate_burst": true,
                    "burst_interval": 2.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 8,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 68.10963392991107,
                        "average_latency_ms_per_batch": 4256.852120619442,
                        "throughput_queries_per_sec": 1.8793229770067439,
                        "throughput_tokens_per_sec": 240.55334105686322
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 12870114304,
                        "gpu_max_memory_allocated_bytes": 12870114304,
                        "gpu_current_memory_reserved_bytes": 17999855616,
                        "gpu_max_memory_reserved_bytes": 17999855616
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 1.1,
                        "cpu_memory_usage_bytes": 2659209216
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0483",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_0": 24.56251337734418
                        },
                        "ram_power": {
                            "process_0": 0.9273505210876465
                        },
                        "cpu_energy": {
                            "process_0": 0.002169847406785266
                        },
                        "gpu_energy": {
                            "process_0": 0.001957194065754564
                        },
                        "ram_energy": {
                            "process_0": 1.6578456075190834e-05
                        },
                        "total_energy_kwh": {
                            "process_0": 0.004143619928615022
                        },
                        "total_energy_joules": {
                            "process_0": 14917.031743014079
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 24.56251337734418,
                        "ram_power_avg": 0.9273505210876465,
                        "cpu_energy_total": 0.002169847406785266,
                        "gpu_energy_total": 0.001957194065754564,
                        "ram_energy_total": 1.6578456075190834e-05,
                        "total_energy_kwh": 0.004143619928615022,
                        "total_energy_joules": 14917.031743014079
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 1.0983418338352018,
                        "joules_per_token": 0.9104633632210741,
                        "flops_per_joule": 3528757142.553888,
                        "joules_per_flop": 2.83385894693865e-10
                    },
                    "per-process_emissions": [
                        0.0015785120118058926
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0483": {
            "setup": {
                "experiment_id": "0483",
                "cycle_id": 2,
                "date_time": "April 25, 2025 at 11:52:37 AM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "R4_High_Load_Cloud_API_Deployment",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "greedy",
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.05,
                    "delay_max": 0.2,
                    "simulate_burst": true,
                    "burst_interval": 2.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 8,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 68.10963392991107,
                        "average_latency_ms_per_batch": 4256.852120619442,
                        "throughput_queries_per_sec": 1.8793229770067439,
                        "throughput_tokens_per_sec": 240.55334105686322
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 12870114304,
                        "gpu_max_memory_allocated_bytes": 12870114304,
                        "gpu_current_memory_reserved_bytes": 17999855616,
                        "gpu_max_memory_reserved_bytes": 17999855616
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 1.1,
                        "cpu_memory_usage_bytes": 2659209216
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0483",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_0": 24.56251337734418
                        },
                        "ram_power": {
                            "process_0": 0.9273505210876465
                        },
                        "cpu_energy": {
                            "process_0": 0.002169847406785266
                        },
                        "gpu_energy": {
                            "process_0": 0.001957194065754564
                        },
                        "ram_energy": {
                            "process_0": 1.6578456075190834e-05
                        },
                        "total_energy_kwh": {
                            "process_0": 0.004143619928615022
                        },
                        "total_energy_joules": {
                            "process_0": 14917.031743014079
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 24.56251337734418,
                        "ram_power_avg": 0.9273505210876465,
                        "cpu_energy_total": 0.002169847406785266,
                        "gpu_energy_total": 0.001957194065754564,
                        "ram_energy_total": 1.6578456075190834e-05,
                        "total_energy_kwh": 0.004143619928615022,
                        "total_energy_joules": 14917.031743014079
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 1.0983418338352018,
                        "joules_per_token": 0.9104633632210741,
                        "flops_per_joule": 3528757142.553888,
                        "joules_per_flop": 2.83385894693865e-10
                    },
                    "per-process_emissions": [
                        0.0015785120118058926
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0484": {
            "setup": {
                "experiment_id": "0484",
                "cycle_id": 2,
                "date_time": "April 25, 2025 at 11:53:40 AM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "R6_Medium_Scale_Language_Model_Serving",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "greedy",
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.01,
                    "delay_max": 0.1,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 32,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 16.346849305031355,
                        "average_latency_ms_per_batch": 4086.712326257839,
                        "throughput_queries_per_sec": 7.830255091456872,
                        "throughput_tokens_per_sec": 1002.2726517064796
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 12870114304,
                        "gpu_max_memory_allocated_bytes": 12870114304,
                        "gpu_current_memory_reserved_bytes": 20375928832,
                        "gpu_max_memory_reserved_bytes": 20375928832
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.5,
                        "cpu_memory_usage_bytes": 2664161280
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0484",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_3": 145.7727927336311,
                            "process_1": 145.56463103820187,
                            "process_0": 152.13594107154057,
                            "process_2": 115.1617384464482
                        },
                        "ram_power": {
                            "process_3": 0.9642992019653321,
                            "process_1": 0.9574828147888184,
                            "process_0": 0.928919792175293,
                            "process_2": 0.9649715423583984
                        },
                        "cpu_energy": {
                            "process_3": 0.00048765023000123616,
                            "process_1": 0.0004916589078766264,
                            "process_0": 0.0005134442350008612,
                            "process_2": 0.0004931631354011188
                        },
                        "gpu_energy": {
                            "process_3": 0.0030714394015962654,
                            "process_1": 0.00307316690297732,
                            "process_0": 0.00307316690297732,
                            "process_2": 0.0030714394015962654
                        },
                        "ram_energy": {
                            "process_3": 4.059633067503022e-06,
                            "process_1": 4.089752158442609e-06,
                            "process_0": 3.6549100919729438e-06,
                            "process_2": 4.129408690046399e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.003563149264665004,
                            "process_1": 0.00356891556301239,
                            "process_0": 0.003590266048070154,
                            "process_2": 0.00356873194568743
                        },
                        "total_energy_joules": {
                            "process_3": 12827.337352794015,
                            "process_1": 12848.096026844603,
                            "process_0": 12924.957773052554,
                            "process_2": 12847.435004474748
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 139.65877582245542,
                        "ram_power_avg": 0.9539183378219604,
                        "cpu_energy_total": 0.0019859165082798426,
                        "gpu_energy_total": 0.012289212609147171,
                        "ram_energy_total": 1.5933704007964973e-05,
                        "total_energy_kwh": 0.014291062821434978,
                        "total_energy_joules": 51447.826157165924
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.31845854769352483,
                        "joules_per_token": 3.1401261082254597,
                        "flops_per_joule": 1023144926.435968,
                        "joules_per_flop": 9.773786432029808e-10
                    },
                    "per-process_emissions": [
                        0.0013573817123741334,
                        0.00135957838372957,
                        0.0013677118510123252,
                        0.0013595084347096265
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0484": {
            "setup": {
                "experiment_id": "0484",
                "cycle_id": 2,
                "date_time": "April 25, 2025 at 11:53:40 AM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "R6_Medium_Scale_Language_Model_Serving",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "greedy",
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.01,
                    "delay_max": 0.1,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 32,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 16.346849305031355,
                        "average_latency_ms_per_batch": 4086.712326257839,
                        "throughput_queries_per_sec": 7.830255091456872,
                        "throughput_tokens_per_sec": 1002.2726517064796
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 12870114304,
                        "gpu_max_memory_allocated_bytes": 12870114304,
                        "gpu_current_memory_reserved_bytes": 20375928832,
                        "gpu_max_memory_reserved_bytes": 20375928832
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.5,
                        "cpu_memory_usage_bytes": 2664161280
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0484",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_3": 145.7727927336311,
                            "process_1": 145.56463103820187,
                            "process_0": 152.13594107154057,
                            "process_2": 115.1617384464482
                        },
                        "ram_power": {
                            "process_3": 0.9642992019653321,
                            "process_1": 0.9574828147888184,
                            "process_0": 0.928919792175293,
                            "process_2": 0.9649715423583984
                        },
                        "cpu_energy": {
                            "process_3": 0.00048765023000123616,
                            "process_1": 0.0004916589078766264,
                            "process_0": 0.0005134442350008612,
                            "process_2": 0.0004931631354011188
                        },
                        "gpu_energy": {
                            "process_3": 0.0030714394015962654,
                            "process_1": 0.00307316690297732,
                            "process_0": 0.00307316690297732,
                            "process_2": 0.0030714394015962654
                        },
                        "ram_energy": {
                            "process_3": 4.059633067503022e-06,
                            "process_1": 4.089752158442609e-06,
                            "process_0": 3.6549100919729438e-06,
                            "process_2": 4.129408690046399e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.003563149264665004,
                            "process_1": 0.00356891556301239,
                            "process_0": 0.003590266048070154,
                            "process_2": 0.00356873194568743
                        },
                        "total_energy_joules": {
                            "process_3": 12827.337352794015,
                            "process_1": 12848.096026844603,
                            "process_0": 12924.957773052554,
                            "process_2": 12847.435004474748
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 139.65877582245542,
                        "ram_power_avg": 0.9539183378219604,
                        "cpu_energy_total": 0.0019859165082798426,
                        "gpu_energy_total": 0.012289212609147171,
                        "ram_energy_total": 1.5933704007964973e-05,
                        "total_energy_kwh": 0.014291062821434978,
                        "total_energy_joules": 51447.826157165924
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.31845854769352483,
                        "joules_per_token": 3.1401261082254597,
                        "flops_per_joule": 1023144926.435968,
                        "joules_per_flop": 9.773786432029808e-10
                    },
                    "per-process_emissions": [
                        0.0013573817123741334,
                        0.00135957838372957,
                        0.0013677118510123252,
                        0.0013595084347096265
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0485": {
            "setup": {
                "experiment_id": "0485",
                "cycle_id": 2,
                "date_time": "April 25, 2025 at 11:54:35 AM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "A4_Latency_Ignorance_Exploit",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "greedy",
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 32,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 17.284788739925716,
                        "average_latency_ms_per_batch": 4321.197184981429,
                        "throughput_queries_per_sec": 7.405355189811252,
                        "throughput_tokens_per_sec": 947.8854642958403
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 12870114304,
                        "gpu_max_memory_allocated_bytes": 12870114304,
                        "gpu_current_memory_reserved_bytes": 18551406592,
                        "gpu_max_memory_reserved_bytes": 18551406592
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 1.1,
                        "cpu_memory_usage_bytes": 2665062400
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0485",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_0": 62.879976123085406
                        },
                        "ram_power": {
                            "process_0": 0.9292817115783691
                        },
                        "cpu_energy": {
                            "process_0": 0.00054685519506711
                        },
                        "gpu_energy": {
                            "process_0": 0.0007851459058958454
                        },
                        "ram_energy": {
                            "process_0": 4.425946485010555e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.0013364270474479655
                        },
                        "total_energy_joules": {
                            "process_0": 4811.137370812676
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 62.879976123085406,
                        "ram_power_avg": 0.9292817115783691,
                        "cpu_energy_total": 0.00054685519506711,
                        "gpu_energy_total": 0.0007851459058958454,
                        "ram_energy_total": 4.425946485010555e-06,
                        "total_energy_kwh": 0.0013364270474479655,
                        "total_energy_joules": 4811.137370812676
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 3.405431759108655,
                        "joules_per_token": 0.29364852116776585,
                        "flops_per_joule": 10940985104.312773,
                        "joules_per_flop": 9.139944808130805e-11
                    },
                    "per-process_emissions": [
                        0.0005091118837253025
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0485": {
            "setup": {
                "experiment_id": "0485",
                "cycle_id": 2,
                "date_time": "April 25, 2025 at 11:54:35 AM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "A4_Latency_Ignorance_Exploit",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "greedy",
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 32,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 17.284788739925716,
                        "average_latency_ms_per_batch": 4321.197184981429,
                        "throughput_queries_per_sec": 7.405355189811252,
                        "throughput_tokens_per_sec": 947.8854642958403
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 12870114304,
                        "gpu_max_memory_allocated_bytes": 12870114304,
                        "gpu_current_memory_reserved_bytes": 18551406592,
                        "gpu_max_memory_reserved_bytes": 18551406592
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 1.1,
                        "cpu_memory_usage_bytes": 2665062400
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0485",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_0": 62.879976123085406
                        },
                        "ram_power": {
                            "process_0": 0.9292817115783691
                        },
                        "cpu_energy": {
                            "process_0": 0.00054685519506711
                        },
                        "gpu_energy": {
                            "process_0": 0.0007851459058958454
                        },
                        "ram_energy": {
                            "process_0": 4.425946485010555e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.0013364270474479655
                        },
                        "total_energy_joules": {
                            "process_0": 4811.137370812676
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 62.879976123085406,
                        "ram_power_avg": 0.9292817115783691,
                        "cpu_energy_total": 0.00054685519506711,
                        "gpu_energy_total": 0.0007851459058958454,
                        "ram_energy_total": 4.425946485010555e-06,
                        "total_energy_kwh": 0.0013364270474479655,
                        "total_energy_joules": 4811.137370812676
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 3.405431759108655,
                        "joules_per_token": 0.29364852116776585,
                        "flops_per_joule": 10940985104.312773,
                        "joules_per_flop": 9.139944808130805e-11
                    },
                    "per-process_emissions": [
                        0.0005091118837253025
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0486": {
            "setup": {
                "experiment_id": "0486",
                "cycle_id": 2,
                "date_time": "April 25, 2025 at 11:55:53 AM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "A2_Precision_Minimalist",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "greedy",
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 128,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1803463680,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 16.044764503953047,
                        "average_latency_ms_per_batch": 16044.764503953047,
                        "throughput_queries_per_sec": 7.977680193963822,
                        "throughput_tokens_per_sec": 1021.1430648273692
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 3194348544,
                        "gpu_max_memory_allocated_bytes": 3194348544,
                        "gpu_current_memory_reserved_bytes": 34565259264,
                        "gpu_max_memory_reserved_bytes": 34565259264
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 1.9,
                        "cpu_memory_usage_bytes": 2285195264
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0486",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 56.201548087841196,
                            "process_1": 50.461161538897144
                        },
                        "ram_power": {
                            "process_0": 0.7966032028198242,
                            "process_1": 0.8392767906188966
                        },
                        "cpu_energy": {
                            "process_0": 0.0004734911653158634,
                            "process_1": 0.0004735232800394442
                        },
                        "gpu_energy": {
                            "process_0": 0.0014507850495153463,
                            "process_1": 0.0014491936593543642
                        },
                        "ram_energy": {
                            "process_0": 3.2907411442606535e-06,
                            "process_1": 3.4710922034873737e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.00192756695597547,
                            "process_1": 0.0019261880315972959
                        },
                        "total_energy_joules": {
                            "process_0": 6939.241041511692,
                            "process_1": 6934.2769137502655
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 53.331354813369174,
                        "ram_power_avg": 0.8179399967193604,
                        "cpu_energy_total": 0.0009470144453553077,
                        "gpu_energy_total": 0.0028999787088697104,
                        "ram_energy_total": 6.761833347748027e-06,
                        "total_energy_kwh": 0.003853754987572766,
                        "total_energy_joules": 13873.517955261957
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 1.1809549713946825,
                        "joules_per_token": 0.8467723361365941,
                        "flops_per_joule": 3794176969.288399,
                        "joules_per_flop": 2.635617706012144e-10
                    },
                    "per-process_emissions": [
                        0.0007343066318788553,
                        0.0007337813306369899
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0486": {
            "setup": {
                "experiment_id": "0486",
                "cycle_id": 2,
                "date_time": "April 25, 2025 at 11:55:53 AM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "A2_Precision_Minimalist",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "greedy",
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 128,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1803463680,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 16.044764503953047,
                        "average_latency_ms_per_batch": 16044.764503953047,
                        "throughput_queries_per_sec": 7.977680193963822,
                        "throughput_tokens_per_sec": 1021.1430648273692
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 3194348544,
                        "gpu_max_memory_allocated_bytes": 3194348544,
                        "gpu_current_memory_reserved_bytes": 34565259264,
                        "gpu_max_memory_reserved_bytes": 34565259264
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 1.9,
                        "cpu_memory_usage_bytes": 2285195264
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0486",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 56.201548087841196,
                            "process_1": 50.461161538897144
                        },
                        "ram_power": {
                            "process_0": 0.7966032028198242,
                            "process_1": 0.8392767906188966
                        },
                        "cpu_energy": {
                            "process_0": 0.0004734911653158634,
                            "process_1": 0.0004735232800394442
                        },
                        "gpu_energy": {
                            "process_0": 0.0014507850495153463,
                            "process_1": 0.0014491936593543642
                        },
                        "ram_energy": {
                            "process_0": 3.2907411442606535e-06,
                            "process_1": 3.4710922034873737e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.00192756695597547,
                            "process_1": 0.0019261880315972959
                        },
                        "total_energy_joules": {
                            "process_0": 6939.241041511692,
                            "process_1": 6934.2769137502655
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 53.331354813369174,
                        "ram_power_avg": 0.8179399967193604,
                        "cpu_energy_total": 0.0009470144453553077,
                        "gpu_energy_total": 0.0028999787088697104,
                        "ram_energy_total": 6.761833347748027e-06,
                        "total_energy_kwh": 0.003853754987572766,
                        "total_energy_joules": 13873.517955261957
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 1.1809549713946825,
                        "joules_per_token": 0.8467723361365941,
                        "flops_per_joule": 3794176969.288399,
                        "joules_per_flop": 2.635617706012144e-10
                    },
                    "per-process_emissions": [
                        0.0007343066318788553,
                        0.0007337813306369899
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0487": {
            "setup": {
                "experiment_id": "0487",
                "cycle_id": 2,
                "date_time": "April 25, 2025 at 11:56:53 AM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "A1_Max_Throughput_Exploit",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "greedy",
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 256,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 12.959604653005954,
                        "average_latency_ms_per_batch": 12959.604653005954,
                        "throughput_queries_per_sec": 9.876844504690245,
                        "throughput_tokens_per_sec": 1264.2360966003514
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 12870114304,
                        "gpu_max_memory_allocated_bytes": 12870114304,
                        "gpu_current_memory_reserved_bytes": 13509853184,
                        "gpu_max_memory_reserved_bytes": 13509853184
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            21.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.3,
                        "cpu_memory_usage_bytes": 2662486016
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0487",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_3": 0.0,
                            "process_2": 729.6270039059884,
                            "process_1": 781.9810679896974,
                            "process_0": 0.0
                        },
                        "ram_power": {
                            "process_3": 0.9642176628112793,
                            "process_2": 0.9718136787414551,
                            "process_1": 0.9639701843261719,
                            "process_0": 0.9271173477172852
                        },
                        "cpu_energy": {
                            "process_3": 0.00033358642121856976,
                            "process_2": 0.00034864445259518106,
                            "process_1": 0.00031110492865991544,
                            "process_0": 0.0003359473451600934
                        },
                        "gpu_energy": {
                            "process_3": 0.00235741549704116,
                            "process_2": 0.0022755587648877196,
                            "process_1": 0.0023449052092541223,
                            "process_0": 0.00235741549704116
                        },
                        "ram_energy": {
                            "process_3": 2.4930099918396752e-06,
                            "process_2": 2.7386930516722837e-06,
                            "process_1": 2.380521174692064e-06,
                            "process_0": 2.43403209127191e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.0026934949282515696,
                            "process_2": 0.002626941910534573,
                            "process_1": 0.0026583906590887303,
                            "process_0": 0.002695796874292525
                        },
                        "total_energy_joules": {
                            "process_3": 9696.581741705651,
                            "process_2": 9456.990877924463,
                            "process_1": 9570.206372719429,
                            "process_0": 9704.86874745309
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 377.9020179739215,
                        "ram_power_avg": 0.9567797183990479,
                        "cpu_energy_total": 0.0013292831476337595,
                        "gpu_energy_total": 0.009335294968224161,
                        "ram_energy_total": 1.0046256309475932e-05,
                        "total_energy_kwh": 0.010674624372167398,
                        "total_energy_joules": 38428.64773980263
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.4263485957386474,
                        "joules_per_token": 2.345498519275063,
                        "flops_per_joule": 1369774514.7129745,
                        "joules_per_flop": 7.300471641564612e-10
                    },
                    "per-process_emissions": [
                        0.0010260868929174356,
                        0.0010007335208181455,
                        0.0010127139215798519,
                        0.0010269638192617373
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0487": {
            "setup": {
                "experiment_id": "0487",
                "cycle_id": 2,
                "date_time": "April 25, 2025 at 11:56:53 AM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "A1_Max_Throughput_Exploit",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "greedy",
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 256,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 12.959604653005954,
                        "average_latency_ms_per_batch": 12959.604653005954,
                        "throughput_queries_per_sec": 9.876844504690245,
                        "throughput_tokens_per_sec": 1264.2360966003514
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 12870114304,
                        "gpu_max_memory_allocated_bytes": 12870114304,
                        "gpu_current_memory_reserved_bytes": 13509853184,
                        "gpu_max_memory_reserved_bytes": 13509853184
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            21.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.3,
                        "cpu_memory_usage_bytes": 2662486016
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0487",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_3": 0.0,
                            "process_2": 729.6270039059884,
                            "process_1": 781.9810679896974,
                            "process_0": 0.0
                        },
                        "ram_power": {
                            "process_3": 0.9642176628112793,
                            "process_2": 0.9718136787414551,
                            "process_1": 0.9639701843261719,
                            "process_0": 0.9271173477172852
                        },
                        "cpu_energy": {
                            "process_3": 0.00033358642121856976,
                            "process_2": 0.00034864445259518106,
                            "process_1": 0.00031110492865991544,
                            "process_0": 0.0003359473451600934
                        },
                        "gpu_energy": {
                            "process_3": 0.00235741549704116,
                            "process_2": 0.0022755587648877196,
                            "process_1": 0.0023449052092541223,
                            "process_0": 0.00235741549704116
                        },
                        "ram_energy": {
                            "process_3": 2.4930099918396752e-06,
                            "process_2": 2.7386930516722837e-06,
                            "process_1": 2.380521174692064e-06,
                            "process_0": 2.43403209127191e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.0026934949282515696,
                            "process_2": 0.002626941910534573,
                            "process_1": 0.0026583906590887303,
                            "process_0": 0.002695796874292525
                        },
                        "total_energy_joules": {
                            "process_3": 9696.581741705651,
                            "process_2": 9456.990877924463,
                            "process_1": 9570.206372719429,
                            "process_0": 9704.86874745309
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 377.9020179739215,
                        "ram_power_avg": 0.9567797183990479,
                        "cpu_energy_total": 0.0013292831476337595,
                        "gpu_energy_total": 0.009335294968224161,
                        "ram_energy_total": 1.0046256309475932e-05,
                        "total_energy_kwh": 0.010674624372167398,
                        "total_energy_joules": 38428.64773980263
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.4263485957386474,
                        "joules_per_token": 2.345498519275063,
                        "flops_per_joule": 1369774514.7129745,
                        "joules_per_flop": 7.300471641564612e-10
                    },
                    "per-process_emissions": [
                        0.0010260868929174356,
                        0.0010007335208181455,
                        0.0010127139215798519,
                        0.0010269638192617373
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0488": {
            "setup": {
                "experiment_id": "0488",
                "cycle_id": 2,
                "date_time": "April 25, 2025 at 12:07:00 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "R7_anti_platonic_ideal",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "greedy",
                    "decoder_temperature": 0.8,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.4,
                    "delay_max": 0.5,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 8
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 1,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 553.7504835675354,
                        "average_latency_ms_per_batch": 4326.17565287137,
                        "throughput_queries_per_sec": 0.23115103968011094,
                        "throughput_tokens_per_sec": 29.5873330790542
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38040240128,
                        "gpu_max_memory_reserved_bytes": 38040240128
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.5,
                        "cpu_memory_usage_bytes": 1959522304
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0488",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_2": 232.49253917868361,
                            "process_0": 464.3060579391184,
                            "process_1": 421.67270379013,
                            "process_3": 375.40093539888016
                        },
                        "ram_power": {
                            "process_2": 0.7258529663085938,
                            "process_0": 0.6831951141357423,
                            "process_1": 0.7094106674194336,
                            "process_3": 0.7254495620727539
                        },
                        "cpu_energy": {
                            "process_2": 0.015865672231522708,
                            "process_0": 0.01612164500106336,
                            "process_1": 0.01612569598966,
                            "process_3": 0.015945749120690376
                        },
                        "gpu_energy": {
                            "process_2": 0.06740190003258739,
                            "process_0": 0.06778043950208534,
                            "process_1": 0.06767573136276361,
                            "process_3": 0.06743608811549429
                        },
                        "ram_energy": {
                            "process_2": 0.0001005421022489059,
                            "process_0": 9.566318810275421e-05,
                            "process_1": 9.90058917215389e-05,
                            "process_3": 0.00010019687070694467
                        },
                        "total_energy_kwh": {
                            "process_2": 0.08336811436635881,
                            "process_0": 0.08399774769125146,
                            "process_1": 0.08390043324414527,
                            "process_3": 0.08348203410689159
                        },
                        "total_energy_joules": {
                            "process_2": 300125.21171889175,
                            "process_0": 302391.89168850525,
                            "process_1": 302041.559678923,
                            "process_3": 300535.32278480974
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 373.46805907670307,
                        "ram_power_avg": 0.7109770774841309,
                        "cpu_energy_total": 0.06405876234293645,
                        "gpu_energy_total": 0.27029415901293063,
                        "ram_energy_total": 0.0003954080527801437,
                        "total_energy_kwh": 0.3347483294086471,
                        "total_energy_joules": 1205093.9858711297
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.013595620086143283,
                        "joules_per_token": 73.55309972357969,
                        "flops_per_joule": 43680063.90042101,
                        "joules_per_flop": 2.2893739402024124e-08
                    },
                    "per-process_emissions": [
                        0.03175908316786439,
                        0.03199894198298224,
                        0.03196187004435714,
                        0.03180248089302035
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0488": {
            "setup": {
                "experiment_id": "0488",
                "cycle_id": 2,
                "date_time": "April 25, 2025 at 12:07:00 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "R7_anti_platonic_ideal",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "greedy",
                    "decoder_temperature": 0.8,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.4,
                    "delay_max": 0.5,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 8
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 1,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 553.7504835675354,
                        "average_latency_ms_per_batch": 4326.17565287137,
                        "throughput_queries_per_sec": 0.23115103968011094,
                        "throughput_tokens_per_sec": 29.5873330790542
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38040240128,
                        "gpu_max_memory_reserved_bytes": 38040240128
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.5,
                        "cpu_memory_usage_bytes": 1959522304
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0488",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_2": 232.49253917868361,
                            "process_0": 464.3060579391184,
                            "process_1": 421.67270379013,
                            "process_3": 375.40093539888016
                        },
                        "ram_power": {
                            "process_2": 0.7258529663085938,
                            "process_0": 0.6831951141357423,
                            "process_1": 0.7094106674194336,
                            "process_3": 0.7254495620727539
                        },
                        "cpu_energy": {
                            "process_2": 0.015865672231522708,
                            "process_0": 0.01612164500106336,
                            "process_1": 0.01612569598966,
                            "process_3": 0.015945749120690376
                        },
                        "gpu_energy": {
                            "process_2": 0.06740190003258739,
                            "process_0": 0.06778043950208534,
                            "process_1": 0.06767573136276361,
                            "process_3": 0.06743608811549429
                        },
                        "ram_energy": {
                            "process_2": 0.0001005421022489059,
                            "process_0": 9.566318810275421e-05,
                            "process_1": 9.90058917215389e-05,
                            "process_3": 0.00010019687070694467
                        },
                        "total_energy_kwh": {
                            "process_2": 0.08336811436635881,
                            "process_0": 0.08399774769125146,
                            "process_1": 0.08390043324414527,
                            "process_3": 0.08348203410689159
                        },
                        "total_energy_joules": {
                            "process_2": 300125.21171889175,
                            "process_0": 302391.89168850525,
                            "process_1": 302041.559678923,
                            "process_3": 300535.32278480974
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 373.46805907670307,
                        "ram_power_avg": 0.7109770774841309,
                        "cpu_energy_total": 0.06405876234293645,
                        "gpu_energy_total": 0.27029415901293063,
                        "ram_energy_total": 0.0003954080527801437,
                        "total_energy_kwh": 0.3347483294086471,
                        "total_energy_joules": 1205093.9858711297
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.013595620086143283,
                        "joules_per_token": 73.55309972357969,
                        "flops_per_joule": 43680063.90042101,
                        "joules_per_flop": 2.2893739402024124e-08
                    },
                    "per-process_emissions": [
                        0.03175908316786439,
                        0.03199894198298224,
                        0.03196187004435714,
                        0.03180248089302035
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0489": {
            "setup": {
                "experiment_id": "0489",
                "cycle_id": 2,
                "date_time": "April 25, 2025 at 12:09:42 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "R2_Low_Latency_Chatbot_Deployment",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_p",
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": 0.9
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.01,
                    "delay_max": 0.05,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 4,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 120.20984280109406,
                        "average_latency_ms_per_batch": 3756.557587534189,
                        "throughput_queries_per_sec": 1.0648046534075915,
                        "throughput_tokens_per_sec": 136.2949956361717
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 35271999488,
                        "gpu_max_memory_reserved_bytes": 35271999488
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 1.1,
                        "cpu_memory_usage_bytes": 1975029760
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0489",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_0": 37.76692243490865
                        },
                        "ram_power": {
                            "process_0": 0.688601016998291
                        },
                        "cpu_energy": {
                            "process_0": 0.0037308296632782008
                        },
                        "gpu_energy": {
                            "process_0": 0.00631307893934796
                        },
                        "ram_energy": {
                            "process_0": 2.1865871857630804e-05
                        },
                        "total_energy_kwh": {
                            "process_0": 0.010065774474483785
                        },
                        "total_energy_joules": {
                            "process_0": 36236.78810814163
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 37.76692243490865,
                        "ram_power_avg": 0.688601016998291,
                        "cpu_energy_total": 0.0037308296632782008,
                        "gpu_energy_total": 0.00631307893934796,
                        "ram_energy_total": 2.1865871857630804e-05,
                        "total_energy_kwh": 0.010065774474483785,
                        "total_energy_joules": 36236.78810814163
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.45213720242271876,
                        "joules_per_token": 2.211718024178566,
                        "flops_per_joule": 1452628255.897692,
                        "joules_per_flop": 6.884073719067389e-10
                    },
                    "per-process_emissions": [
                        0.003834556786054598
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0489": {
            "setup": {
                "experiment_id": "0489",
                "cycle_id": 2,
                "date_time": "April 25, 2025 at 12:09:42 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "R2_Low_Latency_Chatbot_Deployment",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_p",
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": 0.9
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.01,
                    "delay_max": 0.05,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 4,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 120.20984280109406,
                        "average_latency_ms_per_batch": 3756.557587534189,
                        "throughput_queries_per_sec": 1.0648046534075915,
                        "throughput_tokens_per_sec": 136.2949956361717
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 35271999488,
                        "gpu_max_memory_reserved_bytes": 35271999488
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 1.1,
                        "cpu_memory_usage_bytes": 1975029760
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0489",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_0": 37.76692243490865
                        },
                        "ram_power": {
                            "process_0": 0.688601016998291
                        },
                        "cpu_energy": {
                            "process_0": 0.0037308296632782008
                        },
                        "gpu_energy": {
                            "process_0": 0.00631307893934796
                        },
                        "ram_energy": {
                            "process_0": 2.1865871857630804e-05
                        },
                        "total_energy_kwh": {
                            "process_0": 0.010065774474483785
                        },
                        "total_energy_joules": {
                            "process_0": 36236.78810814163
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 37.76692243490865,
                        "ram_power_avg": 0.688601016998291,
                        "cpu_energy_total": 0.0037308296632782008,
                        "gpu_energy_total": 0.00631307893934796,
                        "ram_energy_total": 2.1865871857630804e-05,
                        "total_energy_kwh": 0.010065774474483785,
                        "total_energy_joules": 36236.78810814163
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.45213720242271876,
                        "joules_per_token": 2.211718024178566,
                        "flops_per_joule": 1452628255.897692,
                        "joules_per_flop": 6.884073719067389e-10
                    },
                    "per-process_emissions": [
                        0.003834556786054598
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0490": {
            "setup": {
                "experiment_id": "0490",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 12:11:04 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_28",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 28,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 29.98872142593609,
                        "average_latency_ms_per_batch": 5997.744285187218,
                        "throughput_queries_per_sec": 4.268271333812109,
                        "throughput_tokens_per_sec": 546.33873072795
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 39535509504,
                        "gpu_max_memory_reserved_bytes": 39535509504
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            27.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.5,
                        "cpu_memory_usage_bytes": 1953939456
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0490",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_3": 720.3124085615051,
                            "process_1": 0.0,
                            "process_0": 5.89504448479379,
                            "process_2": 0.0
                        },
                        "ram_power": {
                            "process_3": 0.7299370765686035,
                            "process_1": 0.7235269546508789,
                            "process_0": 0.6803483963012695,
                            "process_2": 0.7106723785400391
                        },
                        "cpu_energy": {
                            "process_3": 0.0008406406421127032,
                            "process_1": 0.0008953188128107288,
                            "process_0": 0.0009294317986586975,
                            "process_2": 0.0010376890335937789
                        },
                        "gpu_energy": {
                            "process_3": 0.007957577199388055,
                            "process_1": 0.007952504139772998,
                            "process_0": 0.00795420025224125,
                            "process_2": 0.00795371302963055
                        },
                        "ram_energy": {
                            "process_3": 5.4699447632472775e-06,
                            "process_1": 5.788598127896644e-06,
                            "process_0": 5.241099319252047e-06,
                            "process_2": 6.150095217963878e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.008803687786264008,
                            "process_1": 0.008853611550711626,
                            "process_0": 0.0088888731502192,
                            "process_2": 0.008997552158442291
                        },
                        "total_energy_joules": {
                            "process_3": 31693.27603055043,
                            "process_1": 31873.001582561854,
                            "process_0": 31999.94334078912,
                            "process_2": 32391.187770392247
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 181.5518632615747,
                        "ram_power_avg": 0.7111212015151978,
                        "cpu_energy_total": 0.003703080287175908,
                        "gpu_energy_total": 0.03181799462103285,
                        "ram_energy_total": 2.2649737428359848e-05,
                        "total_energy_kwh": 0.035543724645637126,
                        "total_energy_joules": 127957.40872429365
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.12804260545242957,
                        "joules_per_token": 7.809900434832375,
                        "flops_per_joule": 411375807.2600776,
                        "joules_per_flop": 2.4308673051543496e-09
                    },
                    "per-process_emissions": [
                        0.003353764862177274,
                        0.003372783320243594,
                        0.0033862162265760045,
                        0.003427617494758591
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0490": {
            "setup": {
                "experiment_id": "0490",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 12:11:04 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_28",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 28,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 29.98872142593609,
                        "average_latency_ms_per_batch": 5997.744285187218,
                        "throughput_queries_per_sec": 4.268271333812109,
                        "throughput_tokens_per_sec": 546.33873072795
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 39535509504,
                        "gpu_max_memory_reserved_bytes": 39535509504
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            27.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.5,
                        "cpu_memory_usage_bytes": 1953939456
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0490",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_3": 720.3124085615051,
                            "process_1": 0.0,
                            "process_0": 5.89504448479379,
                            "process_2": 0.0
                        },
                        "ram_power": {
                            "process_3": 0.7299370765686035,
                            "process_1": 0.7235269546508789,
                            "process_0": 0.6803483963012695,
                            "process_2": 0.7106723785400391
                        },
                        "cpu_energy": {
                            "process_3": 0.0008406406421127032,
                            "process_1": 0.0008953188128107288,
                            "process_0": 0.0009294317986586975,
                            "process_2": 0.0010376890335937789
                        },
                        "gpu_energy": {
                            "process_3": 0.007957577199388055,
                            "process_1": 0.007952504139772998,
                            "process_0": 0.00795420025224125,
                            "process_2": 0.00795371302963055
                        },
                        "ram_energy": {
                            "process_3": 5.4699447632472775e-06,
                            "process_1": 5.788598127896644e-06,
                            "process_0": 5.241099319252047e-06,
                            "process_2": 6.150095217963878e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.008803687786264008,
                            "process_1": 0.008853611550711626,
                            "process_0": 0.0088888731502192,
                            "process_2": 0.008997552158442291
                        },
                        "total_energy_joules": {
                            "process_3": 31693.27603055043,
                            "process_1": 31873.001582561854,
                            "process_0": 31999.94334078912,
                            "process_2": 32391.187770392247
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 181.5518632615747,
                        "ram_power_avg": 0.7111212015151978,
                        "cpu_energy_total": 0.003703080287175908,
                        "gpu_energy_total": 0.03181799462103285,
                        "ram_energy_total": 2.2649737428359848e-05,
                        "total_energy_kwh": 0.035543724645637126,
                        "total_energy_joules": 127957.40872429365
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.12804260545242957,
                        "joules_per_token": 7.809900434832375,
                        "flops_per_joule": 411375807.2600776,
                        "joules_per_flop": 2.4308673051543496e-09
                    },
                    "per-process_emissions": [
                        0.003353764862177274,
                        0.003372783320243594,
                        0.0033862162265760045,
                        0.003427617494758591
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0491": {
            "setup": {
                "experiment_id": "0491",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 12:12:32 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_100_temp_0.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 0.4,
                    "decoder_top_k": 100,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.42418770608492,
                        "average_latency_ms_per_batch": 4803.023463260615,
                        "throughput_queries_per_sec": 3.33123502776689,
                        "throughput_tokens_per_sec": 426.39808355416193
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38449184768,
                        "gpu_max_memory_reserved_bytes": 38449184768
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            27.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.5,
                        "cpu_memory_usage_bytes": 1976356864
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0491",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_3": 36866.48368739465,
                            "process_1": 167.8770777662622,
                            "process_2": 143.80587642141992,
                            "process_0": 155.30106841392987
                        },
                        "ram_power": {
                            "process_3": 0.7253508567810059,
                            "process_1": 0.7323446273803711,
                            "process_2": 0.7327265739440919,
                            "process_0": 0.6890387535095215
                        },
                        "cpu_energy": {
                            "process_3": 0.0010474925197504492,
                            "process_1": 0.0010899591996840173,
                            "process_2": 0.0010855685959104448,
                            "process_0": 0.001122314423384523
                        },
                        "gpu_energy": {
                            "process_3": 0.010225370958069746,
                            "process_1": 0.010225370958069746,
                            "process_2": 0.010212831503595865,
                            "process_0": 0.010225370958069746
                        },
                        "ram_energy": {
                            "process_3": 7.544272054527963e-06,
                            "process_1": 7.244127939701931e-06,
                            "process_2": 7.222792303915272e-06,
                            "process_0": 6.949015444147299e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.011280407749874726,
                            "process_1": 0.011322574285693466,
                            "process_2": 0.011305622891810228,
                            "process_0": 0.011354634396898412
                        },
                        "total_energy_joules": {
                            "process_3": 40609.46789954901,
                            "process_1": 40761.267428496474,
                            "process_2": 40700.24241051682,
                            "process_0": 40876.683828834284
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 9333.366927499066,
                        "ram_power_avg": 0.7198652029037476,
                        "cpu_energy_total": 0.004345334738729434,
                        "gpu_energy_total": 0.0408889443778051,
                        "ram_energy_total": 2.896020774229246e-05,
                        "total_energy_kwh": 0.04526323932427683,
                        "total_energy_joules": 162947.6615673966
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.10054762272991216,
                        "joules_per_token": 9.945535984338171,
                        "flops_per_joule": 323039814.1497245,
                        "joules_per_flop": 3.0955936581133427e-09
                    },
                    "per-process_emissions": [
                        0.004297271332314777,
                        0.004313334674134926,
                        0.004306877040635106,
                        0.00432554797349845
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0491": {
            "setup": {
                "experiment_id": "0491",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 12:12:32 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_100_temp_0.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 0.4,
                    "decoder_top_k": 100,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.42418770608492,
                        "average_latency_ms_per_batch": 4803.023463260615,
                        "throughput_queries_per_sec": 3.33123502776689,
                        "throughput_tokens_per_sec": 426.39808355416193
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38449184768,
                        "gpu_max_memory_reserved_bytes": 38449184768
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            27.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.5,
                        "cpu_memory_usage_bytes": 1976356864
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0491",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_3": 36866.48368739465,
                            "process_1": 167.8770777662622,
                            "process_2": 143.80587642141992,
                            "process_0": 155.30106841392987
                        },
                        "ram_power": {
                            "process_3": 0.7253508567810059,
                            "process_1": 0.7323446273803711,
                            "process_2": 0.7327265739440919,
                            "process_0": 0.6890387535095215
                        },
                        "cpu_energy": {
                            "process_3": 0.0010474925197504492,
                            "process_1": 0.0010899591996840173,
                            "process_2": 0.0010855685959104448,
                            "process_0": 0.001122314423384523
                        },
                        "gpu_energy": {
                            "process_3": 0.010225370958069746,
                            "process_1": 0.010225370958069746,
                            "process_2": 0.010212831503595865,
                            "process_0": 0.010225370958069746
                        },
                        "ram_energy": {
                            "process_3": 7.544272054527963e-06,
                            "process_1": 7.244127939701931e-06,
                            "process_2": 7.222792303915272e-06,
                            "process_0": 6.949015444147299e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.011280407749874726,
                            "process_1": 0.011322574285693466,
                            "process_2": 0.011305622891810228,
                            "process_0": 0.011354634396898412
                        },
                        "total_energy_joules": {
                            "process_3": 40609.46789954901,
                            "process_1": 40761.267428496474,
                            "process_2": 40700.24241051682,
                            "process_0": 40876.683828834284
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 9333.366927499066,
                        "ram_power_avg": 0.7198652029037476,
                        "cpu_energy_total": 0.004345334738729434,
                        "gpu_energy_total": 0.0408889443778051,
                        "ram_energy_total": 2.896020774229246e-05,
                        "total_energy_kwh": 0.04526323932427683,
                        "total_energy_joules": 162947.6615673966
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.10054762272991216,
                        "joules_per_token": 9.945535984338171,
                        "flops_per_joule": 323039814.1497245,
                        "joules_per_flop": 3.0955936581133427e-09
                    },
                    "per-process_emissions": [
                        0.004297271332314777,
                        0.004313334674134926,
                        0.004306877040635106,
                        0.00432554797349845
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0492": {
            "setup": {
                "experiment_id": "0492",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 12:14:01 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_300_temp_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 0.8,
                    "decoder_top_k": 300,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.39214031101437,
                        "average_latency_ms_per_batch": 4799.017538876797,
                        "throughput_queries_per_sec": 3.3340157376763364,
                        "throughput_tokens_per_sec": 426.75401442257106
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38449184768,
                        "gpu_max_memory_reserved_bytes": 38449184768
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.5,
                        "cpu_memory_usage_bytes": 1956110336
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0492",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_2": 209.0189940405581,
                            "process_3": 177.8920787005605,
                            "process_1": 178.7457111348746,
                            "process_0": 1127.3931836405136
                        },
                        "ram_power": {
                            "process_2": 0.7324719429016113,
                            "process_3": 0.7323775291442871,
                            "process_1": 0.7183141708374023,
                            "process_0": 0.6807346343994141
                        },
                        "cpu_energy": {
                            "process_2": 0.0012232078069446296,
                            "process_3": 0.0012258832491988869,
                            "process_1": 0.00124858359494101,
                            "process_0": 0.001189043277912788
                        },
                        "gpu_energy": {
                            "process_2": 0.010246531808331127,
                            "process_3": 0.010254279036749026,
                            "process_1": 0.010254279036749026,
                            "process_0": 0.01022924096116462
                        },
                        "ram_energy": {
                            "process_2": 7.644515897746445e-06,
                            "process_3": 7.654892981206058e-06,
                            "process_1": 7.230584068114789e-06,
                            "process_0": 6.878189260736437e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.011477384131173498,
                            "process_3": 0.011487817178929117,
                            "process_1": 0.011510093215758152,
                            "process_0": 0.01142516242833814
                        },
                        "total_energy_joules": {
                            "process_2": 41318.58287222459,
                            "process_3": 41356.14184414482,
                            "process_1": 41436.33557672935,
                            "process_0": 41130.5847420173
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 423.2624918791267,
                        "ram_power_avg": 0.7159745693206787,
                        "cpu_energy_total": 0.0048867179289973145,
                        "gpu_energy_total": 0.0409843308429938,
                        "ram_energy_total": 2.940818220780373e-05,
                        "total_energy_kwh": 0.04590045695419891,
                        "total_energy_joules": 165241.64503511606
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09915176042043264,
                        "joules_per_token": 10.085549623725345,
                        "flops_per_joule": 318555182.00437665,
                        "joules_per_flop": 3.139173545091666e-09
                    },
                    "per-process_emissions": [
                        0.0043723094847705445,
                        0.004376283954313047,
                        0.004384770010543068,
                        0.004352415627075414
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0492": {
            "setup": {
                "experiment_id": "0492",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 12:14:01 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_300_temp_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 0.8,
                    "decoder_top_k": 300,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.39214031101437,
                        "average_latency_ms_per_batch": 4799.017538876797,
                        "throughput_queries_per_sec": 3.3340157376763364,
                        "throughput_tokens_per_sec": 426.75401442257106
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38449184768,
                        "gpu_max_memory_reserved_bytes": 38449184768
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.5,
                        "cpu_memory_usage_bytes": 1956110336
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0492",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_2": 209.0189940405581,
                            "process_3": 177.8920787005605,
                            "process_1": 178.7457111348746,
                            "process_0": 1127.3931836405136
                        },
                        "ram_power": {
                            "process_2": 0.7324719429016113,
                            "process_3": 0.7323775291442871,
                            "process_1": 0.7183141708374023,
                            "process_0": 0.6807346343994141
                        },
                        "cpu_energy": {
                            "process_2": 0.0012232078069446296,
                            "process_3": 0.0012258832491988869,
                            "process_1": 0.00124858359494101,
                            "process_0": 0.001189043277912788
                        },
                        "gpu_energy": {
                            "process_2": 0.010246531808331127,
                            "process_3": 0.010254279036749026,
                            "process_1": 0.010254279036749026,
                            "process_0": 0.01022924096116462
                        },
                        "ram_energy": {
                            "process_2": 7.644515897746445e-06,
                            "process_3": 7.654892981206058e-06,
                            "process_1": 7.230584068114789e-06,
                            "process_0": 6.878189260736437e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.011477384131173498,
                            "process_3": 0.011487817178929117,
                            "process_1": 0.011510093215758152,
                            "process_0": 0.01142516242833814
                        },
                        "total_energy_joules": {
                            "process_2": 41318.58287222459,
                            "process_3": 41356.14184414482,
                            "process_1": 41436.33557672935,
                            "process_0": 41130.5847420173
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 423.2624918791267,
                        "ram_power_avg": 0.7159745693206787,
                        "cpu_energy_total": 0.0048867179289973145,
                        "gpu_energy_total": 0.0409843308429938,
                        "ram_energy_total": 2.940818220780373e-05,
                        "total_energy_kwh": 0.04590045695419891,
                        "total_energy_joules": 165241.64503511606
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09915176042043264,
                        "joules_per_token": 10.085549623725345,
                        "flops_per_joule": 318555182.00437665,
                        "joules_per_flop": 3.139173545091666e-09
                    },
                    "per-process_emissions": [
                        0.0043723094847705445,
                        0.004376283954313047,
                        0.004384770010543068,
                        0.004352415627075414
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0493": {
            "setup": {
                "experiment_id": "0493",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 12:15:30 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_100_temp_1.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 1.2,
                    "decoder_top_k": 100,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.308279268152546,
                        "average_latency_ms_per_batch": 4788.534908519068,
                        "throughput_queries_per_sec": 3.3413142653580987,
                        "throughput_tokens_per_sec": 427.68822596583664
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38403047424,
                        "gpu_max_memory_reserved_bytes": 38403047424
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            20.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.6,
                        "cpu_memory_usage_bytes": 1974792192
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0493",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_1": 171.8685279506377,
                            "process_2": 196.0539077982686,
                            "process_0": 186.6350460284309,
                            "process_3": 23891.313805363174
                        },
                        "ram_power": {
                            "process_1": 0.7247400283813477,
                            "process_2": 0.7325978279113771,
                            "process_0": 0.6885294914245605,
                            "process_3": 0.7323246002197266
                        },
                        "cpu_energy": {
                            "process_1": 0.0010913614631408563,
                            "process_2": 0.0010587733543652577,
                            "process_0": 0.0011160664331164299,
                            "process_3": 0.0010417184316193016
                        },
                        "gpu_energy": {
                            "process_1": 0.010258239039922401,
                            "process_2": 0.010256860427707792,
                            "process_0": 0.010256860427707792,
                            "process_3": 0.010261999598485616
                        },
                        "ram_energy": {
                            "process_1": 7.147355953626079e-06,
                            "process_2": 6.268649696896131e-06,
                            "process_0": 6.219380030671223e-06,
                            "process_3": 7.219474709145414e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.011356747859016886,
                            "process_2": 0.011321902431769946,
                            "process_0": 0.011379146240854898,
                            "process_3": 0.01131093750481406
                        },
                        "total_energy_joules": {
                            "process_1": 40884.29229246079,
                            "process_2": 40758.84875437181,
                            "process_0": 40964.92646707763,
                            "process_3": 40719.37501733062
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 6111.467821785128,
                        "ram_power_avg": 0.7195479869842529,
                        "cpu_energy_total": 0.004307919682241845,
                        "gpu_energy_total": 0.0410339594938236,
                        "ram_energy_total": 2.6854860390338847e-05,
                        "total_energy_kwh": 0.04536873403645579,
                        "total_energy_joules": 163327.44253124087
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.10031382201350583,
                        "joules_per_token": 9.968715974807182,
                        "flops_per_joule": 322288658.2504065,
                        "joules_per_flop": 3.102808536386771e-09
                    },
                    "per-process_emissions": [
                        0.0043263530968924825,
                        0.004313078731382761,
                        0.004334885760453673,
                        0.004308901642458916
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0493": {
            "setup": {
                "experiment_id": "0493",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 12:15:30 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_100_temp_1.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 1.2,
                    "decoder_top_k": 100,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.308279268152546,
                        "average_latency_ms_per_batch": 4788.534908519068,
                        "throughput_queries_per_sec": 3.3413142653580987,
                        "throughput_tokens_per_sec": 427.68822596583664
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38403047424,
                        "gpu_max_memory_reserved_bytes": 38403047424
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            20.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.6,
                        "cpu_memory_usage_bytes": 1974792192
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0493",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_1": 171.8685279506377,
                            "process_2": 196.0539077982686,
                            "process_0": 186.6350460284309,
                            "process_3": 23891.313805363174
                        },
                        "ram_power": {
                            "process_1": 0.7247400283813477,
                            "process_2": 0.7325978279113771,
                            "process_0": 0.6885294914245605,
                            "process_3": 0.7323246002197266
                        },
                        "cpu_energy": {
                            "process_1": 0.0010913614631408563,
                            "process_2": 0.0010587733543652577,
                            "process_0": 0.0011160664331164299,
                            "process_3": 0.0010417184316193016
                        },
                        "gpu_energy": {
                            "process_1": 0.010258239039922401,
                            "process_2": 0.010256860427707792,
                            "process_0": 0.010256860427707792,
                            "process_3": 0.010261999598485616
                        },
                        "ram_energy": {
                            "process_1": 7.147355953626079e-06,
                            "process_2": 6.268649696896131e-06,
                            "process_0": 6.219380030671223e-06,
                            "process_3": 7.219474709145414e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.011356747859016886,
                            "process_2": 0.011321902431769946,
                            "process_0": 0.011379146240854898,
                            "process_3": 0.01131093750481406
                        },
                        "total_energy_joules": {
                            "process_1": 40884.29229246079,
                            "process_2": 40758.84875437181,
                            "process_0": 40964.92646707763,
                            "process_3": 40719.37501733062
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 6111.467821785128,
                        "ram_power_avg": 0.7195479869842529,
                        "cpu_energy_total": 0.004307919682241845,
                        "gpu_energy_total": 0.0410339594938236,
                        "ram_energy_total": 2.6854860390338847e-05,
                        "total_energy_kwh": 0.04536873403645579,
                        "total_energy_joules": 163327.44253124087
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.10031382201350583,
                        "joules_per_token": 9.968715974807182,
                        "flops_per_joule": 322288658.2504065,
                        "joules_per_flop": 3.102808536386771e-09
                    },
                    "per-process_emissions": [
                        0.0043263530968924825,
                        0.004313078731382761,
                        0.004334885760453673,
                        0.004308901642458916
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0494": {
            "setup": {
                "experiment_id": "0494",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 12:16:59 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_400_temp_0.6",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 0.6,
                    "decoder_top_k": 400,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.3099511321052,
                        "average_latency_ms_per_batch": 4788.74389151315,
                        "throughput_queries_per_sec": 3.341168448861088,
                        "throughput_tokens_per_sec": 427.66956145421926
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38449184768,
                        "gpu_max_memory_reserved_bytes": 38449184768
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            23.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.5,
                        "cpu_memory_usage_bytes": 1978298368
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0494",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_1": 168.3829699221034,
                            "process_3": 145.907283890704,
                            "process_2": 165.92540551446444,
                            "process_0": 161.95888514931636
                        },
                        "ram_power": {
                            "process_1": 0.7247500419616699,
                            "process_3": 0.7086353302001953,
                            "process_2": 0.7314906120300293,
                            "process_0": 0.6897668838500977
                        },
                        "cpu_energy": {
                            "process_1": 0.001206147848062756,
                            "process_3": 0.001236345788784092,
                            "process_2": 0.0012320706401624192,
                            "process_0": 0.0011654297248060174
                        },
                        "gpu_energy": {
                            "process_1": 0.010254214592253064,
                            "process_3": 0.010258501540128862,
                            "process_2": 0.01024769653148283,
                            "process_0": 0.010251983479358628
                        },
                        "ram_energy": {
                            "process_1": 7.1213432836147175e-06,
                            "process_3": 7.135640348913704e-06,
                            "process_2": 7.366627191289701e-06,
                            "process_0": 6.5350147577546515e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.01146748378359943,
                            "process_3": 0.011501982969261866,
                            "process_2": 0.011487133798836542,
                            "process_0": 0.011423948218922399
                        },
                        "total_energy_joules": {
                            "process_1": 41282.94162095795,
                            "process_3": 41407.13868934272,
                            "process_2": 41353.68167581155,
                            "process_0": 41126.213588120634
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 160.54363611914704,
                        "ram_power_avg": 0.713660717010498,
                        "cpu_energy_total": 0.004839994001815285,
                        "gpu_energy_total": 0.041012396143223384,
                        "ram_energy_total": 2.8158625581572774e-05,
                        "total_energy_kwh": 0.045880548770620234,
                        "total_energy_joules": 165169.97557423287
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09919478369503353,
                        "joules_per_token": 10.081175266981987,
                        "flops_per_joule": 318693407.3572377,
                        "joules_per_flop": 3.1378120065064767e-09
                    },
                    "per-process_emissions": [
                        0.004368537947362203,
                        0.004381680412140308,
                        0.004376023620666781,
                        0.004351953073998488
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0494": {
            "setup": {
                "experiment_id": "0494",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 12:16:59 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_400_temp_0.6",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 0.6,
                    "decoder_top_k": 400,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.3099511321052,
                        "average_latency_ms_per_batch": 4788.74389151315,
                        "throughput_queries_per_sec": 3.341168448861088,
                        "throughput_tokens_per_sec": 427.66956145421926
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38449184768,
                        "gpu_max_memory_reserved_bytes": 38449184768
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            23.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.5,
                        "cpu_memory_usage_bytes": 1978298368
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0494",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_1": 168.3829699221034,
                            "process_3": 145.907283890704,
                            "process_2": 165.92540551446444,
                            "process_0": 161.95888514931636
                        },
                        "ram_power": {
                            "process_1": 0.7247500419616699,
                            "process_3": 0.7086353302001953,
                            "process_2": 0.7314906120300293,
                            "process_0": 0.6897668838500977
                        },
                        "cpu_energy": {
                            "process_1": 0.001206147848062756,
                            "process_3": 0.001236345788784092,
                            "process_2": 0.0012320706401624192,
                            "process_0": 0.0011654297248060174
                        },
                        "gpu_energy": {
                            "process_1": 0.010254214592253064,
                            "process_3": 0.010258501540128862,
                            "process_2": 0.01024769653148283,
                            "process_0": 0.010251983479358628
                        },
                        "ram_energy": {
                            "process_1": 7.1213432836147175e-06,
                            "process_3": 7.135640348913704e-06,
                            "process_2": 7.366627191289701e-06,
                            "process_0": 6.5350147577546515e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.01146748378359943,
                            "process_3": 0.011501982969261866,
                            "process_2": 0.011487133798836542,
                            "process_0": 0.011423948218922399
                        },
                        "total_energy_joules": {
                            "process_1": 41282.94162095795,
                            "process_3": 41407.13868934272,
                            "process_2": 41353.68167581155,
                            "process_0": 41126.213588120634
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 160.54363611914704,
                        "ram_power_avg": 0.713660717010498,
                        "cpu_energy_total": 0.004839994001815285,
                        "gpu_energy_total": 0.041012396143223384,
                        "ram_energy_total": 2.8158625581572774e-05,
                        "total_energy_kwh": 0.045880548770620234,
                        "total_energy_joules": 165169.97557423287
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09919478369503353,
                        "joules_per_token": 10.081175266981987,
                        "flops_per_joule": 318693407.3572377,
                        "joules_per_flop": 3.1378120065064767e-09
                    },
                    "per-process_emissions": [
                        0.004368537947362203,
                        0.004381680412140308,
                        0.004376023620666781,
                        0.004351953073998488
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0495": {
            "setup": {
                "experiment_id": "0495",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 12:18:31 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_topp_0.5_temp_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_p",
                    "decoder_temperature": 0.8,
                    "decoder_top_k": null,
                    "decoder_top_p": 0.5
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 39.05556916107889,
                        "average_latency_ms_per_batch": 4881.946145134862,
                        "throughput_queries_per_sec": 3.2773815040841683,
                        "throughput_tokens_per_sec": 419.50483252277354
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38403047424,
                        "gpu_max_memory_reserved_bytes": 38403047424
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            22.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.9,
                        "cpu_memory_usage_bytes": 1980805120
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0495",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_3": 534.0428500565058,
                            "process_1": 682.1739562561871,
                            "process_0": 982.310655178699,
                            "process_2": 3959.2788257423354
                        },
                        "ram_power": {
                            "process_3": 0.718846321105957,
                            "process_1": 0.7109270095825195,
                            "process_0": 0.689424991607666,
                            "process_2": 0.7097125053405762
                        },
                        "cpu_energy": {
                            "process_3": 0.001075270246652508,
                            "process_1": 0.0011347147013748326,
                            "process_0": 0.0010975996621509696,
                            "process_2": 0.0011373359608769535
                        },
                        "gpu_energy": {
                            "process_3": 0.01040353026726315,
                            "process_1": 0.010391661646654171,
                            "process_0": 0.010381426638467417,
                            "process_2": 0.01034803688953545
                        },
                        "ram_energy": {
                            "process_3": 7.403398182494715e-06,
                            "process_1": 7.285994949846637e-06,
                            "process_0": 7.237291457566746e-06,
                            "process_2": 7.30574100290362e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.011486203912098158,
                            "process_1": 0.01153366234297885,
                            "process_0": 0.01148626359207595,
                            "process_2": 0.011492678591415314
                        },
                        "total_energy_joules": {
                            "process_3": 41350.33408355337,
                            "process_1": 41521.18443472386,
                            "process_0": 41350.54893147342,
                            "process_2": 41373.64292909513
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1539.4515718084317,
                        "ram_power_avg": 0.7072277069091797,
                        "cpu_energy_total": 0.004444920571055264,
                        "gpu_energy_total": 0.04152465544192019,
                        "ram_energy_total": 2.923242559281172e-05,
                        "total_energy_kwh": 0.04599880843856827,
                        "total_energy_joules": 165595.71037884578
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09893976095465933,
                        "joules_per_token": 10.107160057302599,
                        "flops_per_joule": 317874069.2523904,
                        "joules_per_flop": 3.1458998915888454e-09
                    },
                    "per-process_emissions": [
                        0.004375669380313793,
                        0.004393748669557793,
                        0.004375692115401334,
                        0.004378135909399664
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0495": {
            "setup": {
                "experiment_id": "0495",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 12:18:31 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_topp_0.5_temp_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_p",
                    "decoder_temperature": 0.8,
                    "decoder_top_k": null,
                    "decoder_top_p": 0.5
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 39.05556916107889,
                        "average_latency_ms_per_batch": 4881.946145134862,
                        "throughput_queries_per_sec": 3.2773815040841683,
                        "throughput_tokens_per_sec": 419.50483252277354
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38403047424,
                        "gpu_max_memory_reserved_bytes": 38403047424
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            22.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.9,
                        "cpu_memory_usage_bytes": 1980805120
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0495",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_3": 534.0428500565058,
                            "process_1": 682.1739562561871,
                            "process_0": 982.310655178699,
                            "process_2": 3959.2788257423354
                        },
                        "ram_power": {
                            "process_3": 0.718846321105957,
                            "process_1": 0.7109270095825195,
                            "process_0": 0.689424991607666,
                            "process_2": 0.7097125053405762
                        },
                        "cpu_energy": {
                            "process_3": 0.001075270246652508,
                            "process_1": 0.0011347147013748326,
                            "process_0": 0.0010975996621509696,
                            "process_2": 0.0011373359608769535
                        },
                        "gpu_energy": {
                            "process_3": 0.01040353026726315,
                            "process_1": 0.010391661646654171,
                            "process_0": 0.010381426638467417,
                            "process_2": 0.01034803688953545
                        },
                        "ram_energy": {
                            "process_3": 7.403398182494715e-06,
                            "process_1": 7.285994949846637e-06,
                            "process_0": 7.237291457566746e-06,
                            "process_2": 7.30574100290362e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.011486203912098158,
                            "process_1": 0.01153366234297885,
                            "process_0": 0.01148626359207595,
                            "process_2": 0.011492678591415314
                        },
                        "total_energy_joules": {
                            "process_3": 41350.33408355337,
                            "process_1": 41521.18443472386,
                            "process_0": 41350.54893147342,
                            "process_2": 41373.64292909513
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1539.4515718084317,
                        "ram_power_avg": 0.7072277069091797,
                        "cpu_energy_total": 0.004444920571055264,
                        "gpu_energy_total": 0.04152465544192019,
                        "ram_energy_total": 2.923242559281172e-05,
                        "total_energy_kwh": 0.04599880843856827,
                        "total_energy_joules": 165595.71037884578
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09893976095465933,
                        "joules_per_token": 10.107160057302599,
                        "flops_per_joule": 317874069.2523904,
                        "joules_per_flop": 3.1458998915888454e-09
                    },
                    "per-process_emissions": [
                        0.004375669380313793,
                        0.004393748669557793,
                        0.004375692115401334,
                        0.004378135909399664
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0496": {
            "setup": {
                "experiment_id": "0496",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 12:20:02 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_topp_0.9_temp_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_p",
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": 0.9
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.89204676199006,
                        "average_latency_ms_per_batch": 4861.505845248757,
                        "throughput_queries_per_sec": 3.291161321062096,
                        "throughput_tokens_per_sec": 421.26864909594826
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38403047424,
                        "gpu_max_memory_reserved_bytes": 38403047424
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.3,
                        "cpu_memory_usage_bytes": 1980309504
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0496",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_3": 629.4550808633895,
                            "process_0": 2265.7332313667193,
                            "process_1": 2587.0191368257024,
                            "process_2": 3937.2631057149656
                        },
                        "ram_power": {
                            "process_3": 0.7326335906982422,
                            "process_0": 0.6904506683349609,
                            "process_1": 0.7341928482055664,
                            "process_2": 0.7197704315185547
                        },
                        "cpu_energy": {
                            "process_3": 0.0011788846554736665,
                            "process_0": 0.0011819320178110503,
                            "process_1": 0.0011348407487203076,
                            "process_2": 0.0010982005531459437
                        },
                        "gpu_energy": {
                            "process_3": 0.010395956927869321,
                            "process_0": 0.010376076078632224,
                            "process_1": 0.010374208577138333,
                            "process_2": 0.010352053837195285
                        },
                        "ram_energy": {
                            "process_3": 7.108759146319677e-06,
                            "process_0": 6.6551632762022575e-06,
                            "process_1": 6.769344128337274e-06,
                            "process_2": 6.395774962044383e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.011581950342489307,
                            "process_0": 0.011564663259719473,
                            "process_1": 0.011515818669986976,
                            "process_2": 0.011456650165303268
                        },
                        "total_energy_joules": {
                            "process_3": 41695.0212329615,
                            "process_0": 41632.7877349901,
                            "process_1": 41456.94721195311,
                            "process_2": 41243.940595091764
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 2354.8676386926945,
                        "ram_power_avg": 0.719261884689331,
                        "cpu_energy_total": 0.0045938579751509685,
                        "gpu_energy_total": 0.04149829542083516,
                        "ram_energy_total": 2.6929041512903587e-05,
                        "total_energy_kwh": 0.04611908243749903,
                        "total_energy_joules": 166028.69677499647
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09868173585801097,
                        "joules_per_token": 10.13358744964578,
                        "flops_per_joule": 317045085.1650077,
                        "joules_per_flop": 3.1541255385792996e-09
                    },
                    "per-process_emissions": [
                        0.004412143982971302,
                        0.004405558468790133,
                        0.004386951122331539,
                        0.00436441088047228
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0496": {
            "setup": {
                "experiment_id": "0496",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 12:20:02 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_topp_0.9_temp_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_p",
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": 0.9
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.89204676199006,
                        "average_latency_ms_per_batch": 4861.505845248757,
                        "throughput_queries_per_sec": 3.291161321062096,
                        "throughput_tokens_per_sec": 421.26864909594826
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38403047424,
                        "gpu_max_memory_reserved_bytes": 38403047424
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.3,
                        "cpu_memory_usage_bytes": 1980309504
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0496",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_3": 629.4550808633895,
                            "process_0": 2265.7332313667193,
                            "process_1": 2587.0191368257024,
                            "process_2": 3937.2631057149656
                        },
                        "ram_power": {
                            "process_3": 0.7326335906982422,
                            "process_0": 0.6904506683349609,
                            "process_1": 0.7341928482055664,
                            "process_2": 0.7197704315185547
                        },
                        "cpu_energy": {
                            "process_3": 0.0011788846554736665,
                            "process_0": 0.0011819320178110503,
                            "process_1": 0.0011348407487203076,
                            "process_2": 0.0010982005531459437
                        },
                        "gpu_energy": {
                            "process_3": 0.010395956927869321,
                            "process_0": 0.010376076078632224,
                            "process_1": 0.010374208577138333,
                            "process_2": 0.010352053837195285
                        },
                        "ram_energy": {
                            "process_3": 7.108759146319677e-06,
                            "process_0": 6.6551632762022575e-06,
                            "process_1": 6.769344128337274e-06,
                            "process_2": 6.395774962044383e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.011581950342489307,
                            "process_0": 0.011564663259719473,
                            "process_1": 0.011515818669986976,
                            "process_2": 0.011456650165303268
                        },
                        "total_energy_joules": {
                            "process_3": 41695.0212329615,
                            "process_0": 41632.7877349901,
                            "process_1": 41456.94721195311,
                            "process_2": 41243.940595091764
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 2354.8676386926945,
                        "ram_power_avg": 0.719261884689331,
                        "cpu_energy_total": 0.0045938579751509685,
                        "gpu_energy_total": 0.04149829542083516,
                        "ram_energy_total": 2.6929041512903587e-05,
                        "total_energy_kwh": 0.04611908243749903,
                        "total_energy_joules": 166028.69677499647
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09868173585801097,
                        "joules_per_token": 10.13358744964578,
                        "flops_per_joule": 317045085.1650077,
                        "joules_per_flop": 3.1541255385792996e-09
                    },
                    "per-process_emissions": [
                        0.004412143982971302,
                        0.004405558468790133,
                        0.004386951122331539,
                        0.00436441088047228
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0497": {
            "setup": {
                "experiment_id": "0497",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 12:21:31 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_topp_0.8_temp_1.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_p",
                    "decoder_temperature": 1.4,
                    "decoder_top_k": null,
                    "decoder_top_p": 0.8
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.96880467509618,
                        "average_latency_ms_per_batch": 4871.1005843870225,
                        "throughput_queries_per_sec": 3.2846786312078247,
                        "throughput_tokens_per_sec": 420.43886479460156
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38403047424,
                        "gpu_max_memory_reserved_bytes": 38403047424
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.8,
                        "cpu_memory_usage_bytes": 1979744256
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0497",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 0.0,
                            "process_2": 2377.183043027529,
                            "process_1": 13.205056017685104,
                            "process_3": 436.1529100714976
                        },
                        "ram_power": {
                            "process_0": 0.6901946067810059,
                            "process_2": 0.7272763252258301,
                            "process_1": 0.726555347442627,
                            "process_3": 0.7333216667175293
                        },
                        "cpu_energy": {
                            "process_0": 0.00123539687446646,
                            "process_2": 0.0011885089758088723,
                            "process_1": 0.0011786018552611495,
                            "process_3": 0.001170076890246492
                        },
                        "gpu_energy": {
                            "process_0": 0.010396838595243096,
                            "process_2": 0.010369660240165501,
                            "process_1": 0.010396838595243096,
                            "process_3": 0.010420072502714817
                        },
                        "ram_energy": {
                            "process_0": 6.933807992788305e-06,
                            "process_2": 7.032609498926734e-06,
                            "process_1": 7.295980039928302e-06,
                            "process_3": 7.314554931422574e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.011639169277702343,
                            "process_2": 0.011565201825473295,
                            "process_1": 0.011582736430544173,
                            "process_3": 0.01159746394789273
                        },
                        "total_energy_joules": {
                            "process_0": 41901.009399728435,
                            "process_2": 41634.72657170386,
                            "process_1": 41697.851149959024,
                            "process_3": 41750.87021241383
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 706.6352522791778,
                        "ram_power_avg": 0.719336986541748,
                        "cpu_energy_total": 0.004772584595782974,
                        "gpu_energy_total": 0.04158340993336651,
                        "ram_energy_total": 2.8576952463065915e-05,
                        "total_energy_kwh": 0.04638457148161254,
                        "total_energy_joules": 166984.45733380516
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09811691615853844,
                        "joules_per_token": 10.191922444690256,
                        "flops_per_joule": 315230430.1210409,
                        "joules_per_flop": 3.1722825731514057e-09
                    },
                    "per-process_emissions": [
                        0.004433941536340708,
                        0.004405763635414051,
                        0.004412443443215803,
                        0.004418053890949736
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0497": {
            "setup": {
                "experiment_id": "0497",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 12:21:31 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_topp_0.8_temp_1.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_p",
                    "decoder_temperature": 1.4,
                    "decoder_top_k": null,
                    "decoder_top_p": 0.8
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.96880467509618,
                        "average_latency_ms_per_batch": 4871.1005843870225,
                        "throughput_queries_per_sec": 3.2846786312078247,
                        "throughput_tokens_per_sec": 420.43886479460156
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38403047424,
                        "gpu_max_memory_reserved_bytes": 38403047424
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.8,
                        "cpu_memory_usage_bytes": 1979744256
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0497",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 0.0,
                            "process_2": 2377.183043027529,
                            "process_1": 13.205056017685104,
                            "process_3": 436.1529100714976
                        },
                        "ram_power": {
                            "process_0": 0.6901946067810059,
                            "process_2": 0.7272763252258301,
                            "process_1": 0.726555347442627,
                            "process_3": 0.7333216667175293
                        },
                        "cpu_energy": {
                            "process_0": 0.00123539687446646,
                            "process_2": 0.0011885089758088723,
                            "process_1": 0.0011786018552611495,
                            "process_3": 0.001170076890246492
                        },
                        "gpu_energy": {
                            "process_0": 0.010396838595243096,
                            "process_2": 0.010369660240165501,
                            "process_1": 0.010396838595243096,
                            "process_3": 0.010420072502714817
                        },
                        "ram_energy": {
                            "process_0": 6.933807992788305e-06,
                            "process_2": 7.032609498926734e-06,
                            "process_1": 7.295980039928302e-06,
                            "process_3": 7.314554931422574e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.011639169277702343,
                            "process_2": 0.011565201825473295,
                            "process_1": 0.011582736430544173,
                            "process_3": 0.01159746394789273
                        },
                        "total_energy_joules": {
                            "process_0": 41901.009399728435,
                            "process_2": 41634.72657170386,
                            "process_1": 41697.851149959024,
                            "process_3": 41750.87021241383
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 706.6352522791778,
                        "ram_power_avg": 0.719336986541748,
                        "cpu_energy_total": 0.004772584595782974,
                        "gpu_energy_total": 0.04158340993336651,
                        "ram_energy_total": 2.8576952463065915e-05,
                        "total_energy_kwh": 0.04638457148161254,
                        "total_energy_joules": 166984.45733380516
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09811691615853844,
                        "joules_per_token": 10.191922444690256,
                        "flops_per_joule": 315230430.1210409,
                        "joules_per_flop": 3.1722825731514057e-09
                    },
                    "per-process_emissions": [
                        0.004433941536340708,
                        0.004405763635414051,
                        0.004412443443215803,
                        0.004418053890949736
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0498": {
            "setup": {
                "experiment_id": "0498",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 12:23:02 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_topp_0.7_temp_1.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_p",
                    "decoder_temperature": 1.4,
                    "decoder_top_k": null,
                    "decoder_top_p": 0.7
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.8393861069344,
                        "average_latency_ms_per_batch": 4854.9232633668,
                        "throughput_queries_per_sec": 3.2956236653067705,
                        "throughput_tokens_per_sec": 421.8398291592666
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38403047424,
                        "gpu_max_memory_reserved_bytes": 38403047424
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            18.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.0,
                        "cpu_memory_usage_bytes": 1979297792
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0498",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_3": 459.78360892810787,
                            "process_0": 963.9614585945693,
                            "process_1": 964.882412251197,
                            "process_2": 1339.261908328238
                        },
                        "ram_power": {
                            "process_3": 0.7342815399169922,
                            "process_0": 0.6889500617980957,
                            "process_1": 0.7334403991699219,
                            "process_2": 0.7328009605407715
                        },
                        "cpu_energy": {
                            "process_3": 0.0012291597428047683,
                            "process_0": 0.0011598204269757841,
                            "process_1": 0.0011694721927433423,
                            "process_2": 0.001217169557436137
                        },
                        "gpu_energy": {
                            "process_3": 0.01041155499590296,
                            "process_0": 0.010370588852021356,
                            "process_1": 0.01038208441677213,
                            "process_2": 0.010347467722411352
                        },
                        "ram_energy": {
                            "process_3": 7.353130681313656e-06,
                            "process_0": 6.900074915760785e-06,
                            "process_1": 7.399463416904615e-06,
                            "process_2": 7.301511849870363e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.01164806786938904,
                            "process_0": 0.011537309353912902,
                            "process_1": 0.011558956072932374,
                            "process_2": 0.011571938791697357
                        },
                        "total_energy_joules": {
                            "process_3": 41933.04432980054,
                            "process_0": 41534.31367408644,
                            "process_1": 41612.24186255655,
                            "process_2": 41658.97965011049
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 931.972347025528,
                        "ram_power_avg": 0.7223682403564453,
                        "cpu_energy_total": 0.004775621919960032,
                        "gpu_energy_total": 0.0415116959871078,
                        "ram_energy_total": 2.895418086384942e-05,
                        "total_energy_kwh": 0.046316272087931674,
                        "total_energy_joules": 166738.57951655402
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09826160236883495,
                        "joules_per_token": 10.176915253695924,
                        "flops_per_joule": 315695278.5701162,
                        "joules_per_flop": 3.1676115161725453e-09
                    },
                    "per-process_emissions": [
                        0.004437331454843755,
                        0.00439513799837312,
                        0.004403384315983588,
                        0.004408330082697108
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0498": {
            "setup": {
                "experiment_id": "0498",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 12:23:02 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_topp_0.7_temp_1.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_p",
                    "decoder_temperature": 1.4,
                    "decoder_top_k": null,
                    "decoder_top_p": 0.7
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.8393861069344,
                        "average_latency_ms_per_batch": 4854.9232633668,
                        "throughput_queries_per_sec": 3.2956236653067705,
                        "throughput_tokens_per_sec": 421.8398291592666
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38403047424,
                        "gpu_max_memory_reserved_bytes": 38403047424
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            18.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.0,
                        "cpu_memory_usage_bytes": 1979297792
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0498",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_3": 459.78360892810787,
                            "process_0": 963.9614585945693,
                            "process_1": 964.882412251197,
                            "process_2": 1339.261908328238
                        },
                        "ram_power": {
                            "process_3": 0.7342815399169922,
                            "process_0": 0.6889500617980957,
                            "process_1": 0.7334403991699219,
                            "process_2": 0.7328009605407715
                        },
                        "cpu_energy": {
                            "process_3": 0.0012291597428047683,
                            "process_0": 0.0011598204269757841,
                            "process_1": 0.0011694721927433423,
                            "process_2": 0.001217169557436137
                        },
                        "gpu_energy": {
                            "process_3": 0.01041155499590296,
                            "process_0": 0.010370588852021356,
                            "process_1": 0.01038208441677213,
                            "process_2": 0.010347467722411352
                        },
                        "ram_energy": {
                            "process_3": 7.353130681313656e-06,
                            "process_0": 6.900074915760785e-06,
                            "process_1": 7.399463416904615e-06,
                            "process_2": 7.301511849870363e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.01164806786938904,
                            "process_0": 0.011537309353912902,
                            "process_1": 0.011558956072932374,
                            "process_2": 0.011571938791697357
                        },
                        "total_energy_joules": {
                            "process_3": 41933.04432980054,
                            "process_0": 41534.31367408644,
                            "process_1": 41612.24186255655,
                            "process_2": 41658.97965011049
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 931.972347025528,
                        "ram_power_avg": 0.7223682403564453,
                        "cpu_energy_total": 0.004775621919960032,
                        "gpu_energy_total": 0.0415116959871078,
                        "ram_energy_total": 2.895418086384942e-05,
                        "total_energy_kwh": 0.046316272087931674,
                        "total_energy_joules": 166738.57951655402
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09826160236883495,
                        "joules_per_token": 10.176915253695924,
                        "flops_per_joule": 315695278.5701162,
                        "joules_per_flop": 3.1676115161725453e-09
                    },
                    "per-process_emissions": [
                        0.004437331454843755,
                        0.00439513799837312,
                        0.004403384315983588,
                        0.004408330082697108
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0499": {
            "setup": {
                "experiment_id": "0499",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 12:24:33 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_16",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.24086308199912,
                        "average_latency_ms_per_batch": 4780.10788524989,
                        "throughput_queries_per_sec": 3.347204787860884,
                        "throughput_tokens_per_sec": 428.4422128461932
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38352715776,
                        "gpu_max_memory_reserved_bytes": 38352715776
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.6,
                        "cpu_memory_usage_bytes": 1971474432
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0499",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_3": 114.56044738994272,
                            "process_0": 132.66131929699017,
                            "process_1": 115.6349070160307,
                            "process_2": 73.3234401130195
                        },
                        "ram_power": {
                            "process_3": 0.7231593132019043,
                            "process_0": 0.6873350143432618,
                            "process_1": 0.7230620384216309,
                            "process_2": 0.7309541702270508
                        },
                        "cpu_energy": {
                            "process_3": 0.0011806142771947634,
                            "process_0": 0.0010818722387557502,
                            "process_1": 0.0010747811845922112,
                            "process_2": 0.0010923799178162881
                        },
                        "gpu_energy": {
                            "process_3": 0.010223106234032997,
                            "process_0": 0.01022178039964139,
                            "process_1": 0.0102143401159136,
                            "process_2": 0.010212850670276552
                        },
                        "ram_energy": {
                            "process_3": 7.747213241095924e-06,
                            "process_0": 7.082702165817718e-06,
                            "process_1": 7.439189245866312e-06,
                            "process_2": 6.840188734109176e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.011411467724468856,
                            "process_0": 0.011310735340562959,
                            "process_1": 0.011296560489751677,
                            "process_2": 0.011312070776826949
                        },
                        "total_energy_joules": {
                            "process_3": 41081.28380808788,
                            "process_0": 40718.64722602665,
                            "process_1": 40667.61776310604,
                            "process_2": 40723.45479657702
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 109.04502845399577,
                        "ram_power_avg": 0.7161276340484619,
                        "cpu_energy_total": 0.0044296476183590125,
                        "gpu_energy_total": 0.04087207741986454,
                        "ram_energy_total": 2.9109293386889126e-05,
                        "total_energy_kwh": 0.04533083433161044,
                        "total_energy_joules": 163191.0035937976
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.10039769128929303,
                        "joules_per_token": 9.960388402941748,
                        "flops_per_joule": 322558113.80318415,
                        "joules_per_flop": 3.100216541476218e-09
                    },
                    "per-process_emissions": [
                        0.004347198629636411,
                        0.004308824627987459,
                        0.004303424718570902,
                        0.004309333362432226
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0499": {
            "setup": {
                "experiment_id": "0499",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 12:24:33 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_16",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.24086308199912,
                        "average_latency_ms_per_batch": 4780.10788524989,
                        "throughput_queries_per_sec": 3.347204787860884,
                        "throughput_tokens_per_sec": 428.4422128461932
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38352715776,
                        "gpu_max_memory_reserved_bytes": 38352715776
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.6,
                        "cpu_memory_usage_bytes": 1971474432
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0499",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_3": 114.56044738994272,
                            "process_0": 132.66131929699017,
                            "process_1": 115.6349070160307,
                            "process_2": 73.3234401130195
                        },
                        "ram_power": {
                            "process_3": 0.7231593132019043,
                            "process_0": 0.6873350143432618,
                            "process_1": 0.7230620384216309,
                            "process_2": 0.7309541702270508
                        },
                        "cpu_energy": {
                            "process_3": 0.0011806142771947634,
                            "process_0": 0.0010818722387557502,
                            "process_1": 0.0010747811845922112,
                            "process_2": 0.0010923799178162881
                        },
                        "gpu_energy": {
                            "process_3": 0.010223106234032997,
                            "process_0": 0.01022178039964139,
                            "process_1": 0.0102143401159136,
                            "process_2": 0.010212850670276552
                        },
                        "ram_energy": {
                            "process_3": 7.747213241095924e-06,
                            "process_0": 7.082702165817718e-06,
                            "process_1": 7.439189245866312e-06,
                            "process_2": 6.840188734109176e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.011411467724468856,
                            "process_0": 0.011310735340562959,
                            "process_1": 0.011296560489751677,
                            "process_2": 0.011312070776826949
                        },
                        "total_energy_joules": {
                            "process_3": 41081.28380808788,
                            "process_0": 40718.64722602665,
                            "process_1": 40667.61776310604,
                            "process_2": 40723.45479657702
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 109.04502845399577,
                        "ram_power_avg": 0.7161276340484619,
                        "cpu_energy_total": 0.0044296476183590125,
                        "gpu_energy_total": 0.04087207741986454,
                        "ram_energy_total": 2.9109293386889126e-05,
                        "total_energy_kwh": 0.04533083433161044,
                        "total_energy_joules": 163191.0035937976
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.10039769128929303,
                        "joules_per_token": 9.960388402941748,
                        "flops_per_joule": 322558113.80318415,
                        "joules_per_flop": 3.100216541476218e-09
                    },
                    "per-process_emissions": [
                        0.004347198629636411,
                        0.004308824627987459,
                        0.004303424718570902,
                        0.004309333362432226
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0500": {
            "setup": {
                "experiment_id": "0500",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 12:26:10 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_burst_0.05_0.1_6.0_8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.05,
                    "delay_max": 0.1,
                    "simulate_burst": true,
                    "burst_interval": 6.0,
                    "burst_size": 8
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 44.53363673604326,
                        "average_latency_ms_per_batch": 5566.704592005408,
                        "throughput_queries_per_sec": 2.8742319150504794,
                        "throughput_tokens_per_sec": 367.90168512646136
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38352715776,
                        "gpu_max_memory_reserved_bytes": 38352715776
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            28.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.7,
                        "cpu_memory_usage_bytes": 1971867648
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0500",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_3": 874.194677203442,
                            "process_0": 1534.4864999744043,
                            "process_2": 1357.4466818372541,
                            "process_1": 1129.088366331359
                        },
                        "ram_power": {
                            "process_3": 0.7229018211364746,
                            "process_0": 0.6862950325012207,
                            "process_2": 0.7300701141357422,
                            "process_1": 0.7232880592346191
                        },
                        "cpu_energy": {
                            "process_3": 0.0013642156383448314,
                            "process_0": 0.0013352318490087779,
                            "process_2": 0.0013522901206160895,
                            "process_1": 0.0013318943683134422
                        },
                        "gpu_energy": {
                            "process_3": 0.01059449208669605,
                            "process_0": 0.010604090983264314,
                            "process_2": 0.010604090983264314,
                            "process_1": 0.010603393204929645
                        },
                        "ram_energy": {
                            "process_3": 8.064114959506525e-06,
                            "process_0": 7.471281218972714e-06,
                            "process_2": 8.059633591346972e-06,
                            "process_1": 7.862778621580551e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.011966771840000391,
                            "process_0": 0.011946794113492068,
                            "process_2": 0.011964440737471754,
                            "process_1": 0.011943150351864674
                        },
                        "total_energy_joules": {
                            "process_3": 43080.37862400141,
                            "process_0": 43008.45880857145,
                            "process_2": 43071.98665489831,
                            "process_1": 42995.34126671283
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1223.8040563366148,
                        "ram_power_avg": 0.7156387567520142,
                        "cpu_energy_total": 0.005383631976283141,
                        "gpu_energy_total": 0.04240606725815432,
                        "ram_energy_total": 3.145780839140676e-05,
                        "total_energy_kwh": 0.04782115704282889,
                        "total_energy_joules": 172156.165354184
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09516940602326103,
                        "joules_per_token": 10.507578451793457,
                        "flops_per_joule": 305760657.4854201,
                        "joules_per_flop": 3.2705319520961717e-09
                    },
                    "per-process_emissions": [
                        0.004558741732448149,
                        0.0045511312175348035,
                        0.004557853698939865,
                        0.004549743126542848
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0500": {
            "setup": {
                "experiment_id": "0500",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 12:26:10 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_burst_0.05_0.1_6.0_8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.05,
                    "delay_max": 0.1,
                    "simulate_burst": true,
                    "burst_interval": 6.0,
                    "burst_size": 8
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 44.53363673604326,
                        "average_latency_ms_per_batch": 5566.704592005408,
                        "throughput_queries_per_sec": 2.8742319150504794,
                        "throughput_tokens_per_sec": 367.90168512646136
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38352715776,
                        "gpu_max_memory_reserved_bytes": 38352715776
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            28.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.7,
                        "cpu_memory_usage_bytes": 1971867648
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0500",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_3": 874.194677203442,
                            "process_0": 1534.4864999744043,
                            "process_2": 1357.4466818372541,
                            "process_1": 1129.088366331359
                        },
                        "ram_power": {
                            "process_3": 0.7229018211364746,
                            "process_0": 0.6862950325012207,
                            "process_2": 0.7300701141357422,
                            "process_1": 0.7232880592346191
                        },
                        "cpu_energy": {
                            "process_3": 0.0013642156383448314,
                            "process_0": 0.0013352318490087779,
                            "process_2": 0.0013522901206160895,
                            "process_1": 0.0013318943683134422
                        },
                        "gpu_energy": {
                            "process_3": 0.01059449208669605,
                            "process_0": 0.010604090983264314,
                            "process_2": 0.010604090983264314,
                            "process_1": 0.010603393204929645
                        },
                        "ram_energy": {
                            "process_3": 8.064114959506525e-06,
                            "process_0": 7.471281218972714e-06,
                            "process_2": 8.059633591346972e-06,
                            "process_1": 7.862778621580551e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.011966771840000391,
                            "process_0": 0.011946794113492068,
                            "process_2": 0.011964440737471754,
                            "process_1": 0.011943150351864674
                        },
                        "total_energy_joules": {
                            "process_3": 43080.37862400141,
                            "process_0": 43008.45880857145,
                            "process_2": 43071.98665489831,
                            "process_1": 42995.34126671283
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1223.8040563366148,
                        "ram_power_avg": 0.7156387567520142,
                        "cpu_energy_total": 0.005383631976283141,
                        "gpu_energy_total": 0.04240606725815432,
                        "ram_energy_total": 3.145780839140676e-05,
                        "total_energy_kwh": 0.04782115704282889,
                        "total_energy_joules": 172156.165354184
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09516940602326103,
                        "joules_per_token": 10.507578451793457,
                        "flops_per_joule": 305760657.4854201,
                        "joules_per_flop": 3.2705319520961717e-09
                    },
                    "per-process_emissions": [
                        0.004558741732448149,
                        0.0045511312175348035,
                        0.004557853698939865,
                        0.004549743126542848
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0501": {
            "setup": {
                "experiment_id": "0501",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 12:27:46 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_burst_0.2_0.4_6.0_20",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.2,
                    "delay_max": 0.4,
                    "simulate_burst": true,
                    "burst_interval": 6.0,
                    "burst_size": 20
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 42.17764619208174,
                        "average_latency_ms_per_batch": 5272.205774010217,
                        "throughput_queries_per_sec": 3.0347829136095843,
                        "throughput_tokens_per_sec": 388.4522129420268
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38352715776,
                        "gpu_max_memory_reserved_bytes": 38352715776
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.9,
                        "cpu_memory_usage_bytes": 1950048256
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0501",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_3": 502.2006754301085,
                            "process_2": 998.9276513562182,
                            "process_1": 879.6327864222482,
                            "process_0": 1260.5401881503165
                        },
                        "ram_power": {
                            "process_3": 0.724116325378418,
                            "process_2": 0.7236199378967285,
                            "process_1": 0.7297854423522949,
                            "process_0": 0.6798834800720215
                        },
                        "cpu_energy": {
                            "process_3": 0.0011666711267298523,
                            "process_2": 0.0011657026304128521,
                            "process_1": 0.001150050193617062,
                            "process_0": 0.0012063862668765073
                        },
                        "gpu_energy": {
                            "process_3": 0.010477245604009866,
                            "process_2": 0.01009526307620412,
                            "process_1": 0.01017780175334515,
                            "process_0": 0.0104419172424155
                        },
                        "ram_energy": {
                            "process_3": 7.561998029866789e-06,
                            "process_2": 6.806354822641227e-06,
                            "process_1": 6.747704928034273e-06,
                            "process_0": 7.36692281361228e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.011651478728769586,
                            "process_2": 0.01126777206143962,
                            "process_1": 0.011334599651890247,
                            "process_0": 0.011655670432105615
                        },
                        "total_energy_joules": {
                            "process_3": 41945.32342357051,
                            "process_2": 40563.97942118263,
                            "process_1": 40804.55874680489,
                            "process_0": 41960.41355558021
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 910.3253253397229,
                        "ram_power_avg": 0.7143512964248657,
                        "cpu_energy_total": 0.0046888102176362734,
                        "gpu_energy_total": 0.041192227675974635,
                        "ram_energy_total": 2.848298059415457e-05,
                        "total_energy_kwh": 0.04590952087420507,
                        "total_energy_joules": 165274.27514713825
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09913218488124582,
                        "joules_per_token": 10.087541207711075,
                        "flops_per_joule": 318492289.63189584,
                        "joules_per_flop": 3.1397934347351736e-09
                    },
                    "per-process_emissions": [
                        0.004438630821724774,
                        0.004292457766805423,
                        0.00431791573738759,
                        0.004440227651110634
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0501": {
            "setup": {
                "experiment_id": "0501",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 12:27:46 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_burst_0.2_0.4_6.0_20",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.2,
                    "delay_max": 0.4,
                    "simulate_burst": true,
                    "burst_interval": 6.0,
                    "burst_size": 20
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 42.17764619208174,
                        "average_latency_ms_per_batch": 5272.205774010217,
                        "throughput_queries_per_sec": 3.0347829136095843,
                        "throughput_tokens_per_sec": 388.4522129420268
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38352715776,
                        "gpu_max_memory_reserved_bytes": 38352715776
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.9,
                        "cpu_memory_usage_bytes": 1950048256
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0501",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_3": 502.2006754301085,
                            "process_2": 998.9276513562182,
                            "process_1": 879.6327864222482,
                            "process_0": 1260.5401881503165
                        },
                        "ram_power": {
                            "process_3": 0.724116325378418,
                            "process_2": 0.7236199378967285,
                            "process_1": 0.7297854423522949,
                            "process_0": 0.6798834800720215
                        },
                        "cpu_energy": {
                            "process_3": 0.0011666711267298523,
                            "process_2": 0.0011657026304128521,
                            "process_1": 0.001150050193617062,
                            "process_0": 0.0012063862668765073
                        },
                        "gpu_energy": {
                            "process_3": 0.010477245604009866,
                            "process_2": 0.01009526307620412,
                            "process_1": 0.01017780175334515,
                            "process_0": 0.0104419172424155
                        },
                        "ram_energy": {
                            "process_3": 7.561998029866789e-06,
                            "process_2": 6.806354822641227e-06,
                            "process_1": 6.747704928034273e-06,
                            "process_0": 7.36692281361228e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.011651478728769586,
                            "process_2": 0.01126777206143962,
                            "process_1": 0.011334599651890247,
                            "process_0": 0.011655670432105615
                        },
                        "total_energy_joules": {
                            "process_3": 41945.32342357051,
                            "process_2": 40563.97942118263,
                            "process_1": 40804.55874680489,
                            "process_0": 41960.41355558021
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 910.3253253397229,
                        "ram_power_avg": 0.7143512964248657,
                        "cpu_energy_total": 0.0046888102176362734,
                        "gpu_energy_total": 0.041192227675974635,
                        "ram_energy_total": 2.848298059415457e-05,
                        "total_energy_kwh": 0.04590952087420507,
                        "total_energy_joules": 165274.27514713825
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09913218488124582,
                        "joules_per_token": 10.087541207711075,
                        "flops_per_joule": 318492289.63189584,
                        "joules_per_flop": 3.1397934347351736e-09
                    },
                    "per-process_emissions": [
                        0.004438630821724774,
                        0.004292457766805423,
                        0.00431791573738759,
                        0.004440227651110634
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0502": {
            "setup": {
                "experiment_id": "0502",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 12:29:18 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_topp_0.98_temp_0.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_p",
                    "decoder_temperature": 0.4,
                    "decoder_top_k": null,
                    "decoder_top_p": 0.98
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.92781834997004,
                        "average_latency_ms_per_batch": 4865.977293746255,
                        "throughput_queries_per_sec": 3.288137003960781,
                        "throughput_tokens_per_sec": 420.88153650697996
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38403047424,
                        "gpu_max_memory_reserved_bytes": 38403047424
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.2,
                        "cpu_memory_usage_bytes": 1981165568
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0502",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_1": 0.0,
                            "process_3": 384.79759119087146,
                            "process_0": 1180.77687317031,
                            "process_2": 1383.8364397814996
                        },
                        "ram_power": {
                            "process_1": 0.7325363159179688,
                            "process_3": 0.7335648536682129,
                            "process_0": 0.6907482147216797,
                            "process_2": 0.7263779640197754
                        },
                        "cpu_energy": {
                            "process_1": 0.001277514914063431,
                            "process_3": 0.0012885274884138203,
                            "process_0": 0.0012679951259360676,
                            "process_2": 0.0012805960256264367
                        },
                        "gpu_energy": {
                            "process_1": 0.01038928636697456,
                            "process_3": 0.010403539433934128,
                            "process_0": 0.010374988299982846,
                            "process_2": 0.01034084521711165
                        },
                        "ram_energy": {
                            "process_1": 7.348357421841522e-06,
                            "process_3": 7.332732514389268e-06,
                            "process_0": 6.834695971728271e-06,
                            "process_2": 7.224456054718927e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.011674149638459834,
                            "process_3": 0.011699399654862335,
                            "process_0": 0.011649818121890643,
                            "process_2": 0.011628665698792801
                        },
                        "total_energy_joules": {
                            "process_1": 42026.9386984554,
                            "process_3": 42117.83875750441,
                            "process_0": 41939.345238806316,
                            "process_2": 41863.19651565408
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 737.3527260356702,
                        "ram_power_avg": 0.7208068370819092,
                        "cpu_energy_total": 0.005114633554039755,
                        "gpu_energy_total": 0.04150865931800318,
                        "ram_energy_total": 2.8740241962677985e-05,
                        "total_energy_kwh": 0.046652033114005606,
                        "total_energy_joules": 167947.31921042022
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09755440025495483,
                        "joules_per_token": 10.250690869776625,
                        "flops_per_joule": 313423176.72194237,
                        "joules_per_flop": 3.190574514810574e-09
                    },
                    "per-process_emissions": [
                        0.004447267304771274,
                        0.004456886298519807,
                        0.00443799821353424,
                        0.004429940197955118
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0502": {
            "setup": {
                "experiment_id": "0502",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 12:29:18 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_topp_0.98_temp_0.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_p",
                    "decoder_temperature": 0.4,
                    "decoder_top_k": null,
                    "decoder_top_p": 0.98
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.92781834997004,
                        "average_latency_ms_per_batch": 4865.977293746255,
                        "throughput_queries_per_sec": 3.288137003960781,
                        "throughput_tokens_per_sec": 420.88153650697996
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38403047424,
                        "gpu_max_memory_reserved_bytes": 38403047424
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.2,
                        "cpu_memory_usage_bytes": 1981165568
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0502",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_1": 0.0,
                            "process_3": 384.79759119087146,
                            "process_0": 1180.77687317031,
                            "process_2": 1383.8364397814996
                        },
                        "ram_power": {
                            "process_1": 0.7325363159179688,
                            "process_3": 0.7335648536682129,
                            "process_0": 0.6907482147216797,
                            "process_2": 0.7263779640197754
                        },
                        "cpu_energy": {
                            "process_1": 0.001277514914063431,
                            "process_3": 0.0012885274884138203,
                            "process_0": 0.0012679951259360676,
                            "process_2": 0.0012805960256264367
                        },
                        "gpu_energy": {
                            "process_1": 0.01038928636697456,
                            "process_3": 0.010403539433934128,
                            "process_0": 0.010374988299982846,
                            "process_2": 0.01034084521711165
                        },
                        "ram_energy": {
                            "process_1": 7.348357421841522e-06,
                            "process_3": 7.332732514389268e-06,
                            "process_0": 6.834695971728271e-06,
                            "process_2": 7.224456054718927e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.011674149638459834,
                            "process_3": 0.011699399654862335,
                            "process_0": 0.011649818121890643,
                            "process_2": 0.011628665698792801
                        },
                        "total_energy_joules": {
                            "process_1": 42026.9386984554,
                            "process_3": 42117.83875750441,
                            "process_0": 41939.345238806316,
                            "process_2": 41863.19651565408
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 737.3527260356702,
                        "ram_power_avg": 0.7208068370819092,
                        "cpu_energy_total": 0.005114633554039755,
                        "gpu_energy_total": 0.04150865931800318,
                        "ram_energy_total": 2.8740241962677985e-05,
                        "total_energy_kwh": 0.046652033114005606,
                        "total_energy_joules": 167947.31921042022
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09755440025495483,
                        "joules_per_token": 10.250690869776625,
                        "flops_per_joule": 313423176.72194237,
                        "joules_per_flop": 3.190574514810574e-09
                    },
                    "per-process_emissions": [
                        0.004447267304771274,
                        0.004456886298519807,
                        0.00443799821353424,
                        0.004429940197955118
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0503": {
            "setup": {
                "experiment_id": "0503",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 12:31:01 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_burst_0.2_0.4_6.0_5",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.2,
                    "delay_max": 0.4,
                    "simulate_burst": true,
                    "burst_interval": 6.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 46.338852687040344,
                        "average_latency_ms_per_batch": 5792.356585880043,
                        "throughput_queries_per_sec": 2.76226087996775,
                        "throughput_tokens_per_sec": 353.569392635872
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38403047424,
                        "gpu_max_memory_reserved_bytes": 38403047424
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.3,
                        "cpu_memory_usage_bytes": 1970925568
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0503",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_1": 1125.8262126103634,
                            "process_3": 531.3113124124891,
                            "process_0": 2741.581391700172,
                            "process_2": 1679.3891670361827
                        },
                        "ram_power": {
                            "process_1": 0.7302746772766113,
                            "process_3": 0.7298655509948732,
                            "process_0": 0.6858229637145996,
                            "process_2": 0.7313904762268066
                        },
                        "cpu_energy": {
                            "process_1": 0.0014345914634413927,
                            "process_3": 0.001380905218926273,
                            "process_0": 0.0014723974312819338,
                            "process_2": 0.0015295458148484613
                        },
                        "gpu_energy": {
                            "process_1": 0.010622660720340349,
                            "process_3": 0.010840462005694107,
                            "process_0": 0.010628300169301497,
                            "process_2": 0.010624028499217175
                        },
                        "ram_energy": {
                            "process_1": 8.94753548076939e-06,
                            "process_3": 8.982935946853983e-06,
                            "process_0": 7.883164683917984e-06,
                            "process_2": 8.764041645836408e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.012066199719262509,
                            "process_3": 0.012230350160567236,
                            "process_0": 0.01210858076526735,
                            "process_2": 0.012162338355711469
                        },
                        "total_energy_joules": {
                            "process_1": 43438.31898934503,
                            "process_3": 44029.26057804205,
                            "process_0": 43590.890754962464,
                            "process_2": 43784.418080561285
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1519.5270209398018,
                        "ram_power_avg": 0.7193384170532227,
                        "cpu_energy_total": 0.005817439928498061,
                        "gpu_energy_total": 0.04271545139455313,
                        "ram_energy_total": 3.4577677757377766e-05,
                        "total_energy_kwh": 0.048567469000808564,
                        "total_energy_joules": 174842.88840291084
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09370698545224464,
                        "joules_per_token": 10.671563012872976,
                        "flops_per_joule": 301062186.684784,
                        "joules_per_flop": 3.3215728983162303e-09
                    },
                    "per-process_emissions": [
                        0.004596618783053053,
                        0.004659151893668089,
                        0.004612763842528597,
                        0.004633242796608285
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0503": {
            "setup": {
                "experiment_id": "0503",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 12:31:01 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_burst_0.2_0.4_6.0_5",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.2,
                    "delay_max": 0.4,
                    "simulate_burst": true,
                    "burst_interval": 6.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 46.338852687040344,
                        "average_latency_ms_per_batch": 5792.356585880043,
                        "throughput_queries_per_sec": 2.76226087996775,
                        "throughput_tokens_per_sec": 353.569392635872
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38403047424,
                        "gpu_max_memory_reserved_bytes": 38403047424
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.3,
                        "cpu_memory_usage_bytes": 1970925568
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0503",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_1": 1125.8262126103634,
                            "process_3": 531.3113124124891,
                            "process_0": 2741.581391700172,
                            "process_2": 1679.3891670361827
                        },
                        "ram_power": {
                            "process_1": 0.7302746772766113,
                            "process_3": 0.7298655509948732,
                            "process_0": 0.6858229637145996,
                            "process_2": 0.7313904762268066
                        },
                        "cpu_energy": {
                            "process_1": 0.0014345914634413927,
                            "process_3": 0.001380905218926273,
                            "process_0": 0.0014723974312819338,
                            "process_2": 0.0015295458148484613
                        },
                        "gpu_energy": {
                            "process_1": 0.010622660720340349,
                            "process_3": 0.010840462005694107,
                            "process_0": 0.010628300169301497,
                            "process_2": 0.010624028499217175
                        },
                        "ram_energy": {
                            "process_1": 8.94753548076939e-06,
                            "process_3": 8.982935946853983e-06,
                            "process_0": 7.883164683917984e-06,
                            "process_2": 8.764041645836408e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.012066199719262509,
                            "process_3": 0.012230350160567236,
                            "process_0": 0.01210858076526735,
                            "process_2": 0.012162338355711469
                        },
                        "total_energy_joules": {
                            "process_1": 43438.31898934503,
                            "process_3": 44029.26057804205,
                            "process_0": 43590.890754962464,
                            "process_2": 43784.418080561285
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1519.5270209398018,
                        "ram_power_avg": 0.7193384170532227,
                        "cpu_energy_total": 0.005817439928498061,
                        "gpu_energy_total": 0.04271545139455313,
                        "ram_energy_total": 3.4577677757377766e-05,
                        "total_energy_kwh": 0.048567469000808564,
                        "total_energy_joules": 174842.88840291084
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09370698545224464,
                        "joules_per_token": 10.671563012872976,
                        "flops_per_joule": 301062186.684784,
                        "joules_per_flop": 3.3215728983162303e-09
                    },
                    "per-process_emissions": [
                        0.004596618783053053,
                        0.004659151893668089,
                        0.004612763842528597,
                        0.004633242796608285
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0504": {
            "setup": {
                "experiment_id": "0504",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 12:32:30 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_topp_0.1_temp_0.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_p",
                    "decoder_temperature": 0.0,
                    "decoder_top_k": null,
                    "decoder_top_p": 0.1
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.1636168800178,
                        "average_latency_ms_per_batch": 4770.452110002225,
                        "throughput_queries_per_sec": 3.3539797971040817,
                        "throughput_tokens_per_sec": 429.30941402932245
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38325452800,
                        "gpu_max_memory_reserved_bytes": 38325452800
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.6,
                        "cpu_memory_usage_bytes": 1947062272
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0504",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_1": 101.37848286690392,
                            "process_0": 133.4804127443563,
                            "process_2": 98.56295522022693,
                            "process_3": 103.02025679043608
                        },
                        "ram_power": {
                            "process_1": 0.7041835784912109,
                            "process_0": 0.6788277626037598,
                            "process_2": 0.7131242752075195,
                            "process_3": 0.7198905944824219
                        },
                        "cpu_energy": {
                            "process_1": 0.0011429372347738535,
                            "process_0": 0.0010878451836251772,
                            "process_2": 0.0011546314835741216,
                            "process_3": 0.0010996532724057036
                        },
                        "gpu_energy": {
                            "process_1": 0.010201452327825322,
                            "process_0": 0.010201068994184936,
                            "process_2": 0.010196646768426021,
                            "process_3": 0.010205874553584238
                        },
                        "ram_energy": {
                            "process_1": 6.096242157899933e-06,
                            "process_0": 6.947280521220305e-06,
                            "process_2": 6.26438365827966e-06,
                            "process_3": 7.066401440957856e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.011350485804757072,
                            "process_0": 0.011295861458331334,
                            "process_2": 0.011357542635658423,
                            "process_3": 0.0113125942274309
                        },
                        "total_energy_joules": {
                            "process_1": 40861.74889712546,
                            "process_0": 40665.101249992804,
                            "process_2": 40887.15348837033,
                            "process_3": 40725.339218751236
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 109.1105269054808,
                        "ram_power_avg": 0.704006552696228,
                        "cpu_energy_total": 0.004485067174378856,
                        "gpu_energy_total": 0.04080504264402052,
                        "ram_energy_total": 2.6374307778357754e-05,
                        "total_energy_kwh": 0.045316484126177725,
                        "total_energy_joules": 163139.34285423983
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.1004294838593203,
                        "joules_per_token": 9.95723528163085,
                        "flops_per_joule": 322660257.10239017,
                        "joules_per_flop": 3.0992351180166226e-09
                    },
                    "per-process_emissions": [
                        0.004323967567322207,
                        0.004303158422551322,
                        0.0043266558670540765,
                        0.004309532770939801
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0504": {
            "setup": {
                "experiment_id": "0504",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 12:32:30 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_topp_0.1_temp_0.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_p",
                    "decoder_temperature": 0.0,
                    "decoder_top_k": null,
                    "decoder_top_p": 0.1
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.1636168800178,
                        "average_latency_ms_per_batch": 4770.452110002225,
                        "throughput_queries_per_sec": 3.3539797971040817,
                        "throughput_tokens_per_sec": 429.30941402932245
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38325452800,
                        "gpu_max_memory_reserved_bytes": 38325452800
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.6,
                        "cpu_memory_usage_bytes": 1947062272
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0504",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_1": 101.37848286690392,
                            "process_0": 133.4804127443563,
                            "process_2": 98.56295522022693,
                            "process_3": 103.02025679043608
                        },
                        "ram_power": {
                            "process_1": 0.7041835784912109,
                            "process_0": 0.6788277626037598,
                            "process_2": 0.7131242752075195,
                            "process_3": 0.7198905944824219
                        },
                        "cpu_energy": {
                            "process_1": 0.0011429372347738535,
                            "process_0": 0.0010878451836251772,
                            "process_2": 0.0011546314835741216,
                            "process_3": 0.0010996532724057036
                        },
                        "gpu_energy": {
                            "process_1": 0.010201452327825322,
                            "process_0": 0.010201068994184936,
                            "process_2": 0.010196646768426021,
                            "process_3": 0.010205874553584238
                        },
                        "ram_energy": {
                            "process_1": 6.096242157899933e-06,
                            "process_0": 6.947280521220305e-06,
                            "process_2": 6.26438365827966e-06,
                            "process_3": 7.066401440957856e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.011350485804757072,
                            "process_0": 0.011295861458331334,
                            "process_2": 0.011357542635658423,
                            "process_3": 0.0113125942274309
                        },
                        "total_energy_joules": {
                            "process_1": 40861.74889712546,
                            "process_0": 40665.101249992804,
                            "process_2": 40887.15348837033,
                            "process_3": 40725.339218751236
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 109.1105269054808,
                        "ram_power_avg": 0.704006552696228,
                        "cpu_energy_total": 0.004485067174378856,
                        "gpu_energy_total": 0.04080504264402052,
                        "ram_energy_total": 2.6374307778357754e-05,
                        "total_energy_kwh": 0.045316484126177725,
                        "total_energy_joules": 163139.34285423983
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.1004294838593203,
                        "joules_per_token": 9.95723528163085,
                        "flops_per_joule": 322660257.10239017,
                        "joules_per_flop": 3.0992351180166226e-09
                    },
                    "per-process_emissions": [
                        0.004323967567322207,
                        0.004303158422551322,
                        0.0043266558670540765,
                        0.004309532770939801
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0505": {
            "setup": {
                "experiment_id": "0505",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 12:34:01 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_100_temp_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 0.8,
                    "decoder_top_k": 100,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.412122624053154,
                        "average_latency_ms_per_batch": 4801.515328006644,
                        "throughput_queries_per_sec": 3.332281354320371,
                        "throughput_tokens_per_sec": 426.53201335300747
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38449184768,
                        "gpu_max_memory_reserved_bytes": 38449184768
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            28.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.4,
                        "cpu_memory_usage_bytes": 1957208064
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0505",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 189.7506855740273,
                            "process_1": 1382.6215933990993,
                            "process_2": 196.72541707725892,
                            "process_3": 167.25426582506202
                        },
                        "ram_power": {
                            "process_0": 0.6814126968383789,
                            "process_1": 0.724907398223877,
                            "process_2": 0.7324604988098146,
                            "process_3": 0.7323360443115234
                        },
                        "cpu_energy": {
                            "process_0": 0.0010859952632872592,
                            "process_1": 0.001093578200250704,
                            "process_2": 0.0010779370783729973,
                            "process_3": 0.0010979878500584164
                        },
                        "gpu_energy": {
                            "process_0": 0.010251005423018711,
                            "process_1": 0.01024484041808904,
                            "process_2": 0.010242026804723636,
                            "process_3": 0.010251657090206479
                        },
                        "ram_energy": {
                            "process_0": 7.376731208929375e-06,
                            "process_1": 6.762595276375794e-06,
                            "process_2": 7.909593077675798e-06,
                            "process_3": 7.241827547287961e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.011344377417514902,
                            "process_1": 0.011345181213616119,
                            "process_2": 0.011327873476174312,
                            "process_3": 0.011356886767812182
                        },
                        "total_energy_joules": {
                            "process_0": 40839.758703053645,
                            "process_1": 40842.652369018026,
                            "process_2": 40780.34451422752,
                            "process_3": 40884.79236412385
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 484.0879904688619,
                        "ram_power_avg": 0.7177791595458984,
                        "cpu_energy_total": 0.004355498391969376,
                        "gpu_energy_total": 0.040989529736037866,
                        "ram_energy_total": 2.9290747110268927e-05,
                        "total_energy_kwh": 0.04537431887511752,
                        "total_energy_joules": 163347.54795042306
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.1003014750179944,
                        "joules_per_token": 9.969943112208439,
                        "flops_per_joule": 322248989.77265406,
                        "joules_per_flop": 3.103190488527203e-09
                    },
                    "per-process_emissions": [
                        0.004321640577202302,
                        0.0043219467833270605,
                        0.0043153534007486045,
                        0.004326406014198051
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0505": {
            "setup": {
                "experiment_id": "0505",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 12:34:01 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_100_temp_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 0.8,
                    "decoder_top_k": 100,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.412122624053154,
                        "average_latency_ms_per_batch": 4801.515328006644,
                        "throughput_queries_per_sec": 3.332281354320371,
                        "throughput_tokens_per_sec": 426.53201335300747
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38449184768,
                        "gpu_max_memory_reserved_bytes": 38449184768
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            28.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.4,
                        "cpu_memory_usage_bytes": 1957208064
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0505",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 189.7506855740273,
                            "process_1": 1382.6215933990993,
                            "process_2": 196.72541707725892,
                            "process_3": 167.25426582506202
                        },
                        "ram_power": {
                            "process_0": 0.6814126968383789,
                            "process_1": 0.724907398223877,
                            "process_2": 0.7324604988098146,
                            "process_3": 0.7323360443115234
                        },
                        "cpu_energy": {
                            "process_0": 0.0010859952632872592,
                            "process_1": 0.001093578200250704,
                            "process_2": 0.0010779370783729973,
                            "process_3": 0.0010979878500584164
                        },
                        "gpu_energy": {
                            "process_0": 0.010251005423018711,
                            "process_1": 0.01024484041808904,
                            "process_2": 0.010242026804723636,
                            "process_3": 0.010251657090206479
                        },
                        "ram_energy": {
                            "process_0": 7.376731208929375e-06,
                            "process_1": 6.762595276375794e-06,
                            "process_2": 7.909593077675798e-06,
                            "process_3": 7.241827547287961e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.011344377417514902,
                            "process_1": 0.011345181213616119,
                            "process_2": 0.011327873476174312,
                            "process_3": 0.011356886767812182
                        },
                        "total_energy_joules": {
                            "process_0": 40839.758703053645,
                            "process_1": 40842.652369018026,
                            "process_2": 40780.34451422752,
                            "process_3": 40884.79236412385
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 484.0879904688619,
                        "ram_power_avg": 0.7177791595458984,
                        "cpu_energy_total": 0.004355498391969376,
                        "gpu_energy_total": 0.040989529736037866,
                        "ram_energy_total": 2.9290747110268927e-05,
                        "total_energy_kwh": 0.04537431887511752,
                        "total_energy_joules": 163347.54795042306
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.1003014750179944,
                        "joules_per_token": 9.969943112208439,
                        "flops_per_joule": 322248989.77265406,
                        "joules_per_flop": 3.103190488527203e-09
                    },
                    "per-process_emissions": [
                        0.004321640577202302,
                        0.0043219467833270605,
                        0.0043153534007486045,
                        0.004326406014198051
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0506": {
            "setup": {
                "experiment_id": "0506",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 12:35:30 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_burst_0.05_0.1_2.0_10",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.05,
                    "delay_max": 0.1,
                    "simulate_burst": true,
                    "burst_interval": 2.0,
                    "burst_size": 10
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.83367781894049,
                        "average_latency_ms_per_batch": 4854.209727367561,
                        "throughput_queries_per_sec": 3.2961080996961383,
                        "throughput_tokens_per_sec": 421.9018367611057
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38352715776,
                        "gpu_max_memory_reserved_bytes": 38352715776
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.9,
                        "cpu_memory_usage_bytes": 1962987520
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0506",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_3": 13.656789838779137,
                            "process_2": 2653.4847070681026,
                            "process_0": 2667.396520250789,
                            "process_1": 3102.6475999261033
                        },
                        "ram_power": {
                            "process_3": 0.7304563522338867,
                            "process_2": 0.731433391571045,
                            "process_0": 0.684363842010498,
                            "process_1": 0.723332405090332
                        },
                        "cpu_energy": {
                            "process_3": 0.0011259440324665774,
                            "process_2": 0.0011031256512560501,
                            "process_0": 0.0011053873338987617,
                            "process_1": 0.0011362370435890625
                        },
                        "gpu_energy": {
                            "process_3": 0.010264890711908947,
                            "process_2": 0.010231994574478875,
                            "process_0": 0.010239118746845577,
                            "process_1": 0.010228903183115534
                        },
                        "ram_energy": {
                            "process_3": 6.994892533205972e-06,
                            "process_2": 7.2529258271054245e-06,
                            "process_0": 6.458841475365668e-06,
                            "process_1": 6.292944835854091e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.011397829636908729,
                            "process_2": 0.01134237315156203,
                            "process_0": 0.011350964922219705,
                            "process_1": 0.011371433171540454
                        },
                        "total_energy_joules": {
                            "process_3": 41032.18669287142,
                            "process_2": 40832.54334562331,
                            "process_0": 40863.473719990936,
                            "process_1": 40937.159417545634
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 2109.2964042709436,
                        "ram_power_avg": 0.7173964977264404,
                        "cpu_energy_total": 0.0044706940612104515,
                        "gpu_energy_total": 0.040964907216348934,
                        "ram_energy_total": 2.6999604671531155e-05,
                        "total_energy_kwh": 0.04546260088223092,
                        "total_energy_joules": 163665.3631760313
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.10010670359358863,
                        "joules_per_token": 9.989341014162067,
                        "flops_per_joule": 321623227.3425394,
                        "joules_per_flop": 3.109228174415919e-09
                    },
                    "per-process_emissions": [
                        0.004342003200180381,
                        0.0043208770520875555,
                        0.004324150087119597,
                        0.004331947466698336
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0506": {
            "setup": {
                "experiment_id": "0506",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 12:35:30 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_burst_0.05_0.1_2.0_10",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.05,
                    "delay_max": 0.1,
                    "simulate_burst": true,
                    "burst_interval": 2.0,
                    "burst_size": 10
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.83367781894049,
                        "average_latency_ms_per_batch": 4854.209727367561,
                        "throughput_queries_per_sec": 3.2961080996961383,
                        "throughput_tokens_per_sec": 421.9018367611057
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38352715776,
                        "gpu_max_memory_reserved_bytes": 38352715776
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.9,
                        "cpu_memory_usage_bytes": 1962987520
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0506",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_3": 13.656789838779137,
                            "process_2": 2653.4847070681026,
                            "process_0": 2667.396520250789,
                            "process_1": 3102.6475999261033
                        },
                        "ram_power": {
                            "process_3": 0.7304563522338867,
                            "process_2": 0.731433391571045,
                            "process_0": 0.684363842010498,
                            "process_1": 0.723332405090332
                        },
                        "cpu_energy": {
                            "process_3": 0.0011259440324665774,
                            "process_2": 0.0011031256512560501,
                            "process_0": 0.0011053873338987617,
                            "process_1": 0.0011362370435890625
                        },
                        "gpu_energy": {
                            "process_3": 0.010264890711908947,
                            "process_2": 0.010231994574478875,
                            "process_0": 0.010239118746845577,
                            "process_1": 0.010228903183115534
                        },
                        "ram_energy": {
                            "process_3": 6.994892533205972e-06,
                            "process_2": 7.2529258271054245e-06,
                            "process_0": 6.458841475365668e-06,
                            "process_1": 6.292944835854091e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.011397829636908729,
                            "process_2": 0.01134237315156203,
                            "process_0": 0.011350964922219705,
                            "process_1": 0.011371433171540454
                        },
                        "total_energy_joules": {
                            "process_3": 41032.18669287142,
                            "process_2": 40832.54334562331,
                            "process_0": 40863.473719990936,
                            "process_1": 40937.159417545634
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 2109.2964042709436,
                        "ram_power_avg": 0.7173964977264404,
                        "cpu_energy_total": 0.0044706940612104515,
                        "gpu_energy_total": 0.040964907216348934,
                        "ram_energy_total": 2.6999604671531155e-05,
                        "total_energy_kwh": 0.04546260088223092,
                        "total_energy_joules": 163665.3631760313
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.10010670359358863,
                        "joules_per_token": 9.989341014162067,
                        "flops_per_joule": 321623227.3425394,
                        "joules_per_flop": 3.109228174415919e-09
                    },
                    "per-process_emissions": [
                        0.004342003200180381,
                        0.0043208770520875555,
                        0.004324150087119597,
                        0.004331947466698336
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0507": {
            "setup": {
                "experiment_id": "0507",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 12:36:59 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_topp_0.8_temp_0.6",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_p",
                    "decoder_temperature": 0.6,
                    "decoder_top_k": null,
                    "decoder_top_p": 0.8
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 39.22138663002988,
                        "average_latency_ms_per_batch": 4902.673328753735,
                        "throughput_queries_per_sec": 3.2635256169652274,
                        "throughput_tokens_per_sec": 417.7312789715491
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38403047424,
                        "gpu_max_memory_reserved_bytes": 38403047424
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.9,
                        "cpu_memory_usage_bytes": 1980329984
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0507",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_2": 2197.1901200062994,
                            "process_0": 577.368003034387,
                            "process_1": 2198.8288619376135,
                            "process_3": 701.1511880311702
                        },
                        "ram_power": {
                            "process_2": 0.7345218658447266,
                            "process_0": 0.6904506683349609,
                            "process_1": 0.7342472076416016,
                            "process_3": 0.732330322265625
                        },
                        "cpu_energy": {
                            "process_2": 0.0012266475242176968,
                            "process_0": 0.0011956316939940737,
                            "process_1": 0.0011740353525565297,
                            "process_3": 0.0012395683006216137
                        },
                        "gpu_energy": {
                            "process_2": 0.010348251889706006,
                            "process_0": 0.010406767214295343,
                            "process_1": 0.010348251889706006,
                            "process_3": 0.0103986194299992
                        },
                        "ram_energy": {
                            "process_2": 6.943633034334175e-06,
                            "process_0": 6.7157669186562995e-06,
                            "process_1": 6.638968661489595e-06,
                            "process_3": 6.9637939233070116e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.011581843046958036,
                            "process_0": 0.011609114675208074,
                            "process_1": 0.011528926210924027,
                            "process_3": 0.011645151524544123
                        },
                        "total_energy_joules": {
                            "process_2": 41694.63496904893,
                            "process_0": 41792.81283074906,
                            "process_1": 41504.1343593265,
                            "process_3": 41922.54548835884
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1418.6345432523676,
                        "ram_power_avg": 0.7228875160217285,
                        "cpu_energy_total": 0.004835882871389914,
                        "gpu_energy_total": 0.041501890423706556,
                        "ram_energy_total": 2.726216253778708e-05,
                        "total_energy_kwh": 0.04636503545763426,
                        "total_energy_joules": 166914.1276474833
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09815825796724903,
                        "joules_per_token": 10.187629861296589,
                        "flops_per_joule": 315363253.2534023,
                        "joules_per_flop": 3.1709464868960965e-09
                    },
                    "per-process_emissions": [
                        0.004412103108738664,
                        0.0044224922355205155,
                        0.004391944440051508,
                        0.0044362204732750835
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0507": {
            "setup": {
                "experiment_id": "0507",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 12:36:59 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_topp_0.8_temp_0.6",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_p",
                    "decoder_temperature": 0.6,
                    "decoder_top_k": null,
                    "decoder_top_p": 0.8
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 39.22138663002988,
                        "average_latency_ms_per_batch": 4902.673328753735,
                        "throughput_queries_per_sec": 3.2635256169652274,
                        "throughput_tokens_per_sec": 417.7312789715491
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38403047424,
                        "gpu_max_memory_reserved_bytes": 38403047424
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.9,
                        "cpu_memory_usage_bytes": 1980329984
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0507",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_2": 2197.1901200062994,
                            "process_0": 577.368003034387,
                            "process_1": 2198.8288619376135,
                            "process_3": 701.1511880311702
                        },
                        "ram_power": {
                            "process_2": 0.7345218658447266,
                            "process_0": 0.6904506683349609,
                            "process_1": 0.7342472076416016,
                            "process_3": 0.732330322265625
                        },
                        "cpu_energy": {
                            "process_2": 0.0012266475242176968,
                            "process_0": 0.0011956316939940737,
                            "process_1": 0.0011740353525565297,
                            "process_3": 0.0012395683006216137
                        },
                        "gpu_energy": {
                            "process_2": 0.010348251889706006,
                            "process_0": 0.010406767214295343,
                            "process_1": 0.010348251889706006,
                            "process_3": 0.0103986194299992
                        },
                        "ram_energy": {
                            "process_2": 6.943633034334175e-06,
                            "process_0": 6.7157669186562995e-06,
                            "process_1": 6.638968661489595e-06,
                            "process_3": 6.9637939233070116e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.011581843046958036,
                            "process_0": 0.011609114675208074,
                            "process_1": 0.011528926210924027,
                            "process_3": 0.011645151524544123
                        },
                        "total_energy_joules": {
                            "process_2": 41694.63496904893,
                            "process_0": 41792.81283074906,
                            "process_1": 41504.1343593265,
                            "process_3": 41922.54548835884
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1418.6345432523676,
                        "ram_power_avg": 0.7228875160217285,
                        "cpu_energy_total": 0.004835882871389914,
                        "gpu_energy_total": 0.041501890423706556,
                        "ram_energy_total": 2.726216253778708e-05,
                        "total_energy_kwh": 0.04636503545763426,
                        "total_energy_joules": 166914.1276474833
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09815825796724903,
                        "joules_per_token": 10.187629861296589,
                        "flops_per_joule": 315363253.2534023,
                        "joules_per_flop": 3.1709464868960965e-09
                    },
                    "per-process_emissions": [
                        0.004412103108738664,
                        0.0044224922355205155,
                        0.004391944440051508,
                        0.0044362204732750835
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0508": {
            "setup": {
                "experiment_id": "0508",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 12:38:30 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_50_temp_1.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 1.2,
                    "decoder_top_k": 50,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.32900934893405,
                        "average_latency_ms_per_batch": 4791.126168616756,
                        "throughput_queries_per_sec": 3.3395071298277568,
                        "throughput_tokens_per_sec": 427.45691261795287
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38449184768,
                        "gpu_max_memory_reserved_bytes": 38449184768
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            16.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.5,
                        "cpu_memory_usage_bytes": 1976807424
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0508",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 3927.032213741866,
                            "process_1": 167.97688206027678,
                            "process_2": 173.71143485240725,
                            "process_3": 139.49948080741433
                        },
                        "ram_power": {
                            "process_0": 0.689201831817627,
                            "process_1": 0.7173786163330078,
                            "process_2": 0.7323203086853027,
                            "process_3": 0.7310357093811035
                        },
                        "cpu_energy": {
                            "process_0": 0.0011324790563176066,
                            "process_1": 0.0011541164177178871,
                            "process_2": 0.0010860165374670034,
                            "process_3": 0.0010905633058700915
                        },
                        "gpu_energy": {
                            "process_0": 0.010234837354529347,
                            "process_1": 0.010253070424671051,
                            "process_2": 0.010253070424671051,
                            "process_3": 0.010253070424671051
                        },
                        "ram_energy": {
                            "process_0": 6.874994477026648e-06,
                            "process_1": 6.708372920142716e-06,
                            "process_2": 6.828339183642666e-06,
                            "process_3": 7.180313680688707e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.011374191405323981,
                            "process_1": 0.011413895215309082,
                            "process_2": 0.011345915301321697,
                            "process_3": 0.011350814044221832
                        },
                        "total_energy_joules": {
                            "process_0": 40947.089059166334,
                            "process_1": 41090.022775112695,
                            "process_2": 40845.29508475811,
                            "process_3": 40862.9305591986
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1102.0550028654911,
                        "ram_power_avg": 0.7174841165542603,
                        "cpu_energy_total": 0.004463175317372589,
                        "gpu_energy_total": 0.0409940486285425,
                        "ram_energy_total": 2.759202026150074e-05,
                        "total_energy_kwh": 0.045484815966176595,
                        "total_energy_joules": 163745.33747823574
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.10005781081966797,
                        "joules_per_token": 9.9942222581931,
                        "flops_per_joule": 321466144.4382224,
                        "joules_per_flop": 3.1107474839925938e-09
                    },
                    "per-process_emissions": [
                        0.004332998215858171,
                        0.004348123382271995,
                        0.004322226434038501,
                        0.004324092610146307
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0508": {
            "setup": {
                "experiment_id": "0508",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 12:38:30 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_50_temp_1.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 1.2,
                    "decoder_top_k": 50,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.32900934893405,
                        "average_latency_ms_per_batch": 4791.126168616756,
                        "throughput_queries_per_sec": 3.3395071298277568,
                        "throughput_tokens_per_sec": 427.45691261795287
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38449184768,
                        "gpu_max_memory_reserved_bytes": 38449184768
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            16.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.5,
                        "cpu_memory_usage_bytes": 1976807424
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0508",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 3927.032213741866,
                            "process_1": 167.97688206027678,
                            "process_2": 173.71143485240725,
                            "process_3": 139.49948080741433
                        },
                        "ram_power": {
                            "process_0": 0.689201831817627,
                            "process_1": 0.7173786163330078,
                            "process_2": 0.7323203086853027,
                            "process_3": 0.7310357093811035
                        },
                        "cpu_energy": {
                            "process_0": 0.0011324790563176066,
                            "process_1": 0.0011541164177178871,
                            "process_2": 0.0010860165374670034,
                            "process_3": 0.0010905633058700915
                        },
                        "gpu_energy": {
                            "process_0": 0.010234837354529347,
                            "process_1": 0.010253070424671051,
                            "process_2": 0.010253070424671051,
                            "process_3": 0.010253070424671051
                        },
                        "ram_energy": {
                            "process_0": 6.874994477026648e-06,
                            "process_1": 6.708372920142716e-06,
                            "process_2": 6.828339183642666e-06,
                            "process_3": 7.180313680688707e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.011374191405323981,
                            "process_1": 0.011413895215309082,
                            "process_2": 0.011345915301321697,
                            "process_3": 0.011350814044221832
                        },
                        "total_energy_joules": {
                            "process_0": 40947.089059166334,
                            "process_1": 41090.022775112695,
                            "process_2": 40845.29508475811,
                            "process_3": 40862.9305591986
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1102.0550028654911,
                        "ram_power_avg": 0.7174841165542603,
                        "cpu_energy_total": 0.004463175317372589,
                        "gpu_energy_total": 0.0409940486285425,
                        "ram_energy_total": 2.759202026150074e-05,
                        "total_energy_kwh": 0.045484815966176595,
                        "total_energy_joules": 163745.33747823574
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.10005781081966797,
                        "joules_per_token": 9.9942222581931,
                        "flops_per_joule": 321466144.4382224,
                        "joules_per_flop": 3.1107474839925938e-09
                    },
                    "per-process_emissions": [
                        0.004332998215858171,
                        0.004348123382271995,
                        0.004322226434038501,
                        0.004324092610146307
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0509": {
            "setup": {
                "experiment_id": "0509",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 12:39:59 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_burst_0.05_0.1_2.0_20",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.05,
                    "delay_max": 0.1,
                    "simulate_burst": true,
                    "burst_interval": 2.0,
                    "burst_size": 20
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.711671116005164,
                        "average_latency_ms_per_batch": 4838.9588895006455,
                        "throughput_queries_per_sec": 3.3064963694393184,
                        "throughput_tokens_per_sec": 423.23153528823275
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38352715776,
                        "gpu_max_memory_reserved_bytes": 38352715776
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            22.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.0,
                        "cpu_memory_usage_bytes": 1975336960
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0509",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_1": 1227.1129320659193,
                            "process_3": 468.15757727392287,
                            "process_0": 1680.6943882379026,
                            "process_2": 915.9250098596478
                        },
                        "ram_power": {
                            "process_1": 0.7292747497558594,
                            "process_3": 0.7219104766845704,
                            "process_0": 0.6874451637268068,
                            "process_2": 0.7234067916870117
                        },
                        "cpu_energy": {
                            "process_1": 0.0011950641853127311,
                            "process_3": 0.0012132658467471629,
                            "process_0": 0.00119376298262614,
                            "process_2": 0.0012174529529456778
                        },
                        "gpu_energy": {
                            "process_1": 0.01023865819091796,
                            "process_3": 0.010270193216145529,
                            "process_0": 0.010197004546483868,
                            "process_2": 0.010237250412014731
                        },
                        "ram_energy": {
                            "process_1": 6.7275175543265205e-06,
                            "process_3": 6.847655878539894e-06,
                            "process_0": 7.023324575109417e-06,
                            "process_2": 7.16196235120209e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.011440449893785018,
                            "process_3": 0.011490306718771232,
                            "process_0": 0.011397790853685112,
                            "process_2": 0.011461865327311607
                        },
                        "total_energy_joules": {
                            "process_1": 41185.619617626064,
                            "process_3": 41365.104187576435,
                            "process_0": 41032.0470732664,
                            "process_2": 41262.715178321785
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1072.972476859348,
                        "ram_power_avg": 0.715509295463562,
                        "cpu_energy_total": 0.004819545967631712,
                        "gpu_energy_total": 0.04094310636556209,
                        "ram_energy_total": 2.776046035917792e-05,
                        "total_energy_kwh": 0.045790412793552976,
                        "total_energy_joules": 164845.4860567907
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09939004331823542,
                        "joules_per_token": 10.061369998583416,
                        "flops_per_joule": 319320738.2744442,
                        "joules_per_flop": 3.1316475259447053e-09
                    },
                    "per-process_emissions": [
                        0.004358239387037403,
                        0.004377232344515901,
                        0.0043419884257113435,
                        0.004366397596439356
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0509": {
            "setup": {
                "experiment_id": "0509",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 12:39:59 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_burst_0.05_0.1_2.0_20",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.05,
                    "delay_max": 0.1,
                    "simulate_burst": true,
                    "burst_interval": 2.0,
                    "burst_size": 20
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.711671116005164,
                        "average_latency_ms_per_batch": 4838.9588895006455,
                        "throughput_queries_per_sec": 3.3064963694393184,
                        "throughput_tokens_per_sec": 423.23153528823275
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38352715776,
                        "gpu_max_memory_reserved_bytes": 38352715776
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            22.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.0,
                        "cpu_memory_usage_bytes": 1975336960
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0509",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_1": 1227.1129320659193,
                            "process_3": 468.15757727392287,
                            "process_0": 1680.6943882379026,
                            "process_2": 915.9250098596478
                        },
                        "ram_power": {
                            "process_1": 0.7292747497558594,
                            "process_3": 0.7219104766845704,
                            "process_0": 0.6874451637268068,
                            "process_2": 0.7234067916870117
                        },
                        "cpu_energy": {
                            "process_1": 0.0011950641853127311,
                            "process_3": 0.0012132658467471629,
                            "process_0": 0.00119376298262614,
                            "process_2": 0.0012174529529456778
                        },
                        "gpu_energy": {
                            "process_1": 0.01023865819091796,
                            "process_3": 0.010270193216145529,
                            "process_0": 0.010197004546483868,
                            "process_2": 0.010237250412014731
                        },
                        "ram_energy": {
                            "process_1": 6.7275175543265205e-06,
                            "process_3": 6.847655878539894e-06,
                            "process_0": 7.023324575109417e-06,
                            "process_2": 7.16196235120209e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.011440449893785018,
                            "process_3": 0.011490306718771232,
                            "process_0": 0.011397790853685112,
                            "process_2": 0.011461865327311607
                        },
                        "total_energy_joules": {
                            "process_1": 41185.619617626064,
                            "process_3": 41365.104187576435,
                            "process_0": 41032.0470732664,
                            "process_2": 41262.715178321785
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1072.972476859348,
                        "ram_power_avg": 0.715509295463562,
                        "cpu_energy_total": 0.004819545967631712,
                        "gpu_energy_total": 0.04094310636556209,
                        "ram_energy_total": 2.776046035917792e-05,
                        "total_energy_kwh": 0.045790412793552976,
                        "total_energy_joules": 164845.4860567907
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09939004331823542,
                        "joules_per_token": 10.061369998583416,
                        "flops_per_joule": 319320738.2744442,
                        "joules_per_flop": 3.1316475259447053e-09
                    },
                    "per-process_emissions": [
                        0.004358239387037403,
                        0.004377232344515901,
                        0.0043419884257113435,
                        0.004366397596439356
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0511": {
            "setup": {
                "experiment_id": "0511",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 02:27:25 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_topp_0.5_temp_0.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_p",
                    "decoder_temperature": 0.0,
                    "decoder_top_k": null,
                    "decoder_top_p": 0.5
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.12760082585737,
                        "average_latency_ms_per_batch": 4765.950103232171,
                        "throughput_queries_per_sec": 3.3571480299697476,
                        "throughput_tokens_per_sec": 429.7149478361277
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 37415288832,
                        "gpu_max_memory_reserved_bytes": 37415288832
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.5,
                        "cpu_memory_usage_bytes": 1967898624
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0511",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_0": 3312.417299062143,
                            "process_3": 66.84887886408607,
                            "process_1": 75.79264906791708,
                            "process_2": 112.43581038952232
                        },
                        "ram_power": {
                            "process_0": 0.6848816871643066,
                            "process_3": 0.7219991683959961,
                            "process_1": 0.7284579277038575,
                            "process_2": 0.7212595939636232
                        },
                        "cpu_energy": {
                            "process_0": 0.0010742432329025175,
                            "process_3": 0.0010947877273156336,
                            "process_1": 0.0011154825476241965,
                            "process_2": 0.0010660823995895046
                        },
                        "gpu_energy": {
                            "process_0": 0.010126928657090417,
                            "process_3": 0.010140135334321343,
                            "process_1": 0.010140135334321343,
                            "process_2": 0.010142016169158552
                        },
                        "ram_energy": {
                            "process_0": 6.409332063754944e-06,
                            "process_3": 7.08955809732853e-06,
                            "process_1": 6.50785535048085e-06,
                            "process_2": 7.332470988127413e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.011207581222056687,
                            "process_3": 0.011242012619734305,
                            "process_1": 0.011262125737296024,
                            "process_2": 0.01121543103973618
                        },
                        "total_energy_joules": {
                            "process_0": 40347.292399404076,
                            "process_3": 40471.2454310435,
                            "process_1": 40543.65265426569,
                            "process_2": 40375.551743050244
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 891.8736593459171,
                        "ram_power_avg": 0.7141495943069458,
                        "cpu_energy_total": 0.004350595907431852,
                        "gpu_energy_total": 0.040549215494891655,
                        "ram_energy_total": 2.7339216499691737e-05,
                        "total_energy_kwh": 0.04492715061882319,
                        "total_energy_joules": 161737.74222776352
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.10129979418735549,
                        "joules_per_token": 9.87168836839377,
                        "flops_per_joule": 325456393.6890927,
                        "joules_per_flop": 3.072608249187743e-09
                    },
                    "per-process_emissions": [
                        0.004269528066542495,
                        0.004282644707487783,
                        0.004290306799622921,
                        0.004272518454587497
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0511": {
            "setup": {
                "experiment_id": "0511",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 02:27:25 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_topp_0.5_temp_0.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_p",
                    "decoder_temperature": 0.0,
                    "decoder_top_k": null,
                    "decoder_top_p": 0.5
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.12760082585737,
                        "average_latency_ms_per_batch": 4765.950103232171,
                        "throughput_queries_per_sec": 3.3571480299697476,
                        "throughput_tokens_per_sec": 429.7149478361277
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 37415288832,
                        "gpu_max_memory_reserved_bytes": 37415288832
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.5,
                        "cpu_memory_usage_bytes": 1967898624
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0511",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_0": 3312.417299062143,
                            "process_3": 66.84887886408607,
                            "process_1": 75.79264906791708,
                            "process_2": 112.43581038952232
                        },
                        "ram_power": {
                            "process_0": 0.6848816871643066,
                            "process_3": 0.7219991683959961,
                            "process_1": 0.7284579277038575,
                            "process_2": 0.7212595939636232
                        },
                        "cpu_energy": {
                            "process_0": 0.0010742432329025175,
                            "process_3": 0.0010947877273156336,
                            "process_1": 0.0011154825476241965,
                            "process_2": 0.0010660823995895046
                        },
                        "gpu_energy": {
                            "process_0": 0.010126928657090417,
                            "process_3": 0.010140135334321343,
                            "process_1": 0.010140135334321343,
                            "process_2": 0.010142016169158552
                        },
                        "ram_energy": {
                            "process_0": 6.409332063754944e-06,
                            "process_3": 7.08955809732853e-06,
                            "process_1": 6.50785535048085e-06,
                            "process_2": 7.332470988127413e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.011207581222056687,
                            "process_3": 0.011242012619734305,
                            "process_1": 0.011262125737296024,
                            "process_2": 0.01121543103973618
                        },
                        "total_energy_joules": {
                            "process_0": 40347.292399404076,
                            "process_3": 40471.2454310435,
                            "process_1": 40543.65265426569,
                            "process_2": 40375.551743050244
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 891.8736593459171,
                        "ram_power_avg": 0.7141495943069458,
                        "cpu_energy_total": 0.004350595907431852,
                        "gpu_energy_total": 0.040549215494891655,
                        "ram_energy_total": 2.7339216499691737e-05,
                        "total_energy_kwh": 0.04492715061882319,
                        "total_energy_joules": 161737.74222776352
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.10129979418735549,
                        "joules_per_token": 9.87168836839377,
                        "flops_per_joule": 325456393.6890927,
                        "joules_per_flop": 3.072608249187743e-09
                    },
                    "per-process_emissions": [
                        0.004269528066542495,
                        0.004282644707487783,
                        0.004290306799622921,
                        0.004272518454587497
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0512": {
            "setup": {
                "experiment_id": "0512",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 02:29:01 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_burst_0.2_0.4_4.0_20",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.2,
                    "delay_max": 0.4,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 20
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 40.71981145290192,
                        "average_latency_ms_per_batch": 5089.97643161274,
                        "throughput_queries_per_sec": 3.143433022720394,
                        "throughput_tokens_per_sec": 402.3594269082104
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38048628736,
                        "gpu_max_memory_reserved_bytes": 38048628736
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.5,
                        "cpu_memory_usage_bytes": 1972105216
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0512",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_1": 239.86441599505156,
                            "process_0": 237.00673121889304,
                            "process_2": 262.7415302815239,
                            "process_3": 31616.950916633512
                        },
                        "ram_power": {
                            "process_1": 0.7229719161987305,
                            "process_0": 0.6864151954650879,
                            "process_2": 0.7064065933227539,
                            "process_3": 0.7237429618835449
                        },
                        "cpu_energy": {
                            "process_1": 0.00122725398316652,
                            "process_0": 0.001230440537532559,
                            "process_2": 0.001253837665060928,
                            "process_3": 0.0011951930655068283
                        },
                        "gpu_energy": {
                            "process_1": 0.010335262712649396,
                            "process_0": 0.010338081326013437,
                            "process_2": 0.010335461046139471,
                            "process_3": 0.010339354660365174
                        },
                        "ram_energy": {
                            "process_1": 7.214752636695398e-06,
                            "process_0": 7.246661406530092e-06,
                            "process_2": 7.590371758395444e-06,
                            "process_3": 7.783089214417104e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.011569731448452612,
                            "process_0": 0.011575768524952524,
                            "process_2": 0.011596889082958795,
                            "process_3": 0.011542330815086418
                        },
                        "total_energy_joules": {
                            "process_1": 41651.0332144294,
                            "process_0": 41672.766689829084,
                            "process_2": 41748.80069865166,
                            "process_3": 41552.39093431111
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 8089.140898532245,
                        "ram_power_avg": 0.7098841667175293,
                        "cpu_energy_total": 0.004906725251266835,
                        "gpu_energy_total": 0.04134815974516748,
                        "ram_energy_total": 2.9834875016038036e-05,
                        "total_energy_kwh": 0.046284719871450344,
                        "total_energy_joules": 166624.99153722127
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09832858713958335,
                        "joules_per_token": 10.169982393629228,
                        "flops_per_joule": 315910487.5159463,
                        "joules_per_flop": 3.1654536317016783e-09
                    },
                    "per-process_emissions": [
                        0.004407489195288022,
                        0.004409789019580664,
                        0.004417834896153153,
                        0.0043970509240071716
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0512": {
            "setup": {
                "experiment_id": "0512",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 02:29:01 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_burst_0.2_0.4_4.0_20",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.2,
                    "delay_max": 0.4,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 20
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 40.71981145290192,
                        "average_latency_ms_per_batch": 5089.97643161274,
                        "throughput_queries_per_sec": 3.143433022720394,
                        "throughput_tokens_per_sec": 402.3594269082104
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38048628736,
                        "gpu_max_memory_reserved_bytes": 38048628736
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.5,
                        "cpu_memory_usage_bytes": 1972105216
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0512",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_1": 239.86441599505156,
                            "process_0": 237.00673121889304,
                            "process_2": 262.7415302815239,
                            "process_3": 31616.950916633512
                        },
                        "ram_power": {
                            "process_1": 0.7229719161987305,
                            "process_0": 0.6864151954650879,
                            "process_2": 0.7064065933227539,
                            "process_3": 0.7237429618835449
                        },
                        "cpu_energy": {
                            "process_1": 0.00122725398316652,
                            "process_0": 0.001230440537532559,
                            "process_2": 0.001253837665060928,
                            "process_3": 0.0011951930655068283
                        },
                        "gpu_energy": {
                            "process_1": 0.010335262712649396,
                            "process_0": 0.010338081326013437,
                            "process_2": 0.010335461046139471,
                            "process_3": 0.010339354660365174
                        },
                        "ram_energy": {
                            "process_1": 7.214752636695398e-06,
                            "process_0": 7.246661406530092e-06,
                            "process_2": 7.590371758395444e-06,
                            "process_3": 7.783089214417104e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.011569731448452612,
                            "process_0": 0.011575768524952524,
                            "process_2": 0.011596889082958795,
                            "process_3": 0.011542330815086418
                        },
                        "total_energy_joules": {
                            "process_1": 41651.0332144294,
                            "process_0": 41672.766689829084,
                            "process_2": 41748.80069865166,
                            "process_3": 41552.39093431111
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 8089.140898532245,
                        "ram_power_avg": 0.7098841667175293,
                        "cpu_energy_total": 0.004906725251266835,
                        "gpu_energy_total": 0.04134815974516748,
                        "ram_energy_total": 2.9834875016038036e-05,
                        "total_energy_kwh": 0.046284719871450344,
                        "total_energy_joules": 166624.99153722127
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09832858713958335,
                        "joules_per_token": 10.169982393629228,
                        "flops_per_joule": 315910487.5159463,
                        "joules_per_flop": 3.1654536317016783e-09
                    },
                    "per-process_emissions": [
                        0.004407489195288022,
                        0.004409789019580664,
                        0.004417834896153153,
                        0.0043970509240071716
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0513": {
            "setup": {
                "experiment_id": "0513",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 02:30:33 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_5_temp_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 5,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.44587625586428,
                        "average_latency_ms_per_batch": 4805.734531983035,
                        "throughput_queries_per_sec": 3.32935577142622,
                        "throughput_tokens_per_sec": 426.15753874255614
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 32684113920,
                        "gpu_max_memory_reserved_bytes": 32684113920
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            27.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.6,
                        "cpu_memory_usage_bytes": 1980645376
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0513",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_2": 199.60195235844049,
                            "process_3": 166.27445859588371,
                            "process_0": 190.93735587964318,
                            "process_1": 155.8970245128127
                        },
                        "ram_power": {
                            "process_2": 0.7257542610168457,
                            "process_3": 0.7322087287902832,
                            "process_0": 0.689784049987793,
                            "process_1": 0.7085981369018555
                        },
                        "cpu_energy": {
                            "process_2": 0.0010839963750840977,
                            "process_3": 0.0011078999147212016,
                            "process_0": 0.0010841857509367402,
                            "process_1": 0.0011116536122899561
                        },
                        "gpu_energy": {
                            "process_2": 0.010237793745785595,
                            "process_3": 0.01023451513205309,
                            "process_0": 0.010239104024611478,
                            "process_1": 0.01023451513205309
                        },
                        "ram_energy": {
                            "process_2": 7.483836010398788e-06,
                            "process_3": 6.559545703870864e-06,
                            "process_0": 7.084315063701414e-06,
                            "process_1": 6.334966540704689e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.011329273956880092,
                            "process_3": 0.011348974592478165,
                            "process_0": 0.011330374090611918,
                            "process_1": 0.01135250371088375
                        },
                        "total_energy_joules": {
                            "process_2": 40785.38624476833,
                            "process_3": 40856.30853292139,
                            "process_0": 40789.34672620291,
                            "process_1": 40869.013359181496
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 178.17769783669502,
                        "ram_power_avg": 0.7140862941741943,
                        "cpu_energy_total": 0.004387735653031996,
                        "gpu_energy_total": 0.040945928034503254,
                        "ram_energy_total": 2.7462663318675756e-05,
                        "total_energy_kwh": 0.045361126350853924,
                        "total_energy_joules": 163300.05486307413
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.10033064602297814,
                        "joules_per_token": 9.967044364201302,
                        "flops_per_joule": 322342710.49695027,
                        "joules_per_flop": 3.1022882399242626e-09
                    },
                    "per-process_emissions": [
                        0.004315886913873471,
                        0.004323391871004557,
                        0.00431630600981861,
                        0.0043247362886611644
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0513": {
            "setup": {
                "experiment_id": "0513",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 02:30:33 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_5_temp_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 5,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.44587625586428,
                        "average_latency_ms_per_batch": 4805.734531983035,
                        "throughput_queries_per_sec": 3.32935577142622,
                        "throughput_tokens_per_sec": 426.15753874255614
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 32684113920,
                        "gpu_max_memory_reserved_bytes": 32684113920
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            27.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.6,
                        "cpu_memory_usage_bytes": 1980645376
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0513",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_2": 199.60195235844049,
                            "process_3": 166.27445859588371,
                            "process_0": 190.93735587964318,
                            "process_1": 155.8970245128127
                        },
                        "ram_power": {
                            "process_2": 0.7257542610168457,
                            "process_3": 0.7322087287902832,
                            "process_0": 0.689784049987793,
                            "process_1": 0.7085981369018555
                        },
                        "cpu_energy": {
                            "process_2": 0.0010839963750840977,
                            "process_3": 0.0011078999147212016,
                            "process_0": 0.0010841857509367402,
                            "process_1": 0.0011116536122899561
                        },
                        "gpu_energy": {
                            "process_2": 0.010237793745785595,
                            "process_3": 0.01023451513205309,
                            "process_0": 0.010239104024611478,
                            "process_1": 0.01023451513205309
                        },
                        "ram_energy": {
                            "process_2": 7.483836010398788e-06,
                            "process_3": 6.559545703870864e-06,
                            "process_0": 7.084315063701414e-06,
                            "process_1": 6.334966540704689e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.011329273956880092,
                            "process_3": 0.011348974592478165,
                            "process_0": 0.011330374090611918,
                            "process_1": 0.01135250371088375
                        },
                        "total_energy_joules": {
                            "process_2": 40785.38624476833,
                            "process_3": 40856.30853292139,
                            "process_0": 40789.34672620291,
                            "process_1": 40869.013359181496
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 178.17769783669502,
                        "ram_power_avg": 0.7140862941741943,
                        "cpu_energy_total": 0.004387735653031996,
                        "gpu_energy_total": 0.040945928034503254,
                        "ram_energy_total": 2.7462663318675756e-05,
                        "total_energy_kwh": 0.045361126350853924,
                        "total_energy_joules": 163300.05486307413
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.10033064602297814,
                        "joules_per_token": 9.967044364201302,
                        "flops_per_joule": 322342710.49695027,
                        "joules_per_flop": 3.1022882399242626e-09
                    },
                    "per-process_emissions": [
                        0.004315886913873471,
                        0.004323391871004557,
                        0.00431630600981861,
                        0.0043247362886611644
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0514": {
            "setup": {
                "experiment_id": "0514",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 02:32:06 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_200_temp_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 0.8,
                    "decoder_top_k": 200,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 40.12117117794696,
                        "average_latency_ms_per_batch": 5015.14639724337,
                        "throughput_queries_per_sec": 3.190335581987113,
                        "throughput_tokens_per_sec": 408.36295449435045
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 37878759424,
                        "gpu_max_memory_reserved_bytes": 37878759424
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            27.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.5,
                        "cpu_memory_usage_bytes": 1979838464
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0514",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_2": 1024.3199424529762,
                            "process_3": 434.6291058882344,
                            "process_0": 58.92893812194341,
                            "process_1": 1103.6968682411107
                        },
                        "ram_power": {
                            "process_2": 0.7324962615966797,
                            "process_3": 0.7249903678894043,
                            "process_0": 0.6892175674438477,
                            "process_1": 0.7337193489074707
                        },
                        "cpu_energy": {
                            "process_2": 0.0011407042144783188,
                            "process_3": 0.0011254591720407914,
                            "process_0": 0.001224902992744319,
                            "process_1": 0.0010725752718699366
                        },
                        "gpu_energy": {
                            "process_2": 0.010275940720744003,
                            "process_3": 0.010343178274537124,
                            "process_0": 0.010346923555310639,
                            "process_1": 0.010300859629571946
                        },
                        "ram_energy": {
                            "process_2": 7.898197105699966e-06,
                            "process_3": 6.939974647143249e-06,
                            "process_0": 7.582791130396799e-06,
                            "process_1": 6.737603223564282e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.011424543132328024,
                            "process_3": 0.011475577421225059,
                            "process_0": 0.01157940933918535,
                            "process_1": 0.011380172504665446
                        },
                        "total_energy_joules": {
                            "process_2": 41128.355276380884,
                            "process_3": 41312.078716410215,
                            "process_0": 41685.873621067265,
                            "process_1": 40968.6210167956
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 655.3937136760662,
                        "ram_power_avg": 0.7201058864593506,
                        "cpu_energy_total": 0.004563641651133365,
                        "gpu_energy_total": 0.04126690218016371,
                        "ram_energy_total": 2.9158566106804298e-05,
                        "total_energy_kwh": 0.045859702397403876,
                        "total_energy_joules": 165094.92863065397
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09923987451276503,
                        "joules_per_token": 10.076594765054564,
                        "flops_per_joule": 318838275.3211375,
                        "joules_per_flop": 3.13638630428793e-09
                    },
                    "per-process_emissions": [
                        0.004352179706260361,
                        0.004371621218615686,
                        0.00441117598776266,
                        0.004335276715652302
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0514": {
            "setup": {
                "experiment_id": "0514",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 02:32:06 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_200_temp_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 0.8,
                    "decoder_top_k": 200,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 40.12117117794696,
                        "average_latency_ms_per_batch": 5015.14639724337,
                        "throughput_queries_per_sec": 3.190335581987113,
                        "throughput_tokens_per_sec": 408.36295449435045
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 37878759424,
                        "gpu_max_memory_reserved_bytes": 37878759424
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            27.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.5,
                        "cpu_memory_usage_bytes": 1979838464
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0514",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_2": 1024.3199424529762,
                            "process_3": 434.6291058882344,
                            "process_0": 58.92893812194341,
                            "process_1": 1103.6968682411107
                        },
                        "ram_power": {
                            "process_2": 0.7324962615966797,
                            "process_3": 0.7249903678894043,
                            "process_0": 0.6892175674438477,
                            "process_1": 0.7337193489074707
                        },
                        "cpu_energy": {
                            "process_2": 0.0011407042144783188,
                            "process_3": 0.0011254591720407914,
                            "process_0": 0.001224902992744319,
                            "process_1": 0.0010725752718699366
                        },
                        "gpu_energy": {
                            "process_2": 0.010275940720744003,
                            "process_3": 0.010343178274537124,
                            "process_0": 0.010346923555310639,
                            "process_1": 0.010300859629571946
                        },
                        "ram_energy": {
                            "process_2": 7.898197105699966e-06,
                            "process_3": 6.939974647143249e-06,
                            "process_0": 7.582791130396799e-06,
                            "process_1": 6.737603223564282e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.011424543132328024,
                            "process_3": 0.011475577421225059,
                            "process_0": 0.01157940933918535,
                            "process_1": 0.011380172504665446
                        },
                        "total_energy_joules": {
                            "process_2": 41128.355276380884,
                            "process_3": 41312.078716410215,
                            "process_0": 41685.873621067265,
                            "process_1": 40968.6210167956
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 655.3937136760662,
                        "ram_power_avg": 0.7201058864593506,
                        "cpu_energy_total": 0.004563641651133365,
                        "gpu_energy_total": 0.04126690218016371,
                        "ram_energy_total": 2.9158566106804298e-05,
                        "total_energy_kwh": 0.045859702397403876,
                        "total_energy_joules": 165094.92863065397
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09923987451276503,
                        "joules_per_token": 10.076594765054564,
                        "flops_per_joule": 318838275.3211375,
                        "joules_per_flop": 3.13638630428793e-09
                    },
                    "per-process_emissions": [
                        0.004352179706260361,
                        0.004371621218615686,
                        0.00441117598776266,
                        0.004335276715652302
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0515": {
            "setup": {
                "experiment_id": "0515",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 02:33:38 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_5_temp_1.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 1.4,
                    "decoder_top_k": 5,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.383741943049245,
                        "average_latency_ms_per_batch": 4797.967742881156,
                        "throughput_queries_per_sec": 3.334745220773844,
                        "throughput_tokens_per_sec": 426.847388259052
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 33858519040,
                        "gpu_max_memory_reserved_bytes": 33858519040
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            27.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.5,
                        "cpu_memory_usage_bytes": 1978888192
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0515",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_3": 135.4321576743419,
                            "process_2": 144.01174190163377,
                            "process_1": 185.78114156120108,
                            "process_0": 177.0978443669771
                        },
                        "ram_power": {
                            "process_3": 0.7163558006286621,
                            "process_2": 0.7095150947570801,
                            "process_1": 0.7313432693481445,
                            "process_0": 0.6891217231750488
                        },
                        "cpu_energy": {
                            "process_3": 0.0012602797207746333,
                            "process_2": 0.0012239135581839948,
                            "process_1": 0.0012167336823058577,
                            "process_0": 0.0012590767185902219
                        },
                        "gpu_energy": {
                            "process_3": 0.010245509307509337,
                            "process_2": 0.010236106522208743,
                            "process_1": 0.010249820144290211,
                            "process_0": 0.010248366254238306
                        },
                        "ram_energy": {
                            "process_3": 7.368430273611078e-06,
                            "process_2": 7.071116159953156e-06,
                            "process_1": 7.276251617241768e-06,
                            "process_0": 7.073920745396664e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.011513157458557579,
                            "process_2": 0.011467091196552691,
                            "process_1": 0.011473830078213313,
                            "process_0": 0.011514516893573926
                        },
                        "total_energy_joules": {
                            "process_3": 41447.36685080728,
                            "process_2": 41281.52830758969,
                            "process_1": 41305.788281567926,
                            "process_0": 41452.26081686613
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 160.58072137603847,
                        "ram_power_avg": 0.7115839719772339,
                        "cpu_energy_total": 0.004960003679854707,
                        "gpu_energy_total": 0.0409798022282466,
                        "ram_energy_total": 2.8789718796202665e-05,
                        "total_energy_kwh": 0.04596859562689751,
                        "total_energy_joules": 165486.94425683102
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09900478900965444,
                        "joules_per_token": 10.100521500050721,
                        "flops_per_joule": 318082991.65383357,
                        "joules_per_flop": 3.1438336102179576e-09
                    },
                    "per-process_emissions": [
                        0.00438593733383751,
                        0.004368388391326748,
                        0.0043709555682953614,
                        0.004386455210606987
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0515": {
            "setup": {
                "experiment_id": "0515",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 02:33:38 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_5_temp_1.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 1.4,
                    "decoder_top_k": 5,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.383741943049245,
                        "average_latency_ms_per_batch": 4797.967742881156,
                        "throughput_queries_per_sec": 3.334745220773844,
                        "throughput_tokens_per_sec": 426.847388259052
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 33858519040,
                        "gpu_max_memory_reserved_bytes": 33858519040
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            27.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.5,
                        "cpu_memory_usage_bytes": 1978888192
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0515",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_3": 135.4321576743419,
                            "process_2": 144.01174190163377,
                            "process_1": 185.78114156120108,
                            "process_0": 177.0978443669771
                        },
                        "ram_power": {
                            "process_3": 0.7163558006286621,
                            "process_2": 0.7095150947570801,
                            "process_1": 0.7313432693481445,
                            "process_0": 0.6891217231750488
                        },
                        "cpu_energy": {
                            "process_3": 0.0012602797207746333,
                            "process_2": 0.0012239135581839948,
                            "process_1": 0.0012167336823058577,
                            "process_0": 0.0012590767185902219
                        },
                        "gpu_energy": {
                            "process_3": 0.010245509307509337,
                            "process_2": 0.010236106522208743,
                            "process_1": 0.010249820144290211,
                            "process_0": 0.010248366254238306
                        },
                        "ram_energy": {
                            "process_3": 7.368430273611078e-06,
                            "process_2": 7.071116159953156e-06,
                            "process_1": 7.276251617241768e-06,
                            "process_0": 7.073920745396664e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.011513157458557579,
                            "process_2": 0.011467091196552691,
                            "process_1": 0.011473830078213313,
                            "process_0": 0.011514516893573926
                        },
                        "total_energy_joules": {
                            "process_3": 41447.36685080728,
                            "process_2": 41281.52830758969,
                            "process_1": 41305.788281567926,
                            "process_0": 41452.26081686613
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 160.58072137603847,
                        "ram_power_avg": 0.7115839719772339,
                        "cpu_energy_total": 0.004960003679854707,
                        "gpu_energy_total": 0.0409798022282466,
                        "ram_energy_total": 2.8789718796202665e-05,
                        "total_energy_kwh": 0.04596859562689751,
                        "total_energy_joules": 165486.94425683102
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09900478900965444,
                        "joules_per_token": 10.100521500050721,
                        "flops_per_joule": 318082991.65383357,
                        "joules_per_flop": 3.1438336102179576e-09
                    },
                    "per-process_emissions": [
                        0.00438593733383751,
                        0.004368388391326748,
                        0.0043709555682953614,
                        0.004386455210606987
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0516": {
            "setup": {
                "experiment_id": "0516",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 02:35:17 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_burst_0.4_0.5_2.0_5",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.4,
                    "delay_max": 0.5,
                    "simulate_burst": true,
                    "burst_interval": 2.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 45.261361014971044,
                        "average_latency_ms_per_batch": 5657.6701268713805,
                        "throughput_queries_per_sec": 2.8280192448844303,
                        "throughput_tokens_per_sec": 361.9864633452071
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38352715776,
                        "gpu_max_memory_reserved_bytes": 38352715776
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            7.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.5,
                        "cpu_memory_usage_bytes": 1973207040
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0516",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 932.7542419372948,
                            "process_2": 1090.217765945709,
                            "process_3": 802.4577180066276,
                            "process_1": 1096.187150389562
                        },
                        "ram_power": {
                            "process_0": 0.6879315376281738,
                            "process_2": 0.7237157821655273,
                            "process_3": 0.7315449714660645,
                            "process_1": 0.7081003189086915
                        },
                        "cpu_energy": {
                            "process_0": 0.001304478191284943,
                            "process_2": 0.0013054043842421381,
                            "process_3": 0.0014395466249425223,
                            "process_1": 0.001409341584436334
                        },
                        "gpu_energy": {
                            "process_0": 0.010593877086204628,
                            "process_2": 0.010593877086204628,
                            "process_3": 0.010682595768294334,
                            "process_1": 0.010680816877982835
                        },
                        "ram_energy": {
                            "process_0": 8.01677099771056e-06,
                            "process_2": 8.015612949448554e-06,
                            "process_3": 8.613789220913112e-06,
                            "process_1": 8.114202274073712e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.01190637204848728,
                            "process_2": 0.01190729708339621,
                            "process_3": 0.012130756182457774,
                            "process_1": 0.012098272664693243
                        },
                        "total_energy_joules": {
                            "process_0": 42862.93937455421,
                            "process_2": 42866.26950022636,
                            "process_3": 43670.722256847985,
                            "process_1": 43553.78159289568
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 980.4042190697983,
                        "ram_power_avg": 0.7128231525421143,
                        "cpu_energy_total": 0.005458770784905937,
                        "gpu_energy_total": 0.042551166818686426,
                        "ram_energy_total": 3.276037544214594e-05,
                        "total_energy_kwh": 0.04804269797903451,
                        "total_energy_joules": 172953.71272452423
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09473054808656216,
                        "joules_per_token": 10.55625688015895,
                        "flops_per_joule": 304350692.9088319,
                        "joules_per_flop": 3.2856833360308782e-09
                    },
                    "per-process_emissions": [
                        0.0045357324318712295,
                        0.004536084823919787,
                        0.004621211567707289,
                        0.004608836971614891
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0516": {
            "setup": {
                "experiment_id": "0516",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 02:35:17 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_burst_0.4_0.5_2.0_5",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.4,
                    "delay_max": 0.5,
                    "simulate_burst": true,
                    "burst_interval": 2.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 45.261361014971044,
                        "average_latency_ms_per_batch": 5657.6701268713805,
                        "throughput_queries_per_sec": 2.8280192448844303,
                        "throughput_tokens_per_sec": 361.9864633452071
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38352715776,
                        "gpu_max_memory_reserved_bytes": 38352715776
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            7.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.5,
                        "cpu_memory_usage_bytes": 1973207040
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0516",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 932.7542419372948,
                            "process_2": 1090.217765945709,
                            "process_3": 802.4577180066276,
                            "process_1": 1096.187150389562
                        },
                        "ram_power": {
                            "process_0": 0.6879315376281738,
                            "process_2": 0.7237157821655273,
                            "process_3": 0.7315449714660645,
                            "process_1": 0.7081003189086915
                        },
                        "cpu_energy": {
                            "process_0": 0.001304478191284943,
                            "process_2": 0.0013054043842421381,
                            "process_3": 0.0014395466249425223,
                            "process_1": 0.001409341584436334
                        },
                        "gpu_energy": {
                            "process_0": 0.010593877086204628,
                            "process_2": 0.010593877086204628,
                            "process_3": 0.010682595768294334,
                            "process_1": 0.010680816877982835
                        },
                        "ram_energy": {
                            "process_0": 8.01677099771056e-06,
                            "process_2": 8.015612949448554e-06,
                            "process_3": 8.613789220913112e-06,
                            "process_1": 8.114202274073712e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.01190637204848728,
                            "process_2": 0.01190729708339621,
                            "process_3": 0.012130756182457774,
                            "process_1": 0.012098272664693243
                        },
                        "total_energy_joules": {
                            "process_0": 42862.93937455421,
                            "process_2": 42866.26950022636,
                            "process_3": 43670.722256847985,
                            "process_1": 43553.78159289568
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 980.4042190697983,
                        "ram_power_avg": 0.7128231525421143,
                        "cpu_energy_total": 0.005458770784905937,
                        "gpu_energy_total": 0.042551166818686426,
                        "ram_energy_total": 3.276037544214594e-05,
                        "total_energy_kwh": 0.04804269797903451,
                        "total_energy_joules": 172953.71272452423
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09473054808656216,
                        "joules_per_token": 10.55625688015895,
                        "flops_per_joule": 304350692.9088319,
                        "joules_per_flop": 3.2856833360308782e-09
                    },
                    "per-process_emissions": [
                        0.0045357324318712295,
                        0.004536084823919787,
                        0.004621211567707289,
                        0.004608836971614891
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0517": {
            "setup": {
                "experiment_id": "0517",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 02:36:56 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_burst_0.05_0.1_6.0_5",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.05,
                    "delay_max": 0.1,
                    "simulate_burst": true,
                    "burst_interval": 6.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 46.65300378203392,
                        "average_latency_ms_per_batch": 5831.62547275424,
                        "throughput_queries_per_sec": 2.7436604210529487,
                        "throughput_tokens_per_sec": 351.18853389477744
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 32331792384,
                        "gpu_max_memory_reserved_bytes": 32331792384
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            22.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.2,
                        "cpu_memory_usage_bytes": 1952620544
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0517",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_3": 292.139439474143,
                            "process_2": 804.3152828074709,
                            "process_0": 751.8649808660382,
                            "process_1": 785.1664577630066
                        },
                        "ram_power": {
                            "process_3": 0.7226228713989258,
                            "process_2": 0.7072334289550781,
                            "process_0": 0.6807575225830078,
                            "process_1": 0.7308826446533203
                        },
                        "cpu_energy": {
                            "process_3": 0.0013810364777727955,
                            "process_2": 0.0014228884354615732,
                            "process_0": 0.0014599440266883901,
                            "process_1": 0.001412129008129341
                        },
                        "gpu_energy": {
                            "process_3": 0.01045511947519806,
                            "process_2": 0.010644124626402629,
                            "process_0": 0.010690551885767619,
                            "process_1": 0.010688352161785986
                        },
                        "ram_energy": {
                            "process_3": 7.703989001147438e-06,
                            "process_2": 8.220366271173841e-06,
                            "process_0": 8.1257903374236e-06,
                            "process_1": 8.390406961922048e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.011843859941972,
                            "process_2": 0.01207523342813538,
                            "process_0": 0.012158621702793432,
                            "process_1": 0.012108871576877251
                        },
                        "total_energy_joules": {
                            "process_3": 42637.8957910992,
                            "process_2": 43470.840341287374,
                            "process_0": 43771.03813005636,
                            "process_1": 43591.937676758105
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 658.3715402276647,
                        "ram_power_avg": 0.710374116897583,
                        "cpu_energy_total": 0.005675997948052099,
                        "gpu_energy_total": 0.04247814814915429,
                        "ram_energy_total": 3.2440552571666925e-05,
                        "total_energy_kwh": 0.04818658664977807,
                        "total_energy_joules": 173471.71193920105
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.0944476757440563,
                        "joules_per_token": 10.587873043164127,
                        "flops_per_joule": 303441879.4881839,
                        "joules_per_flop": 3.295524011671369e-09
                    },
                    "per-process_emissions": [
                        0.004511918444894234,
                        0.004600060174448173,
                        0.004631826937679159,
                        0.004612874627211389
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0517": {
            "setup": {
                "experiment_id": "0517",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 02:36:56 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_burst_0.05_0.1_6.0_5",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.05,
                    "delay_max": 0.1,
                    "simulate_burst": true,
                    "burst_interval": 6.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 46.65300378203392,
                        "average_latency_ms_per_batch": 5831.62547275424,
                        "throughput_queries_per_sec": 2.7436604210529487,
                        "throughput_tokens_per_sec": 351.18853389477744
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 32331792384,
                        "gpu_max_memory_reserved_bytes": 32331792384
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            22.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.2,
                        "cpu_memory_usage_bytes": 1952620544
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0517",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_3": 292.139439474143,
                            "process_2": 804.3152828074709,
                            "process_0": 751.8649808660382,
                            "process_1": 785.1664577630066
                        },
                        "ram_power": {
                            "process_3": 0.7226228713989258,
                            "process_2": 0.7072334289550781,
                            "process_0": 0.6807575225830078,
                            "process_1": 0.7308826446533203
                        },
                        "cpu_energy": {
                            "process_3": 0.0013810364777727955,
                            "process_2": 0.0014228884354615732,
                            "process_0": 0.0014599440266883901,
                            "process_1": 0.001412129008129341
                        },
                        "gpu_energy": {
                            "process_3": 0.01045511947519806,
                            "process_2": 0.010644124626402629,
                            "process_0": 0.010690551885767619,
                            "process_1": 0.010688352161785986
                        },
                        "ram_energy": {
                            "process_3": 7.703989001147438e-06,
                            "process_2": 8.220366271173841e-06,
                            "process_0": 8.1257903374236e-06,
                            "process_1": 8.390406961922048e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.011843859941972,
                            "process_2": 0.01207523342813538,
                            "process_0": 0.012158621702793432,
                            "process_1": 0.012108871576877251
                        },
                        "total_energy_joules": {
                            "process_3": 42637.8957910992,
                            "process_2": 43470.840341287374,
                            "process_0": 43771.03813005636,
                            "process_1": 43591.937676758105
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 658.3715402276647,
                        "ram_power_avg": 0.710374116897583,
                        "cpu_energy_total": 0.005675997948052099,
                        "gpu_energy_total": 0.04247814814915429,
                        "ram_energy_total": 3.2440552571666925e-05,
                        "total_energy_kwh": 0.04818658664977807,
                        "total_energy_joules": 173471.71193920105
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.0944476757440563,
                        "joules_per_token": 10.587873043164127,
                        "flops_per_joule": 303441879.4881839,
                        "joules_per_flop": 3.295524011671369e-09
                    },
                    "per-process_emissions": [
                        0.004511918444894234,
                        0.004600060174448173,
                        0.004631826937679159,
                        0.004612874627211389
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0518": {
            "setup": {
                "experiment_id": "0518",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 02:38:30 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_topp_0.3_temp_1.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_p",
                    "decoder_temperature": 1.2,
                    "decoder_top_k": null,
                    "decoder_top_p": 0.3
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.899354794062674,
                        "average_latency_ms_per_batch": 4862.419349257834,
                        "throughput_queries_per_sec": 3.290543009714316,
                        "throughput_tokens_per_sec": 421.18950524343245
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38098960384,
                        "gpu_max_memory_reserved_bytes": 38098960384
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.0,
                        "cpu_memory_usage_bytes": 1981657088
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0518",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_2": 1423.963528194875,
                            "process_3": 599.3854515429516,
                            "process_0": 1311.3429624612731,
                            "process_1": 183.70669863188536
                        },
                        "ram_power": {
                            "process_2": 0.7105278968811035,
                            "process_3": 0.710310459136963,
                            "process_0": 0.6908955574035645,
                            "process_1": 0.7323260307312012
                        },
                        "cpu_energy": {
                            "process_2": 0.0011656333016635471,
                            "process_3": 0.0011590967934062067,
                            "process_0": 0.0012212274813809927,
                            "process_1": 0.0011870342283218634
                        },
                        "gpu_energy": {
                            "process_2": 0.010360219399281334,
                            "process_3": 0.010397536651357342,
                            "process_0": 0.010361148288911437,
                            "process_1": 0.010381643027526977
                        },
                        "ram_energy": {
                            "process_2": 7.109193644825042e-06,
                            "process_3": 7.07593733609526e-06,
                            "process_0": 6.90791628687046e-06,
                            "process_1": 6.750422512818581e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.011532961894589706,
                            "process_3": 0.011563709382099644,
                            "process_0": 0.011589283686579297,
                            "process_1": 0.011575427678361658
                        },
                        "total_energy_joules": {
                            "process_2": 41518.66282052294,
                            "process_3": 41629.35377555872,
                            "process_0": 41721.42127168547,
                            "process_1": 41671.53964210197
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 879.5996602077463,
                        "ram_power_avg": 0.711014986038208,
                        "cpu_energy_total": 0.004732991804772609,
                        "gpu_energy_total": 0.04150054736707709,
                        "ram_energy_total": 2.7843469780609342e-05,
                        "total_energy_kwh": 0.04626138264163031,
                        "total_energy_joules": 166540.9775098691
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09837819043081512,
                        "joules_per_token": 10.164854584342596,
                        "flops_per_joule": 316069853.17319083,
                        "joules_per_flop": 3.1638575775591255e-09
                    },
                    "per-process_emissions": [
                        0.004393481833743948,
                        0.004405195089110859,
                        0.004414937620402384,
                        0.004409659174071874
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0518": {
            "setup": {
                "experiment_id": "0518",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 02:38:30 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_topp_0.3_temp_1.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_p",
                    "decoder_temperature": 1.2,
                    "decoder_top_k": null,
                    "decoder_top_p": 0.3
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.899354794062674,
                        "average_latency_ms_per_batch": 4862.419349257834,
                        "throughput_queries_per_sec": 3.290543009714316,
                        "throughput_tokens_per_sec": 421.18950524343245
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38098960384,
                        "gpu_max_memory_reserved_bytes": 38098960384
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.0,
                        "cpu_memory_usage_bytes": 1981657088
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0518",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_2": 1423.963528194875,
                            "process_3": 599.3854515429516,
                            "process_0": 1311.3429624612731,
                            "process_1": 183.70669863188536
                        },
                        "ram_power": {
                            "process_2": 0.7105278968811035,
                            "process_3": 0.710310459136963,
                            "process_0": 0.6908955574035645,
                            "process_1": 0.7323260307312012
                        },
                        "cpu_energy": {
                            "process_2": 0.0011656333016635471,
                            "process_3": 0.0011590967934062067,
                            "process_0": 0.0012212274813809927,
                            "process_1": 0.0011870342283218634
                        },
                        "gpu_energy": {
                            "process_2": 0.010360219399281334,
                            "process_3": 0.010397536651357342,
                            "process_0": 0.010361148288911437,
                            "process_1": 0.010381643027526977
                        },
                        "ram_energy": {
                            "process_2": 7.109193644825042e-06,
                            "process_3": 7.07593733609526e-06,
                            "process_0": 6.90791628687046e-06,
                            "process_1": 6.750422512818581e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.011532961894589706,
                            "process_3": 0.011563709382099644,
                            "process_0": 0.011589283686579297,
                            "process_1": 0.011575427678361658
                        },
                        "total_energy_joules": {
                            "process_2": 41518.66282052294,
                            "process_3": 41629.35377555872,
                            "process_0": 41721.42127168547,
                            "process_1": 41671.53964210197
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 879.5996602077463,
                        "ram_power_avg": 0.711014986038208,
                        "cpu_energy_total": 0.004732991804772609,
                        "gpu_energy_total": 0.04150054736707709,
                        "ram_energy_total": 2.7843469780609342e-05,
                        "total_energy_kwh": 0.04626138264163031,
                        "total_energy_joules": 166540.9775098691
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09837819043081512,
                        "joules_per_token": 10.164854584342596,
                        "flops_per_joule": 316069853.17319083,
                        "joules_per_flop": 3.1638575775591255e-09
                    },
                    "per-process_emissions": [
                        0.004393481833743948,
                        0.004405195089110859,
                        0.004414937620402384,
                        0.004409659174071874
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0519": {
            "setup": {
                "experiment_id": "0519",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 02:40:03 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_50_temp_1.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 1.4,
                    "decoder_top_k": 50,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.38980831898516,
                        "average_latency_ms_per_batch": 4798.726039873145,
                        "throughput_queries_per_sec": 3.334218262733532,
                        "throughput_tokens_per_sec": 426.7799376298921
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38449184768,
                        "gpu_max_memory_reserved_bytes": 38449184768
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.1,
                        "cpu_memory_usage_bytes": 1975963648
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0519",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_1": 39.918203102313555,
                            "process_0": 38.80088655989886,
                            "process_2": 39.37683684691875,
                            "process_3": 1070.7472914798682
                        },
                        "ram_power": {
                            "process_1": 0.7178964614868164,
                            "process_0": 0.6888928413391113,
                            "process_2": 0.7342300415039064,
                            "process_3": 0.7087368965148926
                        },
                        "cpu_energy": {
                            "process_1": 0.0012984812338108895,
                            "process_0": 0.001336691696185881,
                            "process_2": 0.001280069821588768,
                            "process_3": 0.0012595126455307761
                        },
                        "gpu_energy": {
                            "process_1": 0.010227145959490258,
                            "process_0": 0.010227145959490258,
                            "process_2": 0.010227145959490258,
                            "process_3": 0.010249070977026875
                        },
                        "ram_energy": {
                            "process_1": 7.23548839077615e-06,
                            "process_0": 7.172303998073439e-06,
                            "process_2": 7.27646855991281e-06,
                            "process_3": 6.909538635886782e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.011532862681691921,
                            "process_0": 0.011571009959674212,
                            "process_2": 0.011514492249638943,
                            "process_3": 0.01151549316119354
                        },
                        "total_energy_joules": {
                            "process_1": 41518.30565409092,
                            "process_0": 41655.63585482716,
                            "process_2": 41452.17209870019,
                            "process_3": 41455.77538029674
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 297.21080449724985,
                        "ram_power_avg": 0.7124390602111816,
                        "cpu_energy_total": 0.005174755397116314,
                        "gpu_energy_total": 0.04093050885549765,
                        "ram_energy_total": 2.8593799584649182e-05,
                        "total_energy_kwh": 0.04613385805219861,
                        "total_energy_joules": 166081.888987915
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09865013036546198,
                        "joules_per_token": 10.136834044672547,
                        "flops_per_joule": 316943542.9090902,
                        "joules_per_flop": 3.1551360561613734e-09
                    },
                    "per-process_emissions": [
                        0.004393444038590538,
                        0.004407976244137891,
                        0.004386445822499955,
                        0.004386827119756679
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0519": {
            "setup": {
                "experiment_id": "0519",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 02:40:03 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_50_temp_1.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 1.4,
                    "decoder_top_k": 50,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.38980831898516,
                        "average_latency_ms_per_batch": 4798.726039873145,
                        "throughput_queries_per_sec": 3.334218262733532,
                        "throughput_tokens_per_sec": 426.7799376298921
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38449184768,
                        "gpu_max_memory_reserved_bytes": 38449184768
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.1,
                        "cpu_memory_usage_bytes": 1975963648
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0519",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_1": 39.918203102313555,
                            "process_0": 38.80088655989886,
                            "process_2": 39.37683684691875,
                            "process_3": 1070.7472914798682
                        },
                        "ram_power": {
                            "process_1": 0.7178964614868164,
                            "process_0": 0.6888928413391113,
                            "process_2": 0.7342300415039064,
                            "process_3": 0.7087368965148926
                        },
                        "cpu_energy": {
                            "process_1": 0.0012984812338108895,
                            "process_0": 0.001336691696185881,
                            "process_2": 0.001280069821588768,
                            "process_3": 0.0012595126455307761
                        },
                        "gpu_energy": {
                            "process_1": 0.010227145959490258,
                            "process_0": 0.010227145959490258,
                            "process_2": 0.010227145959490258,
                            "process_3": 0.010249070977026875
                        },
                        "ram_energy": {
                            "process_1": 7.23548839077615e-06,
                            "process_0": 7.172303998073439e-06,
                            "process_2": 7.27646855991281e-06,
                            "process_3": 6.909538635886782e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.011532862681691921,
                            "process_0": 0.011571009959674212,
                            "process_2": 0.011514492249638943,
                            "process_3": 0.01151549316119354
                        },
                        "total_energy_joules": {
                            "process_1": 41518.30565409092,
                            "process_0": 41655.63585482716,
                            "process_2": 41452.17209870019,
                            "process_3": 41455.77538029674
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 297.21080449724985,
                        "ram_power_avg": 0.7124390602111816,
                        "cpu_energy_total": 0.005174755397116314,
                        "gpu_energy_total": 0.04093050885549765,
                        "ram_energy_total": 2.8593799584649182e-05,
                        "total_energy_kwh": 0.04613385805219861,
                        "total_energy_joules": 166081.888987915
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09865013036546198,
                        "joules_per_token": 10.136834044672547,
                        "flops_per_joule": 316943542.9090902,
                        "joules_per_flop": 3.1551360561613734e-09
                    },
                    "per-process_emissions": [
                        0.004393444038590538,
                        0.004407976244137891,
                        0.004386445822499955,
                        0.004386827119756679
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0520": {
            "setup": {
                "experiment_id": "0520",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 02:41:37 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_100_temp_1.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 1.4,
                    "decoder_top_k": 100,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.400404317013454,
                        "average_latency_ms_per_batch": 4800.050539626682,
                        "throughput_queries_per_sec": 3.333298236739895,
                        "throughput_tokens_per_sec": 426.66217430270655
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38145097728,
                        "gpu_max_memory_reserved_bytes": 38145097728
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            10.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.6,
                        "cpu_memory_usage_bytes": 1976008704
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0520",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_1": 39.68067081274167,
                            "process_0": 38.86398161500949,
                            "process_2": 4.048282755013721,
                            "process_3": 971.2235712748025
                        },
                        "ram_power": {
                            "process_1": 0.709134578704834,
                            "process_0": 0.6889486312866211,
                            "process_2": 0.732107162475586,
                            "process_3": 0.7247486114501954
                        },
                        "cpu_energy": {
                            "process_1": 0.0011793859815625181,
                            "process_0": 0.0011241025162871665,
                            "process_2": 0.001289025396343277,
                            "process_3": 0.001116243658932945
                        },
                        "gpu_energy": {
                            "process_1": 0.010229764572690314,
                            "process_0": 0.010229764572690314,
                            "process_2": 0.01022802096018438,
                            "process_3": 0.010246680419557563
                        },
                        "ram_energy": {
                            "process_1": 7.5720324470207e-06,
                            "process_0": 7.335273041991252e-06,
                            "process_2": 7.303386213084645e-06,
                            "process_3": 7.318108636502088e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.011416722586699856,
                            "process_0": 0.01136120236201947,
                            "process_2": 0.011524349742740737,
                            "process_3": 0.011370242187127011
                        },
                        "total_energy_joules": {
                            "process_1": 41100.20131211948,
                            "process_0": 40900.328503270095,
                            "process_2": 41487.65907386665,
                            "process_3": 40932.87187365724
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 263.45412661439184,
                        "ram_power_avg": 0.7137347459793091,
                        "cpu_energy_total": 0.004708757553125906,
                        "gpu_energy_total": 0.04093423052512257,
                        "ram_energy_total": 2.9528800338598684e-05,
                        "total_energy_kwh": 0.045672516878587074,
                        "total_energy_joules": 164421.06076291346
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09964660198625569,
                        "joules_per_token": 10.035465134455167,
                        "flops_per_joule": 320145011.013924,
                        "joules_per_flop": 3.1235845182560323e-09
                    },
                    "per-process_emissions": [
                        0.004349200469403311,
                        0.004328050039811317,
                        0.004390201034497084,
                        0.004331493761186035
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0520": {
            "setup": {
                "experiment_id": "0520",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 02:41:37 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_100_temp_1.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 1.4,
                    "decoder_top_k": 100,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.400404317013454,
                        "average_latency_ms_per_batch": 4800.050539626682,
                        "throughput_queries_per_sec": 3.333298236739895,
                        "throughput_tokens_per_sec": 426.66217430270655
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38145097728,
                        "gpu_max_memory_reserved_bytes": 38145097728
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            10.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.6,
                        "cpu_memory_usage_bytes": 1976008704
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0520",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_1": 39.68067081274167,
                            "process_0": 38.86398161500949,
                            "process_2": 4.048282755013721,
                            "process_3": 971.2235712748025
                        },
                        "ram_power": {
                            "process_1": 0.709134578704834,
                            "process_0": 0.6889486312866211,
                            "process_2": 0.732107162475586,
                            "process_3": 0.7247486114501954
                        },
                        "cpu_energy": {
                            "process_1": 0.0011793859815625181,
                            "process_0": 0.0011241025162871665,
                            "process_2": 0.001289025396343277,
                            "process_3": 0.001116243658932945
                        },
                        "gpu_energy": {
                            "process_1": 0.010229764572690314,
                            "process_0": 0.010229764572690314,
                            "process_2": 0.01022802096018438,
                            "process_3": 0.010246680419557563
                        },
                        "ram_energy": {
                            "process_1": 7.5720324470207e-06,
                            "process_0": 7.335273041991252e-06,
                            "process_2": 7.303386213084645e-06,
                            "process_3": 7.318108636502088e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.011416722586699856,
                            "process_0": 0.01136120236201947,
                            "process_2": 0.011524349742740737,
                            "process_3": 0.011370242187127011
                        },
                        "total_energy_joules": {
                            "process_1": 41100.20131211948,
                            "process_0": 40900.328503270095,
                            "process_2": 41487.65907386665,
                            "process_3": 40932.87187365724
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 263.45412661439184,
                        "ram_power_avg": 0.7137347459793091,
                        "cpu_energy_total": 0.004708757553125906,
                        "gpu_energy_total": 0.04093423052512257,
                        "ram_energy_total": 2.9528800338598684e-05,
                        "total_energy_kwh": 0.045672516878587074,
                        "total_energy_joules": 164421.06076291346
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09964660198625569,
                        "joules_per_token": 10.035465134455167,
                        "flops_per_joule": 320145011.013924,
                        "joules_per_flop": 3.1235845182560323e-09
                    },
                    "per-process_emissions": [
                        0.004349200469403311,
                        0.004328050039811317,
                        0.004390201034497084,
                        0.004331493761186035
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0521": {
            "setup": {
                "experiment_id": "0521",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 02:43:15 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_burst_0.2_0.4_2.0_20",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.2,
                    "delay_max": 0.4,
                    "simulate_burst": true,
                    "burst_interval": 2.0,
                    "burst_size": 20
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 41.86948939494323,
                        "average_latency_ms_per_batch": 5233.686174367904,
                        "throughput_queries_per_sec": 3.057118724152847,
                        "throughput_tokens_per_sec": 391.3111966915644
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38352715776,
                        "gpu_max_memory_reserved_bytes": 38352715776
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.6,
                        "cpu_memory_usage_bytes": 1970642944
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0521",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_2": 74.98778401551111,
                            "process_0": 1423.847703625635,
                            "process_1": 54.96189623375469,
                            "process_3": 61.95387998255332
                        },
                        "ram_power": {
                            "process_2": 0.723428249359131,
                            "process_0": 0.6860747337341309,
                            "process_1": 0.7066669464111328,
                            "process_3": 0.7066926956176759
                        },
                        "cpu_energy": {
                            "process_2": 0.0013200345098430262,
                            "process_0": 0.0012749568987819662,
                            "process_1": 0.001262658686717259,
                            "process_3": 0.0012810055753398047
                        },
                        "gpu_energy": {
                            "process_2": 0.010471652266202724,
                            "process_0": 0.010445765578827704,
                            "process_1": 0.010462170314170294,
                            "process_3": 0.010475337546928642
                        },
                        "ram_energy": {
                            "process_2": 7.421379425756329e-06,
                            "process_0": 6.809582156469257e-06,
                            "process_1": 7.247324271694747e-06,
                            "process_3": 7.3734365340852295e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.011799108155471505,
                            "process_0": 0.01172753205976614,
                            "process_1": 0.011732076325159243,
                            "process_3": 0.011763716558802534
                        },
                        "total_energy_joules": {
                            "process_2": 42476.78935969742,
                            "process_0": 42219.11541515811,
                            "process_1": 42235.47477057327,
                            "process_3": 42349.379611689124
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 403.93781596436355,
                        "ram_power_avg": 0.7057156562805176,
                        "cpu_energy_total": 0.005138655670682056,
                        "gpu_energy_total": 0.041854925706129364,
                        "ram_energy_total": 2.8851722388005563e-05,
                        "total_energy_kwh": 0.04702243309919942,
                        "total_energy_joules": 169280.75915711792
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09678595536538911,
                        "joules_per_token": 10.33207758527331,
                        "flops_per_joule": 310954313.8331953,
                        "joules_per_flop": 3.215906503025484e-09
                    },
                    "per-process_emissions": [
                        0.00449487025182687,
                        0.004467603338167912,
                        0.004469334476069413,
                        0.004481387823075825
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0521": {
            "setup": {
                "experiment_id": "0521",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 02:43:15 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_burst_0.2_0.4_2.0_20",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.2,
                    "delay_max": 0.4,
                    "simulate_burst": true,
                    "burst_interval": 2.0,
                    "burst_size": 20
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 41.86948939494323,
                        "average_latency_ms_per_batch": 5233.686174367904,
                        "throughput_queries_per_sec": 3.057118724152847,
                        "throughput_tokens_per_sec": 391.3111966915644
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38352715776,
                        "gpu_max_memory_reserved_bytes": 38352715776
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.6,
                        "cpu_memory_usage_bytes": 1970642944
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0521",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_2": 74.98778401551111,
                            "process_0": 1423.847703625635,
                            "process_1": 54.96189623375469,
                            "process_3": 61.95387998255332
                        },
                        "ram_power": {
                            "process_2": 0.723428249359131,
                            "process_0": 0.6860747337341309,
                            "process_1": 0.7066669464111328,
                            "process_3": 0.7066926956176759
                        },
                        "cpu_energy": {
                            "process_2": 0.0013200345098430262,
                            "process_0": 0.0012749568987819662,
                            "process_1": 0.001262658686717259,
                            "process_3": 0.0012810055753398047
                        },
                        "gpu_energy": {
                            "process_2": 0.010471652266202724,
                            "process_0": 0.010445765578827704,
                            "process_1": 0.010462170314170294,
                            "process_3": 0.010475337546928642
                        },
                        "ram_energy": {
                            "process_2": 7.421379425756329e-06,
                            "process_0": 6.809582156469257e-06,
                            "process_1": 7.247324271694747e-06,
                            "process_3": 7.3734365340852295e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.011799108155471505,
                            "process_0": 0.01172753205976614,
                            "process_1": 0.011732076325159243,
                            "process_3": 0.011763716558802534
                        },
                        "total_energy_joules": {
                            "process_2": 42476.78935969742,
                            "process_0": 42219.11541515811,
                            "process_1": 42235.47477057327,
                            "process_3": 42349.379611689124
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 403.93781596436355,
                        "ram_power_avg": 0.7057156562805176,
                        "cpu_energy_total": 0.005138655670682056,
                        "gpu_energy_total": 0.041854925706129364,
                        "ram_energy_total": 2.8851722388005563e-05,
                        "total_energy_kwh": 0.04702243309919942,
                        "total_energy_joules": 169280.75915711792
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09678595536538911,
                        "joules_per_token": 10.33207758527331,
                        "flops_per_joule": 310954313.8331953,
                        "joules_per_flop": 3.215906503025484e-09
                    },
                    "per-process_emissions": [
                        0.00449487025182687,
                        0.004467603338167912,
                        0.004469334476069413,
                        0.004481387823075825
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0522": {
            "setup": {
                "experiment_id": "0522",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 02:44:46 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_topp_0.3_temp_0.6",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_p",
                    "decoder_temperature": 0.6,
                    "decoder_top_k": null,
                    "decoder_top_p": 0.3
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.98488544096472,
                        "average_latency_ms_per_batch": 4873.11068012059,
                        "throughput_queries_per_sec": 3.283323743347456,
                        "throughput_tokens_per_sec": 420.26543914847434
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38403047424,
                        "gpu_max_memory_reserved_bytes": 38403047424
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.1,
                        "cpu_memory_usage_bytes": 1982148608
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0522",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_1": 755.5692636594393,
                            "process_0": 0.0,
                            "process_3": 452.65213693235495,
                            "process_2": 1445.5368399922845
                        },
                        "ram_power": {
                            "process_1": 0.7262520790100098,
                            "process_0": 0.6910886764526367,
                            "process_3": 0.7096452713012695,
                            "process_2": 0.7109813690185547
                        },
                        "cpu_energy": {
                            "process_1": 0.0012327495476256445,
                            "process_0": 0.0011266321541133946,
                            "process_3": 0.0011629515184140471,
                            "process_2": 0.0012807281557415994
                        },
                        "gpu_energy": {
                            "process_1": 0.010375341078043832,
                            "process_0": 0.010375563855999914,
                            "process_3": 0.01039435220436502,
                            "process_2": 0.01033685104725457
                        },
                        "ram_energy": {
                            "process_1": 7.403756884969334e-06,
                            "process_0": 7.0482893236568e-06,
                            "process_3": 7.4634447032574095e-06,
                            "process_2": 7.091247759015206e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.011615494382554443,
                            "process_0": 0.011509244299436967,
                            "process_3": 0.01156476716748232,
                            "process_2": 0.011624670450755182
                        },
                        "total_energy_joules": {
                            "process_1": 41815.779777195996,
                            "process_0": 41433.27947797308,
                            "process_3": 41633.16180293635,
                            "process_2": 41848.813622718655
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 663.4395601460196,
                        "ram_power_avg": 0.7094918489456177,
                        "cpu_energy_total": 0.004803061375894686,
                        "gpu_energy_total": 0.041482108185663336,
                        "ram_energy_total": 2.900673867089875e-05,
                        "total_energy_kwh": 0.04631417630022891,
                        "total_energy_joules": 166731.0346808241
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.0982660488574557,
                        "joules_per_token": 10.176454753468267,
                        "flops_per_joule": 315709564.2669698,
                        "joules_per_flop": 3.1674681833660942e-09
                    },
                    "per-process_emissions": [
                        0.004424922585034115,
                        0.004384446615870513,
                        0.00440559805245239,
                        0.004428418208215186
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0522": {
            "setup": {
                "experiment_id": "0522",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 02:44:46 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_topp_0.3_temp_0.6",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_p",
                    "decoder_temperature": 0.6,
                    "decoder_top_k": null,
                    "decoder_top_p": 0.3
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.98488544096472,
                        "average_latency_ms_per_batch": 4873.11068012059,
                        "throughput_queries_per_sec": 3.283323743347456,
                        "throughput_tokens_per_sec": 420.26543914847434
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38403047424,
                        "gpu_max_memory_reserved_bytes": 38403047424
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.1,
                        "cpu_memory_usage_bytes": 1982148608
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0522",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_1": 755.5692636594393,
                            "process_0": 0.0,
                            "process_3": 452.65213693235495,
                            "process_2": 1445.5368399922845
                        },
                        "ram_power": {
                            "process_1": 0.7262520790100098,
                            "process_0": 0.6910886764526367,
                            "process_3": 0.7096452713012695,
                            "process_2": 0.7109813690185547
                        },
                        "cpu_energy": {
                            "process_1": 0.0012327495476256445,
                            "process_0": 0.0011266321541133946,
                            "process_3": 0.0011629515184140471,
                            "process_2": 0.0012807281557415994
                        },
                        "gpu_energy": {
                            "process_1": 0.010375341078043832,
                            "process_0": 0.010375563855999914,
                            "process_3": 0.01039435220436502,
                            "process_2": 0.01033685104725457
                        },
                        "ram_energy": {
                            "process_1": 7.403756884969334e-06,
                            "process_0": 7.0482893236568e-06,
                            "process_3": 7.4634447032574095e-06,
                            "process_2": 7.091247759015206e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.011615494382554443,
                            "process_0": 0.011509244299436967,
                            "process_3": 0.01156476716748232,
                            "process_2": 0.011624670450755182
                        },
                        "total_energy_joules": {
                            "process_1": 41815.779777195996,
                            "process_0": 41433.27947797308,
                            "process_3": 41633.16180293635,
                            "process_2": 41848.813622718655
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 663.4395601460196,
                        "ram_power_avg": 0.7094918489456177,
                        "cpu_energy_total": 0.004803061375894686,
                        "gpu_energy_total": 0.041482108185663336,
                        "ram_energy_total": 2.900673867089875e-05,
                        "total_energy_kwh": 0.04631417630022891,
                        "total_energy_joules": 166731.0346808241
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.0982660488574557,
                        "joules_per_token": 10.176454753468267,
                        "flops_per_joule": 315709564.2669698,
                        "joules_per_flop": 3.1674681833660942e-09
                    },
                    "per-process_emissions": [
                        0.004424922585034115,
                        0.004384446615870513,
                        0.00440559805245239,
                        0.004428418208215186
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0523": {
            "setup": {
                "experiment_id": "0523",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 02:46:37 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_10",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 10,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 53.30141020793235,
                        "average_latency_ms_per_batch": 4100.108477533257,
                        "throughput_queries_per_sec": 2.4014374010117834,
                        "throughput_tokens_per_sec": 307.3839873295083
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 37415288832,
                        "gpu_max_memory_reserved_bytes": 37415288832
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            27.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.8,
                        "cpu_memory_usage_bytes": 1970716672
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0523",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_1": 171.50256921320525,
                            "process_0": 186.38282837206833,
                            "process_2": 1473.1540987326584,
                            "process_3": 0.0
                        },
                        "ram_power": {
                            "process_1": 0.7305021286010742,
                            "process_0": 0.6870746612548828,
                            "process_2": 0.7285494804382325,
                            "process_3": 0.7305707931518555
                        },
                        "cpu_energy": {
                            "process_1": 0.0016351169896206559,
                            "process_0": 0.0016675205045066836,
                            "process_2": 0.0015958132032446884,
                            "process_3": 0.0017104245488226298
                        },
                        "gpu_energy": {
                            "process_1": 0.014003652869575944,
                            "process_0": 0.013998096198465149,
                            "process_2": 0.013918386134697869,
                            "process_3": 0.014030269001981566
                        },
                        "ram_energy": {
                            "process_1": 9.88095019828207e-06,
                            "process_0": 9.46891353025585e-06,
                            "process_2": 9.76160636520885e-06,
                            "process_3": 9.678558514889051e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.015648650809394885,
                            "process_0": 0.015675085616502084,
                            "process_2": 0.015523960944307762,
                            "process_3": 0.01575037210931908
                        },
                        "total_energy_joules": {
                            "process_1": 56335.14291382158,
                            "process_0": 56430.3082194075,
                            "process_2": 55886.259399507944,
                            "process_3": 56701.33959354869
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 457.759874079483,
                        "ram_power_avg": 0.7191742658615112,
                        "cpu_energy_total": 0.006608875246194658,
                        "gpu_energy_total": 0.05595040420472053,
                        "ram_energy_total": 3.879002860863582e-05,
                        "total_energy_kwh": 0.06259806947952382,
                        "total_energy_joules": 225353.0501262857
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.07270369755731534,
                        "joules_per_token": 13.75445862587193,
                        "flops_per_joule": 233582737.30648795,
                        "joules_per_flop": 4.2811382875776595e-09
                    },
                    "per-process_emissions": [
                        0.005961353525838982,
                        0.005971423865606469,
                        0.005913852921734042,
                        0.006000104255045104
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0523": {
            "setup": {
                "experiment_id": "0523",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 02:46:37 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_10",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 10,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 53.30141020793235,
                        "average_latency_ms_per_batch": 4100.108477533257,
                        "throughput_queries_per_sec": 2.4014374010117834,
                        "throughput_tokens_per_sec": 307.3839873295083
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 37415288832,
                        "gpu_max_memory_reserved_bytes": 37415288832
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            27.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.8,
                        "cpu_memory_usage_bytes": 1970716672
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0523",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_1": 171.50256921320525,
                            "process_0": 186.38282837206833,
                            "process_2": 1473.1540987326584,
                            "process_3": 0.0
                        },
                        "ram_power": {
                            "process_1": 0.7305021286010742,
                            "process_0": 0.6870746612548828,
                            "process_2": 0.7285494804382325,
                            "process_3": 0.7305707931518555
                        },
                        "cpu_energy": {
                            "process_1": 0.0016351169896206559,
                            "process_0": 0.0016675205045066836,
                            "process_2": 0.0015958132032446884,
                            "process_3": 0.0017104245488226298
                        },
                        "gpu_energy": {
                            "process_1": 0.014003652869575944,
                            "process_0": 0.013998096198465149,
                            "process_2": 0.013918386134697869,
                            "process_3": 0.014030269001981566
                        },
                        "ram_energy": {
                            "process_1": 9.88095019828207e-06,
                            "process_0": 9.46891353025585e-06,
                            "process_2": 9.76160636520885e-06,
                            "process_3": 9.678558514889051e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.015648650809394885,
                            "process_0": 0.015675085616502084,
                            "process_2": 0.015523960944307762,
                            "process_3": 0.01575037210931908
                        },
                        "total_energy_joules": {
                            "process_1": 56335.14291382158,
                            "process_0": 56430.3082194075,
                            "process_2": 55886.259399507944,
                            "process_3": 56701.33959354869
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 457.759874079483,
                        "ram_power_avg": 0.7191742658615112,
                        "cpu_energy_total": 0.006608875246194658,
                        "gpu_energy_total": 0.05595040420472053,
                        "ram_energy_total": 3.879002860863582e-05,
                        "total_energy_kwh": 0.06259806947952382,
                        "total_energy_joules": 225353.0501262857
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.07270369755731534,
                        "joules_per_token": 13.75445862587193,
                        "flops_per_joule": 233582737.30648795,
                        "joules_per_flop": 4.2811382875776595e-09
                    },
                    "per-process_emissions": [
                        0.005961353525838982,
                        0.005971423865606469,
                        0.005913852921734042,
                        0.006000104255045104
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0524": {
            "setup": {
                "experiment_id": "0524",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 02:48:10 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_topp_0.5_temp_0.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_p",
                    "decoder_temperature": 0.2,
                    "decoder_top_k": null,
                    "decoder_top_p": 0.5
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.957535585039295,
                        "average_latency_ms_per_batch": 4869.691948129912,
                        "throughput_queries_per_sec": 3.285628777020365,
                        "throughput_tokens_per_sec": 420.5604834586067
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38403047424,
                        "gpu_max_memory_reserved_bytes": 38403047424
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.1,
                        "cpu_memory_usage_bytes": 1960075264
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0524",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_1": 1048.7557154452024,
                            "process_2": 0.0,
                            "process_0": 0.0,
                            "process_3": 540.4381021688247
                        },
                        "ram_power": {
                            "process_1": 0.726222038269043,
                            "process_2": 0.7110228538513184,
                            "process_0": 0.6833868026733398,
                            "process_3": 0.727778434753418
                        },
                        "cpu_energy": {
                            "process_1": 0.0011863566898427966,
                            "process_2": 0.0011028107713482312,
                            "process_0": 0.0011018556644357882,
                            "process_3": 0.001266118599227411
                        },
                        "gpu_energy": {
                            "process_1": 0.010401356932190353,
                            "process_2": 0.01039897054138983,
                            "process_0": 0.010397471095744848,
                            "process_3": 0.010410047772477782
                        },
                        "ram_energy": {
                            "process_1": 7.52168300047546e-06,
                            "process_2": 7.401157951722442e-06,
                            "process_0": 7.1339990661598084e-06,
                            "process_3": 7.5455120985544945e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.011595235305033624,
                            "process_2": 0.011509182470689782,
                            "process_0": 0.011506460759246792,
                            "process_3": 0.011683711883803743
                        },
                        "total_energy_joules": {
                            "process_1": 41742.84709812105,
                            "process_2": 41433.05689448321,
                            "process_0": 41423.25873328845,
                            "process_3": 42061.362781693475
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 397.29845440350675,
                        "ram_power_avg": 0.7121025323867798,
                        "cpu_energy_total": 0.004657141724854227,
                        "gpu_energy_total": 0.04160784634180281,
                        "ram_energy_total": 2.9602352116912205e-05,
                        "total_energy_kwh": 0.04629459041877394,
                        "total_energy_joules": 166660.5255075862
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09830762233648555,
                        "joules_per_token": 10.172151215062634,
                        "flops_per_joule": 315843131.71068186,
                        "joules_per_flop": 3.166128687313101e-09
                    },
                    "per-process_emissions": [
                        0.0044172048894525594,
                        0.004384423062209273,
                        0.004383386226235066,
                        0.004450910042135036
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0524": {
            "setup": {
                "experiment_id": "0524",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 02:48:10 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_topp_0.5_temp_0.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_p",
                    "decoder_temperature": 0.2,
                    "decoder_top_k": null,
                    "decoder_top_p": 0.5
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.957535585039295,
                        "average_latency_ms_per_batch": 4869.691948129912,
                        "throughput_queries_per_sec": 3.285628777020365,
                        "throughput_tokens_per_sec": 420.5604834586067
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38403047424,
                        "gpu_max_memory_reserved_bytes": 38403047424
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.1,
                        "cpu_memory_usage_bytes": 1960075264
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0524",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_1": 1048.7557154452024,
                            "process_2": 0.0,
                            "process_0": 0.0,
                            "process_3": 540.4381021688247
                        },
                        "ram_power": {
                            "process_1": 0.726222038269043,
                            "process_2": 0.7110228538513184,
                            "process_0": 0.6833868026733398,
                            "process_3": 0.727778434753418
                        },
                        "cpu_energy": {
                            "process_1": 0.0011863566898427966,
                            "process_2": 0.0011028107713482312,
                            "process_0": 0.0011018556644357882,
                            "process_3": 0.001266118599227411
                        },
                        "gpu_energy": {
                            "process_1": 0.010401356932190353,
                            "process_2": 0.01039897054138983,
                            "process_0": 0.010397471095744848,
                            "process_3": 0.010410047772477782
                        },
                        "ram_energy": {
                            "process_1": 7.52168300047546e-06,
                            "process_2": 7.401157951722442e-06,
                            "process_0": 7.1339990661598084e-06,
                            "process_3": 7.5455120985544945e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.011595235305033624,
                            "process_2": 0.011509182470689782,
                            "process_0": 0.011506460759246792,
                            "process_3": 0.011683711883803743
                        },
                        "total_energy_joules": {
                            "process_1": 41742.84709812105,
                            "process_2": 41433.05689448321,
                            "process_0": 41423.25873328845,
                            "process_3": 42061.362781693475
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 397.29845440350675,
                        "ram_power_avg": 0.7121025323867798,
                        "cpu_energy_total": 0.004657141724854227,
                        "gpu_energy_total": 0.04160784634180281,
                        "ram_energy_total": 2.9602352116912205e-05,
                        "total_energy_kwh": 0.04629459041877394,
                        "total_energy_joules": 166660.5255075862
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09830762233648555,
                        "joules_per_token": 10.172151215062634,
                        "flops_per_joule": 315843131.71068186,
                        "joules_per_flop": 3.166128687313101e-09
                    },
                    "per-process_emissions": [
                        0.0044172048894525594,
                        0.004384423062209273,
                        0.004383386226235066,
                        0.004450910042135036
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0525": {
            "setup": {
                "experiment_id": "0525",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 02:49:45 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_400_temp_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 400,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.54815987503389,
                        "average_latency_ms_per_batch": 4818.519984379236,
                        "throughput_queries_per_sec": 3.3205216647163622,
                        "throughput_tokens_per_sec": 425.02677308369437
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38451281920,
                        "gpu_max_memory_reserved_bytes": 38451281920
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.5,
                        "cpu_memory_usage_bytes": 1979629568
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0525",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_3": 211.20850421495226,
                            "process_0": 197.01946351006265,
                            "process_2": 222.9255529973207,
                            "process_1": 207.09862925828367
                        },
                        "ram_power": {
                            "process_3": 0.7090787887573242,
                            "process_0": 0.6893520355224609,
                            "process_2": 0.7091445922851562,
                            "process_1": 0.7091331481933594
                        },
                        "cpu_energy": {
                            "process_3": 0.001095801887306152,
                            "process_0": 0.0011416188223447536,
                            "process_2": 0.0010990231560372194,
                            "process_1": 0.0010926327434481208
                        },
                        "gpu_energy": {
                            "process_3": 0.010251240978766063,
                            "process_0": 0.010251240978766063,
                            "process_2": 0.010247665975905207,
                            "process_1": 0.010247665975905207
                        },
                        "ram_energy": {
                            "process_3": 7.025062212840149e-06,
                            "process_0": 7.418275165147374e-06,
                            "process_2": 6.656727269909338e-06,
                            "process_1": 6.22394326652069e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.011354067928285057,
                            "process_0": 0.011400278076275965,
                            "process_2": 0.011353345859212336,
                            "process_1": 0.011346522662619853
                        },
                        "total_energy_joules": {
                            "process_3": 40874.64454182621,
                            "process_0": 41041.00107459347,
                            "process_2": 40872.04509316441,
                            "process_1": 40847.48158543147
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 209.5630374951548,
                        "ram_power_avg": 0.7041771411895752,
                        "cpu_energy_total": 0.0044290766091362455,
                        "gpu_energy_total": 0.04099781390934254,
                        "ram_energy_total": 2.7324007914417553e-05,
                        "total_energy_kwh": 0.045454214526393207,
                        "total_energy_joules": 163635.17229501554
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.10012517339769422,
                        "joules_per_token": 9.987498309021944,
                        "flops_per_joule": 321682567.20482224,
                        "joules_per_flop": 3.1086546239954572e-09
                    },
                    "per-process_emissions": [
                        0.004325332177280193,
                        0.004342935933157329,
                        0.00432505710506694,
                        0.004322457808325033
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0525": {
            "setup": {
                "experiment_id": "0525",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 02:49:45 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_400_temp_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 400,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.54815987503389,
                        "average_latency_ms_per_batch": 4818.519984379236,
                        "throughput_queries_per_sec": 3.3205216647163622,
                        "throughput_tokens_per_sec": 425.02677308369437
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38451281920,
                        "gpu_max_memory_reserved_bytes": 38451281920
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.5,
                        "cpu_memory_usage_bytes": 1979629568
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0525",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_3": 211.20850421495226,
                            "process_0": 197.01946351006265,
                            "process_2": 222.9255529973207,
                            "process_1": 207.09862925828367
                        },
                        "ram_power": {
                            "process_3": 0.7090787887573242,
                            "process_0": 0.6893520355224609,
                            "process_2": 0.7091445922851562,
                            "process_1": 0.7091331481933594
                        },
                        "cpu_energy": {
                            "process_3": 0.001095801887306152,
                            "process_0": 0.0011416188223447536,
                            "process_2": 0.0010990231560372194,
                            "process_1": 0.0010926327434481208
                        },
                        "gpu_energy": {
                            "process_3": 0.010251240978766063,
                            "process_0": 0.010251240978766063,
                            "process_2": 0.010247665975905207,
                            "process_1": 0.010247665975905207
                        },
                        "ram_energy": {
                            "process_3": 7.025062212840149e-06,
                            "process_0": 7.418275165147374e-06,
                            "process_2": 6.656727269909338e-06,
                            "process_1": 6.22394326652069e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.011354067928285057,
                            "process_0": 0.011400278076275965,
                            "process_2": 0.011353345859212336,
                            "process_1": 0.011346522662619853
                        },
                        "total_energy_joules": {
                            "process_3": 40874.64454182621,
                            "process_0": 41041.00107459347,
                            "process_2": 40872.04509316441,
                            "process_1": 40847.48158543147
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 209.5630374951548,
                        "ram_power_avg": 0.7041771411895752,
                        "cpu_energy_total": 0.0044290766091362455,
                        "gpu_energy_total": 0.04099781390934254,
                        "ram_energy_total": 2.7324007914417553e-05,
                        "total_energy_kwh": 0.045454214526393207,
                        "total_energy_joules": 163635.17229501554
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.10012517339769422,
                        "joules_per_token": 9.987498309021944,
                        "flops_per_joule": 321682567.20482224,
                        "joules_per_flop": 3.1086546239954572e-09
                    },
                    "per-process_emissions": [
                        0.004325332177280193,
                        0.004342935933157329,
                        0.00432505710506694,
                        0.004322457808325033
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0526": {
            "setup": {
                "experiment_id": "0526",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 02:51:16 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_1_temp_0.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 0.0,
                    "decoder_top_k": 1,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.994967542996164,
                        "average_latency_ms_per_batch": 4749.370942874521,
                        "throughput_queries_per_sec": 3.3688672020880563,
                        "throughput_tokens_per_sec": 431.2150018672712
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 32558284800,
                        "gpu_max_memory_reserved_bytes": 32558284800
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.5,
                        "cpu_memory_usage_bytes": 1969246208
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0526",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_1": 52.127991407288754,
                            "process_3": 62.19316521452224,
                            "process_0": 54.369799932797406,
                            "process_2": 79.00204277639637
                        },
                        "ram_power": {
                            "process_1": 0.7046012878417969,
                            "process_3": 0.7045812606811523,
                            "process_0": 0.6855926513671875,
                            "process_2": 0.7216372489929199
                        },
                        "cpu_energy": {
                            "process_1": 0.001177287868151325,
                            "process_3": 0.0011873869881255815,
                            "process_0": 0.0011771013128727644,
                            "process_2": 0.0011159272684344616
                        },
                        "gpu_energy": {
                            "process_1": 0.01019234204275854,
                            "process_3": 0.010199257326068079,
                            "process_0": 0.01019234204275854,
                            "process_2": 0.010198441214301468
                        },
                        "ram_energy": {
                            "process_1": 6.3578182182294165e-06,
                            "process_3": 6.418671487645785e-06,
                            "process_0": 6.155589616632877e-06,
                            "process_2": 6.891053440984273e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.011375987729128097,
                            "process_3": 0.011393062985681305,
                            "process_0": 0.011375598945247938,
                            "process_2": 0.011321259536176914
                        },
                        "total_energy_joules": {
                            "process_1": 40953.55582486115,
                            "process_3": 41015.0267484527,
                            "process_0": 40952.15620289258,
                            "process_2": 40756.53433023689
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 61.92324983275119,
                        "ram_power_avg": 0.7041031122207642,
                        "cpu_energy_total": 0.004657703437584133,
                        "gpu_energy_total": 0.040782382625886626,
                        "ram_energy_total": 2.5823132763492352e-05,
                        "total_energy_kwh": 0.04546590919623426,
                        "total_energy_joules": 163677.27310644332
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.1000994193576593,
                        "joules_per_token": 9.99006793862569,
                        "flops_per_joule": 321599824.5195095,
                        "joules_per_flop": 3.1094544329869065e-09
                    },
                    "per-process_emissions": [
                        0.004333682525411348,
                        0.004340187344395293,
                        0.004333534418192202,
                        0.0043128338203065955
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0526": {
            "setup": {
                "experiment_id": "0526",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 02:51:16 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_1_temp_0.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 0.0,
                    "decoder_top_k": 1,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.994967542996164,
                        "average_latency_ms_per_batch": 4749.370942874521,
                        "throughput_queries_per_sec": 3.3688672020880563,
                        "throughput_tokens_per_sec": 431.2150018672712
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 32558284800,
                        "gpu_max_memory_reserved_bytes": 32558284800
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.5,
                        "cpu_memory_usage_bytes": 1969246208
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0526",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_1": 52.127991407288754,
                            "process_3": 62.19316521452224,
                            "process_0": 54.369799932797406,
                            "process_2": 79.00204277639637
                        },
                        "ram_power": {
                            "process_1": 0.7046012878417969,
                            "process_3": 0.7045812606811523,
                            "process_0": 0.6855926513671875,
                            "process_2": 0.7216372489929199
                        },
                        "cpu_energy": {
                            "process_1": 0.001177287868151325,
                            "process_3": 0.0011873869881255815,
                            "process_0": 0.0011771013128727644,
                            "process_2": 0.0011159272684344616
                        },
                        "gpu_energy": {
                            "process_1": 0.01019234204275854,
                            "process_3": 0.010199257326068079,
                            "process_0": 0.01019234204275854,
                            "process_2": 0.010198441214301468
                        },
                        "ram_energy": {
                            "process_1": 6.3578182182294165e-06,
                            "process_3": 6.418671487645785e-06,
                            "process_0": 6.155589616632877e-06,
                            "process_2": 6.891053440984273e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.011375987729128097,
                            "process_3": 0.011393062985681305,
                            "process_0": 0.011375598945247938,
                            "process_2": 0.011321259536176914
                        },
                        "total_energy_joules": {
                            "process_1": 40953.55582486115,
                            "process_3": 41015.0267484527,
                            "process_0": 40952.15620289258,
                            "process_2": 40756.53433023689
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 61.92324983275119,
                        "ram_power_avg": 0.7041031122207642,
                        "cpu_energy_total": 0.004657703437584133,
                        "gpu_energy_total": 0.040782382625886626,
                        "ram_energy_total": 2.5823132763492352e-05,
                        "total_energy_kwh": 0.04546590919623426,
                        "total_energy_joules": 163677.27310644332
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.1000994193576593,
                        "joules_per_token": 9.99006793862569,
                        "flops_per_joule": 321599824.5195095,
                        "joules_per_flop": 3.1094544329869065e-09
                    },
                    "per-process_emissions": [
                        0.004333682525411348,
                        0.004340187344395293,
                        0.004333534418192202,
                        0.0043128338203065955
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0527": {
            "setup": {
                "experiment_id": "0527",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 02:52:49 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_20_temp_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 0.8,
                    "decoder_top_k": 20,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.29515538609121,
                        "average_latency_ms_per_batch": 4786.894423261401,
                        "throughput_queries_per_sec": 3.342459345301144,
                        "throughput_tokens_per_sec": 427.8347961985464
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38145097728,
                        "gpu_max_memory_reserved_bytes": 38145097728
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.5,
                        "cpu_memory_usage_bytes": 1957441536
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0527",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_2": 14.107236271696477,
                            "process_0": 213.0384915412664,
                            "process_3": 4772.997539799029,
                            "process_1": 338.433339593703
                        },
                        "ram_power": {
                            "process_2": 0.7313976287841797,
                            "process_0": 0.6824154853820802,
                            "process_3": 0.7326078414916992,
                            "process_1": 0.7327365875244141
                        },
                        "cpu_energy": {
                            "process_2": 0.0012205892963738736,
                            "process_0": 0.0012678003737783003,
                            "process_3": 0.0011072021729032714,
                            "process_1": 0.001131149635912152
                        },
                        "gpu_energy": {
                            "process_2": 0.010250874034028357,
                            "process_0": 0.010251446534486064,
                            "process_3": 0.010250874034028357,
                            "process_1": 0.010243542639273073
                        },
                        "ram_energy": {
                            "process_2": 7.673323799461233e-06,
                            "process_0": 6.7155470994111474e-06,
                            "process_3": 7.312395313370652e-06,
                            "process_1": 7.109971947752844e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.011479136654201697,
                            "process_0": 0.011525962455363775,
                            "process_3": 0.011365388602245,
                            "process_1": 0.011381802247132977
                        },
                        "total_energy_joules": {
                            "process_2": 41324.89195512611,
                            "process_0": 41493.46483930959,
                            "process_3": 40915.398968081994,
                            "process_1": 40974.488089678714
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1334.6441518014237,
                        "ram_power_avg": 0.7197893857955933,
                        "cpu_energy_total": 0.004726741478967598,
                        "gpu_energy_total": 0.04099673724181585,
                        "ram_energy_total": 2.8811238159995877e-05,
                        "total_energy_kwh": 0.04575228995894345,
                        "total_energy_joules": 164708.2438521964
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09947285950484935,
                        "joules_per_token": 10.05299339918191,
                        "flops_per_joule": 319586810.4580125,
                        "joules_per_flop": 3.1290402709888443e-09
                    },
                    "per-process_emissions": [
                        0.0043729771084181366,
                        0.00439081539737083,
                        0.004329644788025232,
                        0.004335897566045308
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0527": {
            "setup": {
                "experiment_id": "0527",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 02:52:49 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_20_temp_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 0.8,
                    "decoder_top_k": 20,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.29515538609121,
                        "average_latency_ms_per_batch": 4786.894423261401,
                        "throughput_queries_per_sec": 3.342459345301144,
                        "throughput_tokens_per_sec": 427.8347961985464
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38145097728,
                        "gpu_max_memory_reserved_bytes": 38145097728
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.5,
                        "cpu_memory_usage_bytes": 1957441536
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0527",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_2": 14.107236271696477,
                            "process_0": 213.0384915412664,
                            "process_3": 4772.997539799029,
                            "process_1": 338.433339593703
                        },
                        "ram_power": {
                            "process_2": 0.7313976287841797,
                            "process_0": 0.6824154853820802,
                            "process_3": 0.7326078414916992,
                            "process_1": 0.7327365875244141
                        },
                        "cpu_energy": {
                            "process_2": 0.0012205892963738736,
                            "process_0": 0.0012678003737783003,
                            "process_3": 0.0011072021729032714,
                            "process_1": 0.001131149635912152
                        },
                        "gpu_energy": {
                            "process_2": 0.010250874034028357,
                            "process_0": 0.010251446534486064,
                            "process_3": 0.010250874034028357,
                            "process_1": 0.010243542639273073
                        },
                        "ram_energy": {
                            "process_2": 7.673323799461233e-06,
                            "process_0": 6.7155470994111474e-06,
                            "process_3": 7.312395313370652e-06,
                            "process_1": 7.109971947752844e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.011479136654201697,
                            "process_0": 0.011525962455363775,
                            "process_3": 0.011365388602245,
                            "process_1": 0.011381802247132977
                        },
                        "total_energy_joules": {
                            "process_2": 41324.89195512611,
                            "process_0": 41493.46483930959,
                            "process_3": 40915.398968081994,
                            "process_1": 40974.488089678714
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1334.6441518014237,
                        "ram_power_avg": 0.7197893857955933,
                        "cpu_energy_total": 0.004726741478967598,
                        "gpu_energy_total": 0.04099673724181585,
                        "ram_energy_total": 2.8811238159995877e-05,
                        "total_energy_kwh": 0.04575228995894345,
                        "total_energy_joules": 164708.2438521964
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09947285950484935,
                        "joules_per_token": 10.05299339918191,
                        "flops_per_joule": 319586810.4580125,
                        "joules_per_flop": 3.1290402709888443e-09
                    },
                    "per-process_emissions": [
                        0.0043729771084181366,
                        0.00439081539737083,
                        0.004329644788025232,
                        0.004335897566045308
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0528": {
            "setup": {
                "experiment_id": "0528",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 02:54:23 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_topp_0.9_temp_1.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_p",
                    "decoder_temperature": 1.4,
                    "decoder_top_k": null,
                    "decoder_top_p": 0.9
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 39.12960439414019,
                        "average_latency_ms_per_batch": 4891.2005492675235,
                        "throughput_queries_per_sec": 3.2711805289595133,
                        "throughput_tokens_per_sec": 418.7111077068177
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38403047424,
                        "gpu_max_memory_reserved_bytes": 38403047424
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            27.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.9,
                        "cpu_memory_usage_bytes": 1980420096
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0528",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_1": 5115.957781558799,
                            "process_3": 558.507952884512,
                            "process_0": 643.0532337831696,
                            "process_2": 2387.39323066273
                        },
                        "ram_power": {
                            "process_1": 0.7342929840087891,
                            "process_3": 0.732454776763916,
                            "process_0": 0.6904864311218262,
                            "process_2": 0.7104306221008301
                        },
                        "cpu_energy": {
                            "process_1": 0.001060794747218097,
                            "process_3": 0.0010709136451259838,
                            "process_0": 0.0010569146261514108,
                            "process_2": 0.0011089587082460637
                        },
                        "gpu_energy": {
                            "process_1": 0.010390102200961593,
                            "process_3": 0.010403716934080265,
                            "process_0": 0.010396308594817327,
                            "process_2": 0.010352640504324384
                        },
                        "ram_energy": {
                            "process_1": 5.964114116573469e-06,
                            "process_3": 6.6637803091087346e-06,
                            "process_0": 6.910350055433288e-06,
                            "process_2": 6.010463961373154e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.011456861062296259,
                            "process_3": 0.011481294359515361,
                            "process_0": 0.011460133571024168,
                            "process_2": 0.011467609676531825
                        },
                        "total_energy_joules": {
                            "process_1": 41244.69982426653,
                            "process_3": 41332.6596942553,
                            "process_0": 41256.480855687005,
                            "process_2": 41283.39483551457
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 2176.228049722303,
                        "ram_power_avg": 0.7169162034988403,
                        "cpu_energy_total": 0.004297581726741555,
                        "gpu_energy_total": 0.04154276823418357,
                        "ram_energy_total": 2.5548708442488646e-05,
                        "total_energy_kwh": 0.04586589866936762,
                        "total_energy_joules": 165117.2352097234
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09922646766214252,
                        "joules_per_token": 10.07795625059347,
                        "flops_per_joule": 318795201.736543,
                        "joules_per_flop": 3.136810072901958e-09
                    },
                    "per-process_emissions": [
                        0.00436449122168176,
                        0.004373799086257377,
                        0.004365737883881657,
                        0.004368585906274799
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0528": {
            "setup": {
                "experiment_id": "0528",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 02:54:23 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_topp_0.9_temp_1.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_p",
                    "decoder_temperature": 1.4,
                    "decoder_top_k": null,
                    "decoder_top_p": 0.9
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 39.12960439414019,
                        "average_latency_ms_per_batch": 4891.2005492675235,
                        "throughput_queries_per_sec": 3.2711805289595133,
                        "throughput_tokens_per_sec": 418.7111077068177
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38403047424,
                        "gpu_max_memory_reserved_bytes": 38403047424
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            27.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.9,
                        "cpu_memory_usage_bytes": 1980420096
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0528",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_1": 5115.957781558799,
                            "process_3": 558.507952884512,
                            "process_0": 643.0532337831696,
                            "process_2": 2387.39323066273
                        },
                        "ram_power": {
                            "process_1": 0.7342929840087891,
                            "process_3": 0.732454776763916,
                            "process_0": 0.6904864311218262,
                            "process_2": 0.7104306221008301
                        },
                        "cpu_energy": {
                            "process_1": 0.001060794747218097,
                            "process_3": 0.0010709136451259838,
                            "process_0": 0.0010569146261514108,
                            "process_2": 0.0011089587082460637
                        },
                        "gpu_energy": {
                            "process_1": 0.010390102200961593,
                            "process_3": 0.010403716934080265,
                            "process_0": 0.010396308594817327,
                            "process_2": 0.010352640504324384
                        },
                        "ram_energy": {
                            "process_1": 5.964114116573469e-06,
                            "process_3": 6.6637803091087346e-06,
                            "process_0": 6.910350055433288e-06,
                            "process_2": 6.010463961373154e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.011456861062296259,
                            "process_3": 0.011481294359515361,
                            "process_0": 0.011460133571024168,
                            "process_2": 0.011467609676531825
                        },
                        "total_energy_joules": {
                            "process_1": 41244.69982426653,
                            "process_3": 41332.6596942553,
                            "process_0": 41256.480855687005,
                            "process_2": 41283.39483551457
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 2176.228049722303,
                        "ram_power_avg": 0.7169162034988403,
                        "cpu_energy_total": 0.004297581726741555,
                        "gpu_energy_total": 0.04154276823418357,
                        "ram_energy_total": 2.5548708442488646e-05,
                        "total_energy_kwh": 0.04586589866936762,
                        "total_energy_joules": 165117.2352097234
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09922646766214252,
                        "joules_per_token": 10.07795625059347,
                        "flops_per_joule": 318795201.736543,
                        "joules_per_flop": 3.136810072901958e-09
                    },
                    "per-process_emissions": [
                        0.00436449122168176,
                        0.004373799086257377,
                        0.004365737883881657,
                        0.004368585906274799
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0529": {
            "setup": {
                "experiment_id": "0529",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 02:55:53 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_5_temp_0.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 0.0,
                    "decoder_top_k": 5,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.06951667601243,
                        "average_latency_ms_per_batch": 4758.689584501553,
                        "throughput_queries_per_sec": 3.3622701619601254,
                        "throughput_tokens_per_sec": 430.37058073089605
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 32254197760,
                        "gpu_max_memory_reserved_bytes": 32254197760
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            27.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.5,
                        "cpu_memory_usage_bytes": 1967804416
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0529",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_3": 57.34984102024586,
                            "process_2": 49.09806807998178,
                            "process_0": 49.05070834910294,
                            "process_1": 8.924614349851442
                        },
                        "ram_power": {
                            "process_3": 0.7047758102416992,
                            "process_2": 0.7045927047729492,
                            "process_0": 0.6849675178527832,
                            "process_1": 0.7048287391662598
                        },
                        "cpu_energy": {
                            "process_3": 0.0012345986377476949,
                            "process_2": 0.0012216625287073835,
                            "process_0": 0.0012474106381287129,
                            "process_1": 0.0011887976461603103
                        },
                        "gpu_energy": {
                            "process_3": 0.010194458711119125,
                            "process_2": 0.010193227043465924,
                            "process_0": 0.010190274541104216,
                            "process_1": 0.010196416212684412
                        },
                        "ram_energy": {
                            "process_3": 6.726987708403548e-06,
                            "process_2": 6.620111259345918e-06,
                            "process_0": 6.58335035926018e-06,
                            "process_1": 7.188188044938638e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.01143578433657522,
                            "process_2": 0.011421509683432658,
                            "process_0": 0.011444268529592194,
                            "process_1": 0.011392402046889662
                        },
                        "total_energy_joules": {
                            "process_3": 41168.82361167079,
                            "process_2": 41117.43486035756,
                            "process_0": 41199.366706531895,
                            "process_1": 41012.64736880278
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 41.105807949795505,
                        "ram_power_avg": 0.6997911930084229,
                        "cpu_energy_total": 0.004892469450744102,
                        "gpu_energy_total": 0.04077437650837368,
                        "ram_energy_total": 2.7118637371948284e-05,
                        "total_energy_kwh": 0.04569396459648973,
                        "total_energy_joules": 164498.27254736304
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.0995998301154357,
                        "joules_per_token": 10.040177767783389,
                        "flops_per_joule": 319994742.15577596,
                        "joules_per_flop": 3.1250513469786704e-09
                    },
                    "per-process_emissions": [
                        0.00435646204301833,
                        0.004351024113903671,
                        0.004359694096348146,
                        0.004339935559762617
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0529": {
            "setup": {
                "experiment_id": "0529",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 02:55:53 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_5_temp_0.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 0.0,
                    "decoder_top_k": 5,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.06951667601243,
                        "average_latency_ms_per_batch": 4758.689584501553,
                        "throughput_queries_per_sec": 3.3622701619601254,
                        "throughput_tokens_per_sec": 430.37058073089605
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 32254197760,
                        "gpu_max_memory_reserved_bytes": 32254197760
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            27.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.5,
                        "cpu_memory_usage_bytes": 1967804416
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0529",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_3": 57.34984102024586,
                            "process_2": 49.09806807998178,
                            "process_0": 49.05070834910294,
                            "process_1": 8.924614349851442
                        },
                        "ram_power": {
                            "process_3": 0.7047758102416992,
                            "process_2": 0.7045927047729492,
                            "process_0": 0.6849675178527832,
                            "process_1": 0.7048287391662598
                        },
                        "cpu_energy": {
                            "process_3": 0.0012345986377476949,
                            "process_2": 0.0012216625287073835,
                            "process_0": 0.0012474106381287129,
                            "process_1": 0.0011887976461603103
                        },
                        "gpu_energy": {
                            "process_3": 0.010194458711119125,
                            "process_2": 0.010193227043465924,
                            "process_0": 0.010190274541104216,
                            "process_1": 0.010196416212684412
                        },
                        "ram_energy": {
                            "process_3": 6.726987708403548e-06,
                            "process_2": 6.620111259345918e-06,
                            "process_0": 6.58335035926018e-06,
                            "process_1": 7.188188044938638e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.01143578433657522,
                            "process_2": 0.011421509683432658,
                            "process_0": 0.011444268529592194,
                            "process_1": 0.011392402046889662
                        },
                        "total_energy_joules": {
                            "process_3": 41168.82361167079,
                            "process_2": 41117.43486035756,
                            "process_0": 41199.366706531895,
                            "process_1": 41012.64736880278
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 41.105807949795505,
                        "ram_power_avg": 0.6997911930084229,
                        "cpu_energy_total": 0.004892469450744102,
                        "gpu_energy_total": 0.04077437650837368,
                        "ram_energy_total": 2.7118637371948284e-05,
                        "total_energy_kwh": 0.04569396459648973,
                        "total_energy_joules": 164498.27254736304
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.0995998301154357,
                        "joules_per_token": 10.040177767783389,
                        "flops_per_joule": 319994742.15577596,
                        "joules_per_flop": 3.1250513469786704e-09
                    },
                    "per-process_emissions": [
                        0.00435646204301833,
                        0.004351024113903671,
                        0.004359694096348146,
                        0.004339935559762617
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0530": {
            "setup": {
                "experiment_id": "0530",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 02:57:25 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_topp_0.8_temp_0.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_p",
                    "decoder_temperature": 0.4,
                    "decoder_top_k": null,
                    "decoder_top_p": 0.8
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 39.02465990500059,
                        "average_latency_ms_per_batch": 4878.082488125074,
                        "throughput_queries_per_sec": 3.279977335141316,
                        "throughput_tokens_per_sec": 419.83709889808847
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38403047424,
                        "gpu_max_memory_reserved_bytes": 38403047424
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            7.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.9,
                        "cpu_memory_usage_bytes": 1958715392
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0530",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_3": 543.3213320336529,
                            "process_1": 1034.435053665934,
                            "process_2": 1368.4409408957267,
                            "process_0": 1025.2190043104847
                        },
                        "ram_power": {
                            "process_3": 0.7255768775939941,
                            "process_1": 0.7109441757202148,
                            "process_2": 0.7102804183959961,
                            "process_0": 0.6828932762145996
                        },
                        "cpu_energy": {
                            "process_3": 0.0011765453744083062,
                            "process_1": 0.0011590531661513524,
                            "process_2": 0.0011861164520323658,
                            "process_0": 0.0011797077259325306
                        },
                        "gpu_energy": {
                            "process_3": 0.010404691657079468,
                            "process_1": 0.010390920534952386,
                            "process_2": 0.01033246159929746,
                            "process_0": 0.010390920534952386
                        },
                        "ram_energy": {
                            "process_3": 7.327194046134843e-06,
                            "process_1": 7.158980460576327e-06,
                            "process_2": 6.872960132372952e-06,
                            "process_0": 7.005594416794701e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.011588564225533907,
                            "process_1": 0.011557132681564306,
                            "process_2": 0.0115254510114622,
                            "process_0": 0.011577633855301705
                        },
                        "total_energy_joules": {
                            "process_3": 41718.831211922065,
                            "process_1": 41605.6776536315,
                            "process_2": 41491.62364126392,
                            "process_0": 41679.48187908614
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 992.8540827264496,
                        "ram_power_avg": 0.7074236869812012,
                        "cpu_energy_total": 0.004701422718524555,
                        "gpu_energy_total": 0.0415189943262817,
                        "ram_energy_total": 2.8364729055878823e-05,
                        "total_energy_kwh": 0.046248781773862116,
                        "total_energy_joules": 166495.61438590364
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09840499439237574,
                        "joules_per_token": 10.162085838983376,
                        "flops_per_joule": 316155969.05068177,
                        "joules_per_flop": 3.1629957928761856e-09
                    },
                    "per-process_emissions": [
                        0.004414663541717142,
                        0.004402689695041923,
                        0.004390620562816526,
                        0.004410499617177184
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0530": {
            "setup": {
                "experiment_id": "0530",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 02:57:25 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_topp_0.8_temp_0.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_p",
                    "decoder_temperature": 0.4,
                    "decoder_top_k": null,
                    "decoder_top_p": 0.8
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 39.02465990500059,
                        "average_latency_ms_per_batch": 4878.082488125074,
                        "throughput_queries_per_sec": 3.279977335141316,
                        "throughput_tokens_per_sec": 419.83709889808847
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38403047424,
                        "gpu_max_memory_reserved_bytes": 38403047424
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            7.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.9,
                        "cpu_memory_usage_bytes": 1958715392
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0530",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_3": 543.3213320336529,
                            "process_1": 1034.435053665934,
                            "process_2": 1368.4409408957267,
                            "process_0": 1025.2190043104847
                        },
                        "ram_power": {
                            "process_3": 0.7255768775939941,
                            "process_1": 0.7109441757202148,
                            "process_2": 0.7102804183959961,
                            "process_0": 0.6828932762145996
                        },
                        "cpu_energy": {
                            "process_3": 0.0011765453744083062,
                            "process_1": 0.0011590531661513524,
                            "process_2": 0.0011861164520323658,
                            "process_0": 0.0011797077259325306
                        },
                        "gpu_energy": {
                            "process_3": 0.010404691657079468,
                            "process_1": 0.010390920534952386,
                            "process_2": 0.01033246159929746,
                            "process_0": 0.010390920534952386
                        },
                        "ram_energy": {
                            "process_3": 7.327194046134843e-06,
                            "process_1": 7.158980460576327e-06,
                            "process_2": 6.872960132372952e-06,
                            "process_0": 7.005594416794701e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.011588564225533907,
                            "process_1": 0.011557132681564306,
                            "process_2": 0.0115254510114622,
                            "process_0": 0.011577633855301705
                        },
                        "total_energy_joules": {
                            "process_3": 41718.831211922065,
                            "process_1": 41605.6776536315,
                            "process_2": 41491.62364126392,
                            "process_0": 41679.48187908614
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 992.8540827264496,
                        "ram_power_avg": 0.7074236869812012,
                        "cpu_energy_total": 0.004701422718524555,
                        "gpu_energy_total": 0.0415189943262817,
                        "ram_energy_total": 2.8364729055878823e-05,
                        "total_energy_kwh": 0.046248781773862116,
                        "total_energy_joules": 166495.61438590364
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09840499439237574,
                        "joules_per_token": 10.162085838983376,
                        "flops_per_joule": 316155969.05068177,
                        "joules_per_flop": 3.1629957928761856e-09
                    },
                    "per-process_emissions": [
                        0.004414663541717142,
                        0.004402689695041923,
                        0.004390620562816526,
                        0.004410499617177184
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0531": {
            "setup": {
                "experiment_id": "0531",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 02:58:57 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_300_temp_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 300,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.46686973312171,
                        "average_latency_ms_per_batch": 4808.358716640214,
                        "throughput_queries_per_sec": 3.3275387596663792,
                        "throughput_tokens_per_sec": 425.92496123729654
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38451281920,
                        "gpu_max_memory_reserved_bytes": 38451281920
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.5,
                        "cpu_memory_usage_bytes": 1978798080
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0531",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 213.29516168426298,
                            "process_2": 225.78229119339483,
                            "process_3": 308956.28466582927,
                            "process_1": 204.30746135680118
                        },
                        "ram_power": {
                            "process_0": 0.6887698173522949,
                            "process_2": 0.7245526313781738,
                            "process_3": 0.7086181640625,
                            "process_1": 0.7329239845275879
                        },
                        "cpu_energy": {
                            "process_0": 0.0010778542811549414,
                            "process_2": 0.0011126798684672392,
                            "process_3": 0.001134835094020673,
                            "process_1": 0.0010755141644094692
                        },
                        "gpu_energy": {
                            "process_0": 0.010259174873997523,
                            "process_2": 0.010259174873997523,
                            "process_3": 0.010264664322836126,
                            "process_1": 0.010261795987206668
                        },
                        "ram_energy": {
                            "process_0": 7.462053576604496e-06,
                            "process_2": 8.073958201012244e-06,
                            "process_3": 7.619890205735442e-06,
                            "process_1": 7.936373104837936e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.01134449120872907,
                            "process_2": 0.011379928700665778,
                            "process_3": 0.011407119307062532,
                            "process_1": 0.011345246524720971
                        },
                        "total_energy_joules": {
                            "process_0": 40840.168351424654,
                            "process_2": 40967.743322396804,
                            "process_3": 41065.62950542512,
                            "process_1": 40842.8874889955
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 77399.91739501593,
                        "ram_power_avg": 0.7137161493301392,
                        "cpu_energy_total": 0.004400883408052322,
                        "gpu_energy_total": 0.04104481005803784,
                        "ram_energy_total": 3.109227508819012e-05,
                        "total_energy_kwh": 0.045476785741178354,
                        "total_energy_joules": 163716.42866824206
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.1000754788830682,
                        "joules_per_token": 9.992457804458134,
                        "flops_per_joule": 321522908.464683,
                        "joules_per_flop": 3.1101982896806333e-09
                    },
                    "per-process_emissions": [
                        0.00432168392596534,
                        0.004335183838518628,
                        0.004345542100025471,
                        0.004321971663592454
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0531": {
            "setup": {
                "experiment_id": "0531",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 02:58:57 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_300_temp_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 300,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.46686973312171,
                        "average_latency_ms_per_batch": 4808.358716640214,
                        "throughput_queries_per_sec": 3.3275387596663792,
                        "throughput_tokens_per_sec": 425.92496123729654
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38451281920,
                        "gpu_max_memory_reserved_bytes": 38451281920
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.5,
                        "cpu_memory_usage_bytes": 1978798080
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0531",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 213.29516168426298,
                            "process_2": 225.78229119339483,
                            "process_3": 308956.28466582927,
                            "process_1": 204.30746135680118
                        },
                        "ram_power": {
                            "process_0": 0.6887698173522949,
                            "process_2": 0.7245526313781738,
                            "process_3": 0.7086181640625,
                            "process_1": 0.7329239845275879
                        },
                        "cpu_energy": {
                            "process_0": 0.0010778542811549414,
                            "process_2": 0.0011126798684672392,
                            "process_3": 0.001134835094020673,
                            "process_1": 0.0010755141644094692
                        },
                        "gpu_energy": {
                            "process_0": 0.010259174873997523,
                            "process_2": 0.010259174873997523,
                            "process_3": 0.010264664322836126,
                            "process_1": 0.010261795987206668
                        },
                        "ram_energy": {
                            "process_0": 7.462053576604496e-06,
                            "process_2": 8.073958201012244e-06,
                            "process_3": 7.619890205735442e-06,
                            "process_1": 7.936373104837936e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.01134449120872907,
                            "process_2": 0.011379928700665778,
                            "process_3": 0.011407119307062532,
                            "process_1": 0.011345246524720971
                        },
                        "total_energy_joules": {
                            "process_0": 40840.168351424654,
                            "process_2": 40967.743322396804,
                            "process_3": 41065.62950542512,
                            "process_1": 40842.8874889955
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 77399.91739501593,
                        "ram_power_avg": 0.7137161493301392,
                        "cpu_energy_total": 0.004400883408052322,
                        "gpu_energy_total": 0.04104481005803784,
                        "ram_energy_total": 3.109227508819012e-05,
                        "total_energy_kwh": 0.045476785741178354,
                        "total_energy_joules": 163716.42866824206
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.1000754788830682,
                        "joules_per_token": 9.992457804458134,
                        "flops_per_joule": 321522908.464683,
                        "joules_per_flop": 3.1101982896806333e-09
                    },
                    "per-process_emissions": [
                        0.00432168392596534,
                        0.004335183838518628,
                        0.004345542100025471,
                        0.004321971663592454
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0532": {
            "setup": {
                "experiment_id": "0532",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 03:00:31 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_400_temp_1.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 1.2,
                    "decoder_top_k": 400,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 39.94481318903854,
                        "average_latency_ms_per_batch": 4993.101648629818,
                        "throughput_queries_per_sec": 3.204421044460539,
                        "throughput_tokens_per_sec": 410.165893690949
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 37878759424,
                        "gpu_max_memory_reserved_bytes": 37878759424
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            22.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.6,
                        "cpu_memory_usage_bytes": 1977417728
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0532",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_2": 0.0,
                            "process_3": 0.0,
                            "process_0": 154.62374496047872,
                            "process_1": 8.30721105520246
                        },
                        "ram_power": {
                            "process_2": 0.7244067192077637,
                            "process_3": 0.7249059677124023,
                            "process_0": 0.6887869834899902,
                            "process_1": 0.7246341705322267
                        },
                        "cpu_energy": {
                            "process_2": 0.001283091626872192,
                            "process_3": 0.0013549428122805692,
                            "process_0": 0.0012865887589086926,
                            "process_1": 0.001319873454845947
                        },
                        "gpu_energy": {
                            "process_2": 0.010346325221496144,
                            "process_3": 0.010346325221496144,
                            "process_0": 0.010342098551451073,
                            "process_1": 0.01034786688939704
                        },
                        "ram_energy": {
                            "process_2": 7.571255318855286e-06,
                            "process_3": 7.646054427448608e-06,
                            "process_0": 6.871534197051731e-06,
                            "process_1": 7.813475926419098e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.011636988103687188,
                            "process_3": 0.011708914088204169,
                            "process_0": 0.011635558844556816,
                            "process_1": 0.01167555382016941
                        },
                        "total_energy_joules": {
                            "process_2": 41893.157173273874,
                            "process_3": 42152.09071753501,
                            "process_0": 41888.011840404535,
                            "process_1": 42031.99375260987
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 40.732739003920294,
                        "ram_power_avg": 0.7156834602355957,
                        "cpu_energy_total": 0.005244496652907401,
                        "gpu_energy_total": 0.0413826158838404,
                        "ram_energy_total": 2.9902319869774723e-05,
                        "total_energy_kwh": 0.046657014856617586,
                        "total_energy_joules": 167965.2534838233
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09754398400963292,
                        "joules_per_token": 10.251785490956012,
                        "flops_per_joule": 313389711.36630714,
                        "joules_per_flop": 3.190915220669593e-09
                    },
                    "per-process_emissions": [
                        0.0044331106180996345,
                        0.0044605108219013785,
                        0.004432566141833919,
                        0.0044478022277935365
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0532": {
            "setup": {
                "experiment_id": "0532",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 03:00:31 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_400_temp_1.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 1.2,
                    "decoder_top_k": 400,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 39.94481318903854,
                        "average_latency_ms_per_batch": 4993.101648629818,
                        "throughput_queries_per_sec": 3.204421044460539,
                        "throughput_tokens_per_sec": 410.165893690949
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 37878759424,
                        "gpu_max_memory_reserved_bytes": 37878759424
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            22.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.6,
                        "cpu_memory_usage_bytes": 1977417728
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0532",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_2": 0.0,
                            "process_3": 0.0,
                            "process_0": 154.62374496047872,
                            "process_1": 8.30721105520246
                        },
                        "ram_power": {
                            "process_2": 0.7244067192077637,
                            "process_3": 0.7249059677124023,
                            "process_0": 0.6887869834899902,
                            "process_1": 0.7246341705322267
                        },
                        "cpu_energy": {
                            "process_2": 0.001283091626872192,
                            "process_3": 0.0013549428122805692,
                            "process_0": 0.0012865887589086926,
                            "process_1": 0.001319873454845947
                        },
                        "gpu_energy": {
                            "process_2": 0.010346325221496144,
                            "process_3": 0.010346325221496144,
                            "process_0": 0.010342098551451073,
                            "process_1": 0.01034786688939704
                        },
                        "ram_energy": {
                            "process_2": 7.571255318855286e-06,
                            "process_3": 7.646054427448608e-06,
                            "process_0": 6.871534197051731e-06,
                            "process_1": 7.813475926419098e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.011636988103687188,
                            "process_3": 0.011708914088204169,
                            "process_0": 0.011635558844556816,
                            "process_1": 0.01167555382016941
                        },
                        "total_energy_joules": {
                            "process_2": 41893.157173273874,
                            "process_3": 42152.09071753501,
                            "process_0": 41888.011840404535,
                            "process_1": 42031.99375260987
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 40.732739003920294,
                        "ram_power_avg": 0.7156834602355957,
                        "cpu_energy_total": 0.005244496652907401,
                        "gpu_energy_total": 0.0413826158838404,
                        "ram_energy_total": 2.9902319869774723e-05,
                        "total_energy_kwh": 0.046657014856617586,
                        "total_energy_joules": 167965.2534838233
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09754398400963292,
                        "joules_per_token": 10.251785490956012,
                        "flops_per_joule": 313389711.36630714,
                        "joules_per_flop": 3.190915220669593e-09
                    },
                    "per-process_emissions": [
                        0.0044331106180996345,
                        0.0044605108219013785,
                        0.004432566141833919,
                        0.0044478022277935365
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0533": {
            "setup": {
                "experiment_id": "0533",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 03:02:06 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_const_0.1_0.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.1,
                    "delay_max": 0.2,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 39.22186535905348,
                        "average_latency_ms_per_batch": 4902.733169881685,
                        "throughput_queries_per_sec": 3.2634857834586413,
                        "throughput_tokens_per_sec": 417.7261802827061
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38048628736,
                        "gpu_max_memory_reserved_bytes": 38048628736
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            27.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.3,
                        "cpu_memory_usage_bytes": 1972150272
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0533",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_1": 769.7668044123951,
                            "process_3": 807.7735003595068,
                            "process_0": 841.3217839123679,
                            "process_2": 674.9899555392258
                        },
                        "ram_power": {
                            "process_1": 0.7310914993286133,
                            "process_3": 0.7234840393066406,
                            "process_0": 0.6862378120422363,
                            "process_2": 0.7058944702148438
                        },
                        "cpu_energy": {
                            "process_1": 0.0011529641976194395,
                            "process_3": 0.001049592773651966,
                            "process_0": 0.0010919716737516863,
                            "process_2": 0.0012011900895577128
                        },
                        "gpu_energy": {
                            "process_1": 0.01024449458448018,
                            "process_3": 0.01033702215850063,
                            "process_0": 0.010261732931601486,
                            "process_2": 0.010225719013906343
                        },
                        "ram_energy": {
                            "process_1": 7.192638766086367e-06,
                            "process_3": 6.855532369370967e-06,
                            "process_0": 7.083278496890072e-06,
                            "process_2": 7.381140401011462e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.011404651420865702,
                            "process_3": 0.011393470464521967,
                            "process_0": 0.011360787883850062,
                            "process_2": 0.011434290243865062
                        },
                        "total_energy_joules": {
                            "process_1": 41056.74511511653,
                            "process_3": 41016.49367227908,
                            "process_0": 40898.83638186022,
                            "process_2": 41163.44487791422
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 773.463011055874,
                        "ram_power_avg": 0.7116769552230835,
                        "cpu_energy_total": 0.004495718734580805,
                        "gpu_energy_total": 0.04106896868848864,
                        "ram_energy_total": 2.8512590033358866e-05,
                        "total_energy_kwh": 0.04559320001310279,
                        "total_energy_joules": 164135.52004717005
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09981995362911994,
                        "joules_per_token": 10.018037112254031,
                        "flops_per_joule": 320701955.88216656,
                        "joules_per_flop": 3.1181599664687527e-09
                    },
                    "per-process_emissions": [
                        0.004344601958778789,
                        0.004340342573459643,
                        0.004327892144352681,
                        0.004355892868400395
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0533": {
            "setup": {
                "experiment_id": "0533",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 03:02:06 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_const_0.1_0.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.1,
                    "delay_max": 0.2,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 39.22186535905348,
                        "average_latency_ms_per_batch": 4902.733169881685,
                        "throughput_queries_per_sec": 3.2634857834586413,
                        "throughput_tokens_per_sec": 417.7261802827061
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38048628736,
                        "gpu_max_memory_reserved_bytes": 38048628736
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            27.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.3,
                        "cpu_memory_usage_bytes": 1972150272
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0533",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_1": 769.7668044123951,
                            "process_3": 807.7735003595068,
                            "process_0": 841.3217839123679,
                            "process_2": 674.9899555392258
                        },
                        "ram_power": {
                            "process_1": 0.7310914993286133,
                            "process_3": 0.7234840393066406,
                            "process_0": 0.6862378120422363,
                            "process_2": 0.7058944702148438
                        },
                        "cpu_energy": {
                            "process_1": 0.0011529641976194395,
                            "process_3": 0.001049592773651966,
                            "process_0": 0.0010919716737516863,
                            "process_2": 0.0012011900895577128
                        },
                        "gpu_energy": {
                            "process_1": 0.01024449458448018,
                            "process_3": 0.01033702215850063,
                            "process_0": 0.010261732931601486,
                            "process_2": 0.010225719013906343
                        },
                        "ram_energy": {
                            "process_1": 7.192638766086367e-06,
                            "process_3": 6.855532369370967e-06,
                            "process_0": 7.083278496890072e-06,
                            "process_2": 7.381140401011462e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.011404651420865702,
                            "process_3": 0.011393470464521967,
                            "process_0": 0.011360787883850062,
                            "process_2": 0.011434290243865062
                        },
                        "total_energy_joules": {
                            "process_1": 41056.74511511653,
                            "process_3": 41016.49367227908,
                            "process_0": 40898.83638186022,
                            "process_2": 41163.44487791422
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 773.463011055874,
                        "ram_power_avg": 0.7116769552230835,
                        "cpu_energy_total": 0.004495718734580805,
                        "gpu_energy_total": 0.04106896868848864,
                        "ram_energy_total": 2.8512590033358866e-05,
                        "total_energy_kwh": 0.04559320001310279,
                        "total_energy_joules": 164135.52004717005
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09981995362911994,
                        "joules_per_token": 10.018037112254031,
                        "flops_per_joule": 320701955.88216656,
                        "joules_per_flop": 3.1181599664687527e-09
                    },
                    "per-process_emissions": [
                        0.004344601958778789,
                        0.004340342573459643,
                        0.004327892144352681,
                        0.004355892868400395
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0534": {
            "setup": {
                "experiment_id": "0534",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 03:03:41 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_burst_0.1_0.2_2.0_10",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.1,
                    "delay_max": 0.2,
                    "simulate_burst": true,
                    "burst_interval": 2.0,
                    "burst_size": 10
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 39.37754278880311,
                        "average_latency_ms_per_batch": 4922.192848600389,
                        "throughput_queries_per_sec": 3.2505837321163784,
                        "throughput_tokens_per_sec": 416.07471771089644
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38048628736,
                        "gpu_max_memory_reserved_bytes": 38048628736
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            4.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.2,
                        "cpu_memory_usage_bytes": 1975205888
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0534",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_3": 781.1707415534413,
                            "process_1": 1111.8220945964654,
                            "process_2": 1008.9849891019973,
                            "process_0": 976.5673133888781
                        },
                        "ram_power": {
                            "process_3": 0.715367317199707,
                            "process_1": 0.7230248451232911,
                            "process_2": 0.7300672531127931,
                            "process_0": 0.6877241134643555
                        },
                        "cpu_energy": {
                            "process_3": 0.0012905240472773588,
                            "process_1": 0.001282633185403029,
                            "process_2": 0.0012510053650948976,
                            "process_0": 0.0012369576212167888
                        },
                        "gpu_energy": {
                            "process_3": 0.010282195170194797,
                            "process_1": 0.010261040153269008,
                            "process_2": 0.01025121153429609,
                            "process_0": 0.010277226832885233
                        },
                        "ram_energy": {
                            "process_3": 7.160727777229529e-06,
                            "process_1": 7.225394246050389e-06,
                            "process_2": 7.0874092852488886e-06,
                            "process_0": 6.588126469169073e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.011579879945249385,
                            "process_1": 0.011550898732918084,
                            "process_2": 0.011509304308676234,
                            "process_0": 0.011520772580571196
                        },
                        "total_energy_joules": {
                            "process_3": 41687.567802897785,
                            "process_1": 41583.2354385051,
                            "process_2": 41433.49551123444,
                            "process_0": 41474.78129005631
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 969.6362846601955,
                        "ram_power_avg": 0.7140458822250366,
                        "cpu_energy_total": 0.005061120218992073,
                        "gpu_energy_total": 0.04107167369064513,
                        "ram_energy_total": 2.806165777769788e-05,
                        "total_energy_kwh": 0.0461608555674149,
                        "total_energy_joules": 166179.08004269362
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09859243411258946,
                        "joules_per_token": 10.142766115887062,
                        "flops_per_joule": 316758176.15153754,
                        "joules_per_flop": 3.1569824405151223e-09
                    },
                    "per-process_emissions": [
                        0.004411355265142754,
                        0.004400314872305145,
                        0.004384469476390211,
                        0.004388838314568597
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0534": {
            "setup": {
                "experiment_id": "0534",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 03:03:41 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_burst_0.1_0.2_2.0_10",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.1,
                    "delay_max": 0.2,
                    "simulate_burst": true,
                    "burst_interval": 2.0,
                    "burst_size": 10
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 39.37754278880311,
                        "average_latency_ms_per_batch": 4922.192848600389,
                        "throughput_queries_per_sec": 3.2505837321163784,
                        "throughput_tokens_per_sec": 416.07471771089644
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38048628736,
                        "gpu_max_memory_reserved_bytes": 38048628736
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            4.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.2,
                        "cpu_memory_usage_bytes": 1975205888
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0534",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_3": 781.1707415534413,
                            "process_1": 1111.8220945964654,
                            "process_2": 1008.9849891019973,
                            "process_0": 976.5673133888781
                        },
                        "ram_power": {
                            "process_3": 0.715367317199707,
                            "process_1": 0.7230248451232911,
                            "process_2": 0.7300672531127931,
                            "process_0": 0.6877241134643555
                        },
                        "cpu_energy": {
                            "process_3": 0.0012905240472773588,
                            "process_1": 0.001282633185403029,
                            "process_2": 0.0012510053650948976,
                            "process_0": 0.0012369576212167888
                        },
                        "gpu_energy": {
                            "process_3": 0.010282195170194797,
                            "process_1": 0.010261040153269008,
                            "process_2": 0.01025121153429609,
                            "process_0": 0.010277226832885233
                        },
                        "ram_energy": {
                            "process_3": 7.160727777229529e-06,
                            "process_1": 7.225394246050389e-06,
                            "process_2": 7.0874092852488886e-06,
                            "process_0": 6.588126469169073e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.011579879945249385,
                            "process_1": 0.011550898732918084,
                            "process_2": 0.011509304308676234,
                            "process_0": 0.011520772580571196
                        },
                        "total_energy_joules": {
                            "process_3": 41687.567802897785,
                            "process_1": 41583.2354385051,
                            "process_2": 41433.49551123444,
                            "process_0": 41474.78129005631
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 969.6362846601955,
                        "ram_power_avg": 0.7140458822250366,
                        "cpu_energy_total": 0.005061120218992073,
                        "gpu_energy_total": 0.04107167369064513,
                        "ram_energy_total": 2.806165777769788e-05,
                        "total_energy_kwh": 0.0461608555674149,
                        "total_energy_joules": 166179.08004269362
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09859243411258946,
                        "joules_per_token": 10.142766115887062,
                        "flops_per_joule": 316758176.15153754,
                        "joules_per_flop": 3.1569824405151223e-09
                    },
                    "per-process_emissions": [
                        0.004411355265142754,
                        0.004400314872305145,
                        0.004384469476390211,
                        0.004388838314568597
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0535": {
            "setup": {
                "experiment_id": "0535",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 03:05:13 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_50_temp_0.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 0.4,
                    "decoder_top_k": 50,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.366663055028766,
                        "average_latency_ms_per_batch": 4795.832881878596,
                        "throughput_queries_per_sec": 3.3362296798241586,
                        "throughput_tokens_per_sec": 427.0373990174923
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38449184768,
                        "gpu_max_memory_reserved_bytes": 38449184768
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            3.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.6,
                        "cpu_memory_usage_bytes": 1978777600
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0535",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 187.1215424269042,
                            "process_2": 191.07000292855057,
                            "process_1": 174.47338715758195,
                            "process_3": 168.39044878758384
                        },
                        "ram_power": {
                            "process_0": 0.689095973968506,
                            "process_2": 0.7079372406005859,
                            "process_1": 0.7247142791748047,
                            "process_3": 0.7079730033874512
                        },
                        "cpu_energy": {
                            "process_0": 0.0010907812459390698,
                            "process_2": 0.0011126443627690608,
                            "process_1": 0.0011048269178736516,
                            "process_3": 0.0011377481835006618
                        },
                        "gpu_energy": {
                            "process_0": 0.010253259035936324,
                            "process_2": 0.010251785423646353,
                            "process_1": 0.010251785423646353,
                            "process_3": 0.010256185982722599
                        },
                        "ram_energy": {
                            "process_0": 6.784464846171191e-06,
                            "process_2": 6.711978181735813e-06,
                            "process_1": 6.843033666385526e-06,
                            "process_3": 7.6273069514442494e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.011350824746721569,
                            "process_2": 0.011371141764597147,
                            "process_1": 0.01136345537518639,
                            "process_3": 0.011401561473174705
                        },
                        "total_energy_joules": {
                            "process_0": 40862.96908819765,
                            "process_2": 40936.11035254973,
                            "process_1": 40908.439350671004,
                            "process_3": 41045.62130342894
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 180.26384532515516,
                        "ram_power_avg": 0.7074301242828369,
                        "cpu_energy_total": 0.004446000710082444,
                        "gpu_energy_total": 0.04101301586595163,
                        "ram_energy_total": 2.796678364573678e-05,
                        "total_energy_kwh": 0.045486983359679815,
                        "total_energy_joules": 163753.14009484733
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.10005304319972268,
                        "joules_per_token": 9.994698492117147,
                        "flops_per_joule": 321450827.00933397,
                        "joules_per_flop": 3.110895713984158e-09
                    },
                    "per-process_emissions": [
                        0.0043240966872635815,
                        0.004331836455223283,
                        0.004328908325177255,
                        0.004343424843205904
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0535": {
            "setup": {
                "experiment_id": "0535",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 03:05:13 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_50_temp_0.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 0.4,
                    "decoder_top_k": 50,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.366663055028766,
                        "average_latency_ms_per_batch": 4795.832881878596,
                        "throughput_queries_per_sec": 3.3362296798241586,
                        "throughput_tokens_per_sec": 427.0373990174923
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38449184768,
                        "gpu_max_memory_reserved_bytes": 38449184768
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            3.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.6,
                        "cpu_memory_usage_bytes": 1978777600
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0535",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 187.1215424269042,
                            "process_2": 191.07000292855057,
                            "process_1": 174.47338715758195,
                            "process_3": 168.39044878758384
                        },
                        "ram_power": {
                            "process_0": 0.689095973968506,
                            "process_2": 0.7079372406005859,
                            "process_1": 0.7247142791748047,
                            "process_3": 0.7079730033874512
                        },
                        "cpu_energy": {
                            "process_0": 0.0010907812459390698,
                            "process_2": 0.0011126443627690608,
                            "process_1": 0.0011048269178736516,
                            "process_3": 0.0011377481835006618
                        },
                        "gpu_energy": {
                            "process_0": 0.010253259035936324,
                            "process_2": 0.010251785423646353,
                            "process_1": 0.010251785423646353,
                            "process_3": 0.010256185982722599
                        },
                        "ram_energy": {
                            "process_0": 6.784464846171191e-06,
                            "process_2": 6.711978181735813e-06,
                            "process_1": 6.843033666385526e-06,
                            "process_3": 7.6273069514442494e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.011350824746721569,
                            "process_2": 0.011371141764597147,
                            "process_1": 0.01136345537518639,
                            "process_3": 0.011401561473174705
                        },
                        "total_energy_joules": {
                            "process_0": 40862.96908819765,
                            "process_2": 40936.11035254973,
                            "process_1": 40908.439350671004,
                            "process_3": 41045.62130342894
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 180.26384532515516,
                        "ram_power_avg": 0.7074301242828369,
                        "cpu_energy_total": 0.004446000710082444,
                        "gpu_energy_total": 0.04101301586595163,
                        "ram_energy_total": 2.796678364573678e-05,
                        "total_energy_kwh": 0.045486983359679815,
                        "total_energy_joules": 163753.14009484733
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.10005304319972268,
                        "joules_per_token": 9.994698492117147,
                        "flops_per_joule": 321450827.00933397,
                        "joules_per_flop": 3.110895713984158e-09
                    },
                    "per-process_emissions": [
                        0.0043240966872635815,
                        0.004331836455223283,
                        0.004328908325177255,
                        0.004343424843205904
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0536": {
            "setup": {
                "experiment_id": "0536",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 03:06:49 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_100_temp_0.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 0.0,
                    "decoder_top_k": 100,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 40.954861840989906,
                        "average_latency_ms_per_batch": 5119.357730123738,
                        "throughput_queries_per_sec": 3.1253920596037874,
                        "throughput_tokens_per_sec": 400.0501836292848
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38325452800,
                        "gpu_max_memory_reserved_bytes": 38325452800
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            19.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 11.9,
                        "cpu_memory_usage_bytes": 1945513984
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0536",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_3": 460.6183949415985,
                            "process_1": 263.932314143437,
                            "process_2": 565.1182009955911,
                            "process_0": 0.0
                        },
                        "ram_power": {
                            "process_3": 0.7063393592834473,
                            "process_1": 0.721886157989502,
                            "process_2": 0.7058944702148438,
                            "process_0": 0.6782855987548828
                        },
                        "cpu_energy": {
                            "process_3": 0.001115508151502581,
                            "process_1": 0.0011861991637488246,
                            "process_2": 0.0011346761380991665,
                            "process_0": 0.0011916744323989404
                        },
                        "gpu_energy": {
                            "process_3": 0.010476281714350222,
                            "process_1": 0.010424408061744828,
                            "process_2": 0.010445308356240801,
                            "process_0": 0.010440113629861969
                        },
                        "ram_energy": {
                            "process_3": 7.722065386146924e-06,
                            "process_1": 7.994553380091264e-06,
                            "process_2": 7.539695450438147e-06,
                            "process_0": 7.519559532991054e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.011599511931238949,
                            "process_1": 0.011618601778873748,
                            "process_2": 0.011587524189790404,
                            "process_0": 0.011639307621793901
                        },
                        "total_energy_joules": {
                            "process_3": 41758.24295246021,
                            "process_1": 41826.966403945495,
                            "process_2": 41715.08708324545,
                            "process_0": 41901.507438458044
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 322.41722752015664,
                        "ram_power_avg": 0.703101396560669,
                        "cpu_energy_total": 0.0046280578857495125,
                        "gpu_energy_total": 0.04178611176219782,
                        "ram_energy_total": 3.077587374966739e-05,
                        "total_energy_kwh": 0.046444945521697,
                        "total_energy_joules": 167201.8038781092
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09798937343967894,
                        "joules_per_token": 10.205188224982251,
                        "flops_per_joule": 314820660.3514741,
                        "joules_per_flop": 3.1764116080678237e-09
                    },
                    "per-process_emissions": [
                        0.004418834070205478,
                        0.004426106347661955,
                        0.004414267340100655,
                        0.004433994238522387
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0536": {
            "setup": {
                "experiment_id": "0536",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 03:06:49 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_100_temp_0.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 0.0,
                    "decoder_top_k": 100,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 40.954861840989906,
                        "average_latency_ms_per_batch": 5119.357730123738,
                        "throughput_queries_per_sec": 3.1253920596037874,
                        "throughput_tokens_per_sec": 400.0501836292848
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38325452800,
                        "gpu_max_memory_reserved_bytes": 38325452800
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            19.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 11.9,
                        "cpu_memory_usage_bytes": 1945513984
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0536",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_3": 460.6183949415985,
                            "process_1": 263.932314143437,
                            "process_2": 565.1182009955911,
                            "process_0": 0.0
                        },
                        "ram_power": {
                            "process_3": 0.7063393592834473,
                            "process_1": 0.721886157989502,
                            "process_2": 0.7058944702148438,
                            "process_0": 0.6782855987548828
                        },
                        "cpu_energy": {
                            "process_3": 0.001115508151502581,
                            "process_1": 0.0011861991637488246,
                            "process_2": 0.0011346761380991665,
                            "process_0": 0.0011916744323989404
                        },
                        "gpu_energy": {
                            "process_3": 0.010476281714350222,
                            "process_1": 0.010424408061744828,
                            "process_2": 0.010445308356240801,
                            "process_0": 0.010440113629861969
                        },
                        "ram_energy": {
                            "process_3": 7.722065386146924e-06,
                            "process_1": 7.994553380091264e-06,
                            "process_2": 7.539695450438147e-06,
                            "process_0": 7.519559532991054e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.011599511931238949,
                            "process_1": 0.011618601778873748,
                            "process_2": 0.011587524189790404,
                            "process_0": 0.011639307621793901
                        },
                        "total_energy_joules": {
                            "process_3": 41758.24295246021,
                            "process_1": 41826.966403945495,
                            "process_2": 41715.08708324545,
                            "process_0": 41901.507438458044
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 322.41722752015664,
                        "ram_power_avg": 0.703101396560669,
                        "cpu_energy_total": 0.0046280578857495125,
                        "gpu_energy_total": 0.04178611176219782,
                        "ram_energy_total": 3.077587374966739e-05,
                        "total_energy_kwh": 0.046444945521697,
                        "total_energy_joules": 167201.8038781092
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09798937343967894,
                        "joules_per_token": 10.205188224982251,
                        "flops_per_joule": 314820660.3514741,
                        "joules_per_flop": 3.1764116080678237e-09
                    },
                    "per-process_emissions": [
                        0.004418834070205478,
                        0.004426106347661955,
                        0.004414267340100655,
                        0.004433994238522387
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0537": {
            "setup": {
                "experiment_id": "0537",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 03:08:26 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_greedy_temp_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "greedy",
                    "decoder_temperature": 0.8,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 42.487970172893256,
                        "average_latency_ms_per_batch": 5310.996271611657,
                        "throughput_queries_per_sec": 3.012617441575551,
                        "throughput_tokens_per_sec": 385.6150325216705
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38405144576,
                        "gpu_max_memory_reserved_bytes": 38405144576
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            28.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 53.5,
                        "cpu_memory_usage_bytes": 1973575680
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0537",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 718.4955827025641,
                            "process_3": 250.15110871201085,
                            "process_2": 0.0,
                            "process_1": 817.149400206322
                        },
                        "ram_power": {
                            "process_0": 0.6880903244018555,
                            "process_3": 0.706629753112793,
                            "process_2": 0.7232279777526855,
                            "process_1": 0.7309226989746094
                        },
                        "cpu_energy": {
                            "process_0": 0.0013159106488656108,
                            "process_3": 0.0013866633107154484,
                            "process_2": 0.0013523521335573607,
                            "process_1": 0.0012903359908505085
                        },
                        "gpu_energy": {
                            "process_0": 0.010662745752409108,
                            "process_3": 0.010396073316851329,
                            "process_2": 0.010590190694365731,
                            "process_1": 0.010477956437913605
                        },
                        "ram_energy": {
                            "process_0": 7.34179718231925e-06,
                            "process_3": 7.3841885154649376e-06,
                            "process_2": 7.581089962580658e-06,
                            "process_1": 7.724575039457479e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.011985998198457038,
                            "process_3": 0.011790120816082243,
                            "process_2": 0.011950123917885674,
                            "process_1": 0.01177601700380357
                        },
                        "total_energy_joules": {
                            "process_0": 43149.59351444534,
                            "process_3": 42444.434937896076,
                            "process_2": 43020.44610438842,
                            "process_1": 42393.66121369285
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 446.4490229052242,
                        "ram_power_avg": 0.7122176885604858,
                        "cpu_energy_total": 0.005345262083988929,
                        "gpu_energy_total": 0.04212696620153977,
                        "ram_energy_total": 3.0031650699822323e-05,
                        "total_energy_kwh": 0.04750225993622852,
                        "total_energy_joules": 171008.1357704227
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.0958083071673,
                        "joules_per_token": 10.437508286768963,
                        "flops_per_joule": 307813321.6979276,
                        "joules_per_flop": 3.2487222920824375e-09
                    },
                    "per-process_emissions": [
                        0.004566066013702209,
                        0.004491446524886531,
                        0.004552399706518547,
                        0.00448607367759897
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0537": {
            "setup": {
                "experiment_id": "0537",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 03:08:26 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_greedy_temp_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "greedy",
                    "decoder_temperature": 0.8,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 42.487970172893256,
                        "average_latency_ms_per_batch": 5310.996271611657,
                        "throughput_queries_per_sec": 3.012617441575551,
                        "throughput_tokens_per_sec": 385.6150325216705
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38405144576,
                        "gpu_max_memory_reserved_bytes": 38405144576
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            28.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 53.5,
                        "cpu_memory_usage_bytes": 1973575680
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0537",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 718.4955827025641,
                            "process_3": 250.15110871201085,
                            "process_2": 0.0,
                            "process_1": 817.149400206322
                        },
                        "ram_power": {
                            "process_0": 0.6880903244018555,
                            "process_3": 0.706629753112793,
                            "process_2": 0.7232279777526855,
                            "process_1": 0.7309226989746094
                        },
                        "cpu_energy": {
                            "process_0": 0.0013159106488656108,
                            "process_3": 0.0013866633107154484,
                            "process_2": 0.0013523521335573607,
                            "process_1": 0.0012903359908505085
                        },
                        "gpu_energy": {
                            "process_0": 0.010662745752409108,
                            "process_3": 0.010396073316851329,
                            "process_2": 0.010590190694365731,
                            "process_1": 0.010477956437913605
                        },
                        "ram_energy": {
                            "process_0": 7.34179718231925e-06,
                            "process_3": 7.3841885154649376e-06,
                            "process_2": 7.581089962580658e-06,
                            "process_1": 7.724575039457479e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.011985998198457038,
                            "process_3": 0.011790120816082243,
                            "process_2": 0.011950123917885674,
                            "process_1": 0.01177601700380357
                        },
                        "total_energy_joules": {
                            "process_0": 43149.59351444534,
                            "process_3": 42444.434937896076,
                            "process_2": 43020.44610438842,
                            "process_1": 42393.66121369285
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 446.4490229052242,
                        "ram_power_avg": 0.7122176885604858,
                        "cpu_energy_total": 0.005345262083988929,
                        "gpu_energy_total": 0.04212696620153977,
                        "ram_energy_total": 3.0031650699822323e-05,
                        "total_energy_kwh": 0.04750225993622852,
                        "total_energy_joules": 171008.1357704227
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.0958083071673,
                        "joules_per_token": 10.437508286768963,
                        "flops_per_joule": 307813321.6979276,
                        "joules_per_flop": 3.2487222920824375e-09
                    },
                    "per-process_emissions": [
                        0.004566066013702209,
                        0.004491446524886531,
                        0.004552399706518547,
                        0.00448607367759897
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0538": {
            "setup": {
                "experiment_id": "0538",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 03:10:14 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_topp_0.5_temp_1.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_p",
                    "decoder_temperature": 1.2,
                    "decoder_top_k": null,
                    "decoder_top_p": 0.5
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 45.15065634198254,
                        "average_latency_ms_per_batch": 5643.8320427478175,
                        "throughput_queries_per_sec": 2.8349532514100235,
                        "throughput_tokens_per_sec": 362.874016180483
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38403047424,
                        "gpu_max_memory_reserved_bytes": 38403047424
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            6.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 53.2,
                        "cpu_memory_usage_bytes": 1957429248
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0538",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_0": 134.74681950962133,
                            "process_3": 421.93270057399315,
                            "process_1": 1176.0178598183454,
                            "process_2": 871.9665918717186
                        },
                        "ram_power": {
                            "process_0": 0.6824512481689453,
                            "process_3": 0.7108068466186523,
                            "process_1": 0.7334432601928711,
                            "process_2": 0.7338624000549316
                        },
                        "cpu_energy": {
                            "process_0": 0.0013660061068749203,
                            "process_3": 0.0013941948295214386,
                            "process_1": 0.0012743280951945058,
                            "process_2": 0.0013507356161553614
                        },
                        "gpu_energy": {
                            "process_0": 0.010919247346500072,
                            "process_3": 0.011037064940756736,
                            "process_1": 0.010957555710479738,
                            "process_2": 0.010643604070427415
                        },
                        "ram_energy": {
                            "process_0": 7.81275387569153e-06,
                            "process_3": 7.92854074257586e-06,
                            "process_1": 8.18473936578488e-06,
                            "process_2": 8.13196610150355e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.012293066207250685,
                            "process_3": 0.012439188311020742,
                            "process_1": 0.012240068545040029,
                            "process_2": 0.012002471652684282
                        },
                        "total_energy_joules": {
                            "process_0": 44255.03834610247,
                            "process_3": 44781.07791967467,
                            "process_1": 44064.2467621441,
                            "process_2": 43208.89794966341
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 651.1659929434196,
                        "ram_power_avg": 0.7151409387588501,
                        "cpu_energy_total": 0.005385264647746226,
                        "gpu_energy_total": 0.04355747206816396,
                        "ram_energy_total": 3.2058000085555813e-05,
                        "total_energy_kwh": 0.04897479471599574,
                        "total_energy_joules": 176309.26097758464
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.0929276199625328,
                        "joules_per_token": 10.761063292088906,
                        "flops_per_joule": 298558238.04715675,
                        "joules_per_flop": 3.3494302704253357e-09
                    },
                    "per-process_emissions": [
                        0.004683043571652149,
                        0.004738708787083352,
                        0.004662854112232999,
                        0.004572341576090077
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0538": {
            "setup": {
                "experiment_id": "0538",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 03:10:14 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_topp_0.5_temp_1.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_p",
                    "decoder_temperature": 1.2,
                    "decoder_top_k": null,
                    "decoder_top_p": 0.5
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 45.15065634198254,
                        "average_latency_ms_per_batch": 5643.8320427478175,
                        "throughput_queries_per_sec": 2.8349532514100235,
                        "throughput_tokens_per_sec": 362.874016180483
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38403047424,
                        "gpu_max_memory_reserved_bytes": 38403047424
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            6.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 53.2,
                        "cpu_memory_usage_bytes": 1957429248
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0538",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_0": 134.74681950962133,
                            "process_3": 421.93270057399315,
                            "process_1": 1176.0178598183454,
                            "process_2": 871.9665918717186
                        },
                        "ram_power": {
                            "process_0": 0.6824512481689453,
                            "process_3": 0.7108068466186523,
                            "process_1": 0.7334432601928711,
                            "process_2": 0.7338624000549316
                        },
                        "cpu_energy": {
                            "process_0": 0.0013660061068749203,
                            "process_3": 0.0013941948295214386,
                            "process_1": 0.0012743280951945058,
                            "process_2": 0.0013507356161553614
                        },
                        "gpu_energy": {
                            "process_0": 0.010919247346500072,
                            "process_3": 0.011037064940756736,
                            "process_1": 0.010957555710479738,
                            "process_2": 0.010643604070427415
                        },
                        "ram_energy": {
                            "process_0": 7.81275387569153e-06,
                            "process_3": 7.92854074257586e-06,
                            "process_1": 8.18473936578488e-06,
                            "process_2": 8.13196610150355e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.012293066207250685,
                            "process_3": 0.012439188311020742,
                            "process_1": 0.012240068545040029,
                            "process_2": 0.012002471652684282
                        },
                        "total_energy_joules": {
                            "process_0": 44255.03834610247,
                            "process_3": 44781.07791967467,
                            "process_1": 44064.2467621441,
                            "process_2": 43208.89794966341
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 651.1659929434196,
                        "ram_power_avg": 0.7151409387588501,
                        "cpu_energy_total": 0.005385264647746226,
                        "gpu_energy_total": 0.04355747206816396,
                        "ram_energy_total": 3.2058000085555813e-05,
                        "total_energy_kwh": 0.04897479471599574,
                        "total_energy_joules": 176309.26097758464
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.0929276199625328,
                        "joules_per_token": 10.761063292088906,
                        "flops_per_joule": 298558238.04715675,
                        "joules_per_flop": 3.3494302704253357e-09
                    },
                    "per-process_emissions": [
                        0.004683043571652149,
                        0.004738708787083352,
                        0.004662854112232999,
                        0.004572341576090077
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0539": {
            "setup": {
                "experiment_id": "0539",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 03:12:05 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_300_temp_0.6",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 0.6,
                    "decoder_top_k": 300,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 43.369817316997796,
                        "average_latency_ms_per_batch": 5421.2271646247245,
                        "throughput_queries_per_sec": 2.951361290374478,
                        "throughput_tokens_per_sec": 377.7742451679332
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 37878759424,
                        "gpu_max_memory_reserved_bytes": 37878759424
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            27.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 53.6,
                        "cpu_memory_usage_bytes": 1975844864
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0539",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_2": 160.46916821177894,
                            "process_3": 1136.8989648923114,
                            "process_1": 527.0199076185828,
                            "process_0": 692.0215090670217
                        },
                        "ram_power": {
                            "process_2": 0.7327408790588379,
                            "process_3": 0.7321887016296387,
                            "process_1": 0.7245655059814453,
                            "process_0": 0.6888785362243652
                        },
                        "cpu_energy": {
                            "process_2": 0.0013367216846818337,
                            "process_3": 0.001247359468066861,
                            "process_1": 0.0013291147654417728,
                            "process_0": 0.0013513998140861076
                        },
                        "gpu_energy": {
                            "process_2": 0.01056181178277349,
                            "process_3": 0.010179263698960384,
                            "process_1": 0.010801280585463147,
                            "process_0": 0.010624359332815914
                        },
                        "ram_energy": {
                            "process_2": 7.989702130439573e-06,
                            "process_3": 7.566117882398778e-06,
                            "process_1": 8.18040006944602e-06,
                            "process_0": 7.568944402013902e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.011906523169585757,
                            "process_3": 0.011434189284909645,
                            "process_1": 0.012138575750974363,
                            "process_0": 0.011983328091304037
                        },
                        "total_energy_joules": {
                            "process_2": 42863.48341050873,
                            "process_3": 41163.081425674725,
                            "process_1": 43698.87270350771,
                            "process_0": 43139.98112869453
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 629.1023874474237,
                        "ram_power_avg": 0.7195934057235718,
                        "cpu_energy_total": 0.005264595732276575,
                        "gpu_energy_total": 0.04216671540001293,
                        "ram_energy_total": 3.130516448429827e-05,
                        "total_energy_kwh": 0.0474626162967738,
                        "total_energy_joules": 170865.4186683857
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09588833204334893,
                        "joules_per_token": 10.428797526146589,
                        "flops_per_joule": 308070425.9474795,
                        "joules_per_flop": 3.2460110279150332e-09
                    },
                    "per-process_emissions": [
                        0.004535790001453694,
                        0.00435585440808633,
                        0.004624190432333684,
                        0.004565048836382273
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0539": {
            "setup": {
                "experiment_id": "0539",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 03:12:05 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_300_temp_0.6",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 0.6,
                    "decoder_top_k": 300,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 43.369817316997796,
                        "average_latency_ms_per_batch": 5421.2271646247245,
                        "throughput_queries_per_sec": 2.951361290374478,
                        "throughput_tokens_per_sec": 377.7742451679332
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 37878759424,
                        "gpu_max_memory_reserved_bytes": 37878759424
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            27.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 53.6,
                        "cpu_memory_usage_bytes": 1975844864
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0539",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_2": 160.46916821177894,
                            "process_3": 1136.8989648923114,
                            "process_1": 527.0199076185828,
                            "process_0": 692.0215090670217
                        },
                        "ram_power": {
                            "process_2": 0.7327408790588379,
                            "process_3": 0.7321887016296387,
                            "process_1": 0.7245655059814453,
                            "process_0": 0.6888785362243652
                        },
                        "cpu_energy": {
                            "process_2": 0.0013367216846818337,
                            "process_3": 0.001247359468066861,
                            "process_1": 0.0013291147654417728,
                            "process_0": 0.0013513998140861076
                        },
                        "gpu_energy": {
                            "process_2": 0.01056181178277349,
                            "process_3": 0.010179263698960384,
                            "process_1": 0.010801280585463147,
                            "process_0": 0.010624359332815914
                        },
                        "ram_energy": {
                            "process_2": 7.989702130439573e-06,
                            "process_3": 7.566117882398778e-06,
                            "process_1": 8.18040006944602e-06,
                            "process_0": 7.568944402013902e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.011906523169585757,
                            "process_3": 0.011434189284909645,
                            "process_1": 0.012138575750974363,
                            "process_0": 0.011983328091304037
                        },
                        "total_energy_joules": {
                            "process_2": 42863.48341050873,
                            "process_3": 41163.081425674725,
                            "process_1": 43698.87270350771,
                            "process_0": 43139.98112869453
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 629.1023874474237,
                        "ram_power_avg": 0.7195934057235718,
                        "cpu_energy_total": 0.005264595732276575,
                        "gpu_energy_total": 0.04216671540001293,
                        "ram_energy_total": 3.130516448429827e-05,
                        "total_energy_kwh": 0.0474626162967738,
                        "total_energy_joules": 170865.4186683857
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09588833204334893,
                        "joules_per_token": 10.428797526146589,
                        "flops_per_joule": 308070425.9474795,
                        "joules_per_flop": 3.2460110279150332e-09
                    },
                    "per-process_emissions": [
                        0.004535790001453694,
                        0.00435585440808633,
                        0.004624190432333684,
                        0.004565048836382273
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0540": {
            "setup": {
                "experiment_id": "0540",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 03:13:42 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_24",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 24,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 36.969622927950695,
                        "average_latency_ms_per_batch": 6161.603821325116,
                        "throughput_queries_per_sec": 3.4623020161567903,
                        "throughput_tokens_per_sec": 443.17465806806916
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 39118176256,
                        "gpu_max_memory_reserved_bytes": 39118176256
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            27.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 53.7,
                        "cpu_memory_usage_bytes": 1972834304
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0540",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_1": 32.56837725064146,
                            "process_2": 2444.5967309334014,
                            "process_0": 44.23728914189299,
                            "process_3": 60.857619903750646
                        },
                        "ram_power": {
                            "process_1": 0.7295207977294922,
                            "process_2": 0.7309298515319824,
                            "process_0": 0.6867356300354004,
                            "process_3": 0.706216335296631
                        },
                        "cpu_energy": {
                            "process_1": 0.0011127373356866884,
                            "process_2": 0.0010734991310018815,
                            "process_0": 0.0011021786345081638,
                            "process_3": 0.0011229811120610975
                        },
                        "gpu_energy": {
                            "process_1": 0.008856603751942416,
                            "process_2": 0.008857900419648956,
                            "process_0": 0.009120003684886413,
                            "process_3": 0.009129663692613477
                        },
                        "ram_energy": {
                            "process_1": 6.471367987707079e-06,
                            "process_2": 6.965071176037e-06,
                            "process_0": 6.2768273910578385e-06,
                            "process_3": 6.622551358122147e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.00997581245561681,
                            "process_2": 0.009938364621826873,
                            "process_0": 0.010228459146785636,
                            "process_3": 0.010259267356032694
                        },
                        "total_energy_joules": {
                            "process_1": 35912.92484022052,
                            "process_2": 35778.11263857674,
                            "process_0": 36822.45292842829,
                            "process_3": 36933.3624817177
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 645.5650043074216,
                        "ram_power_avg": 0.7133506536483765,
                        "cpu_energy_total": 0.004411396213257831,
                        "gpu_energy_total": 0.03596417154909126,
                        "ram_energy_total": 2.6335817912924066e-05,
                        "total_energy_kwh": 0.04040190358026201,
                        "total_energy_joules": 145446.85288894325
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.11264595743786972,
                        "joules_per_token": 8.877371392147415,
                        "flops_per_joule": 361909393.45422953,
                        "joules_per_flop": 2.763122533116758e-09
                    },
                    "per-process_emissions": [
                        0.003800285754967224,
                        0.0037860200026849475,
                        0.0038965315119679884,
                        0.003908267899280655
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0540": {
            "setup": {
                "experiment_id": "0540",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 03:13:42 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_24",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 24,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 36.969622927950695,
                        "average_latency_ms_per_batch": 6161.603821325116,
                        "throughput_queries_per_sec": 3.4623020161567903,
                        "throughput_tokens_per_sec": 443.17465806806916
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 39118176256,
                        "gpu_max_memory_reserved_bytes": 39118176256
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            27.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 53.7,
                        "cpu_memory_usage_bytes": 1972834304
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0540",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_1": 32.56837725064146,
                            "process_2": 2444.5967309334014,
                            "process_0": 44.23728914189299,
                            "process_3": 60.857619903750646
                        },
                        "ram_power": {
                            "process_1": 0.7295207977294922,
                            "process_2": 0.7309298515319824,
                            "process_0": 0.6867356300354004,
                            "process_3": 0.706216335296631
                        },
                        "cpu_energy": {
                            "process_1": 0.0011127373356866884,
                            "process_2": 0.0010734991310018815,
                            "process_0": 0.0011021786345081638,
                            "process_3": 0.0011229811120610975
                        },
                        "gpu_energy": {
                            "process_1": 0.008856603751942416,
                            "process_2": 0.008857900419648956,
                            "process_0": 0.009120003684886413,
                            "process_3": 0.009129663692613477
                        },
                        "ram_energy": {
                            "process_1": 6.471367987707079e-06,
                            "process_2": 6.965071176037e-06,
                            "process_0": 6.2768273910578385e-06,
                            "process_3": 6.622551358122147e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.00997581245561681,
                            "process_2": 0.009938364621826873,
                            "process_0": 0.010228459146785636,
                            "process_3": 0.010259267356032694
                        },
                        "total_energy_joules": {
                            "process_1": 35912.92484022052,
                            "process_2": 35778.11263857674,
                            "process_0": 36822.45292842829,
                            "process_3": 36933.3624817177
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 645.5650043074216,
                        "ram_power_avg": 0.7133506536483765,
                        "cpu_energy_total": 0.004411396213257831,
                        "gpu_energy_total": 0.03596417154909126,
                        "ram_energy_total": 2.6335817912924066e-05,
                        "total_energy_kwh": 0.04040190358026201,
                        "total_energy_joules": 145446.85288894325
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.11264595743786972,
                        "joules_per_token": 8.877371392147415,
                        "flops_per_joule": 361909393.45422953,
                        "joules_per_flop": 2.763122533116758e-09
                    },
                    "per-process_emissions": [
                        0.003800285754967224,
                        0.0037860200026849475,
                        0.0038965315119679884,
                        0.003908267899280655
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0541": {
            "setup": {
                "experiment_id": "0541",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 03:15:34 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_topp_0.8_temp_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_p",
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": 0.8
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 42.18166609900072,
                        "average_latency_ms_per_batch": 5272.70826237509,
                        "throughput_queries_per_sec": 3.034493699219536,
                        "throughput_tokens_per_sec": 388.4151935001006
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38403047424,
                        "gpu_max_memory_reserved_bytes": 38403047424
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            27.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 53.8,
                        "cpu_memory_usage_bytes": 1980665856
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0541",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_3": 824.6239442963572,
                            "process_0": 175.5497614015274,
                            "process_2": 2936.976765755806,
                            "process_1": 459.4199881030602
                        },
                        "ram_power": {
                            "process_3": 0.7263121604919435,
                            "process_0": 0.6893577575683594,
                            "process_2": 0.7107367515563966,
                            "process_1": 0.7331886291503906
                        },
                        "cpu_energy": {
                            "process_3": 0.0012977024226856883,
                            "process_0": 0.0013173984523746189,
                            "process_2": 0.0011653473320002382,
                            "process_1": 0.001275697424876853
                        },
                        "gpu_energy": {
                            "process_3": 0.010852054792751531,
                            "process_0": 0.01052330758531106,
                            "process_2": 0.010576737628051447,
                            "process_1": 0.010939557362746655
                        },
                        "ram_energy": {
                            "process_3": 7.987332427929127e-06,
                            "process_0": 7.53247272168058e-06,
                            "process_2": 7.937740825521188e-06,
                            "process_1": 7.965416950399595e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.01215774454786515,
                            "process_0": 0.011848238510407356,
                            "process_2": 0.011750022700877202,
                            "process_1": 0.012223220204573908
                        },
                        "total_energy_joules": {
                            "process_3": 43767.88037231454,
                            "process_0": 42653.658637466484,
                            "process_2": 42300.08172315793,
                            "process_1": 44003.59273646607
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1099.1426148891876,
                        "ram_power_avg": 0.7148988246917725,
                        "cpu_energy_total": 0.005056145631937398,
                        "gpu_energy_total": 0.04289165736886069,
                        "ram_energy_total": 3.1422962925530496e-05,
                        "total_energy_kwh": 0.047979225963723615,
                        "total_energy_joules": 172725.21346940502
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09485586771558463,
                        "joules_per_token": 10.54231039241974,
                        "flops_per_joule": 304753320.3262645,
                        "joules_per_flop": 3.2813424278016546e-09
                    },
                    "per-process_emissions": [
                        0.004631492785509229,
                        0.004513586460539682,
                        0.0044761711478991705,
                        0.0046564357369324304
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0541": {
            "setup": {
                "experiment_id": "0541",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 03:15:34 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_topp_0.8_temp_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_p",
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": 0.8
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 42.18166609900072,
                        "average_latency_ms_per_batch": 5272.70826237509,
                        "throughput_queries_per_sec": 3.034493699219536,
                        "throughput_tokens_per_sec": 388.4151935001006
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38403047424,
                        "gpu_max_memory_reserved_bytes": 38403047424
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            27.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 53.8,
                        "cpu_memory_usage_bytes": 1980665856
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0541",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_3": 824.6239442963572,
                            "process_0": 175.5497614015274,
                            "process_2": 2936.976765755806,
                            "process_1": 459.4199881030602
                        },
                        "ram_power": {
                            "process_3": 0.7263121604919435,
                            "process_0": 0.6893577575683594,
                            "process_2": 0.7107367515563966,
                            "process_1": 0.7331886291503906
                        },
                        "cpu_energy": {
                            "process_3": 0.0012977024226856883,
                            "process_0": 0.0013173984523746189,
                            "process_2": 0.0011653473320002382,
                            "process_1": 0.001275697424876853
                        },
                        "gpu_energy": {
                            "process_3": 0.010852054792751531,
                            "process_0": 0.01052330758531106,
                            "process_2": 0.010576737628051447,
                            "process_1": 0.010939557362746655
                        },
                        "ram_energy": {
                            "process_3": 7.987332427929127e-06,
                            "process_0": 7.53247272168058e-06,
                            "process_2": 7.937740825521188e-06,
                            "process_1": 7.965416950399595e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.01215774454786515,
                            "process_0": 0.011848238510407356,
                            "process_2": 0.011750022700877202,
                            "process_1": 0.012223220204573908
                        },
                        "total_energy_joules": {
                            "process_3": 43767.88037231454,
                            "process_0": 42653.658637466484,
                            "process_2": 42300.08172315793,
                            "process_1": 44003.59273646607
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1099.1426148891876,
                        "ram_power_avg": 0.7148988246917725,
                        "cpu_energy_total": 0.005056145631937398,
                        "gpu_energy_total": 0.04289165736886069,
                        "ram_energy_total": 3.1422962925530496e-05,
                        "total_energy_kwh": 0.047979225963723615,
                        "total_energy_joules": 172725.21346940502
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09485586771558463,
                        "joules_per_token": 10.54231039241974,
                        "flops_per_joule": 304753320.3262645,
                        "joules_per_flop": 3.2813424278016546e-09
                    },
                    "per-process_emissions": [
                        0.004631492785509229,
                        0.004513586460539682,
                        0.0044761711478991705,
                        0.0046564357369324304
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0542": {
            "setup": {
                "experiment_id": "0542",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 03:17:21 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_burst_0.4_0.5_6.0_10",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.4,
                    "delay_max": 0.5,
                    "simulate_burst": true,
                    "burst_interval": 6.0,
                    "burst_size": 10
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 46.487412640068214,
                        "average_latency_ms_per_batch": 5810.926580008527,
                        "throughput_queries_per_sec": 2.7534335152409586,
                        "throughput_tokens_per_sec": 352.4394899508427
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38352715776,
                        "gpu_max_memory_reserved_bytes": 38352715776
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            28.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.1,
                        "cpu_memory_usage_bytes": 1972682752
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0542",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_3": 0.0,
                            "process_1": 138.668644503093,
                            "process_2": 1093.6928324595806,
                            "process_0": 1875.41774230331
                        },
                        "ram_power": {
                            "process_3": 0.7064323425292969,
                            "process_1": 0.7306051254272461,
                            "process_2": 0.7073850631713868,
                            "process_0": 0.687769889831543
                        },
                        "cpu_energy": {
                            "process_3": 0.0014517634571257073,
                            "process_1": 0.0014727923663394905,
                            "process_2": 0.001347467346062331,
                            "process_0": 0.0013896918168520638
                        },
                        "gpu_energy": {
                            "process_3": 0.010889864545220007,
                            "process_1": 0.01084970117975459,
                            "process_2": 0.010511903131737554,
                            "process_0": 0.010791427244246421
                        },
                        "ram_energy": {
                            "process_3": 8.386982039692484e-06,
                            "process_1": 8.54932692027936e-06,
                            "process_2": 8.271179644287465e-06,
                            "process_0": 7.91973523969317e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.012350014984385404,
                            "process_1": 0.012331042873014356,
                            "process_2": 0.011867641657444175,
                            "process_0": 0.01218903879633818
                        },
                        "total_energy_joules": {
                            "process_3": 44460.053943787454,
                            "process_1": 44391.75434285168,
                            "process_2": 42723.50996679903,
                            "process_0": 43880.53966681745
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 776.9448048164959,
                        "ram_power_avg": 0.7080481052398682,
                        "cpu_energy_total": 0.005661714986379593,
                        "gpu_energy_total": 0.04304289610095857,
                        "ram_energy_total": 3.3127223843952485e-05,
                        "total_energy_kwh": 0.04873773831118212,
                        "total_energy_joules": 175455.85792025563
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09337961236635654,
                        "joules_per_token": 10.708975703140602,
                        "flops_per_joule": 300010401.09352255,
                        "joules_per_flop": 3.3332177696341566e-09
                    },
                    "per-process_emissions": [
                        0.00470473820830162,
                        0.004697510782474819,
                        0.004520978089403359,
                        0.00464341432946503
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0542": {
            "setup": {
                "experiment_id": "0542",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 03:17:21 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_burst_0.4_0.5_6.0_10",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.4,
                    "delay_max": 0.5,
                    "simulate_burst": true,
                    "burst_interval": 6.0,
                    "burst_size": 10
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 46.487412640068214,
                        "average_latency_ms_per_batch": 5810.926580008527,
                        "throughput_queries_per_sec": 2.7534335152409586,
                        "throughput_tokens_per_sec": 352.4394899508427
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38352715776,
                        "gpu_max_memory_reserved_bytes": 38352715776
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            28.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.1,
                        "cpu_memory_usage_bytes": 1972682752
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0542",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_3": 0.0,
                            "process_1": 138.668644503093,
                            "process_2": 1093.6928324595806,
                            "process_0": 1875.41774230331
                        },
                        "ram_power": {
                            "process_3": 0.7064323425292969,
                            "process_1": 0.7306051254272461,
                            "process_2": 0.7073850631713868,
                            "process_0": 0.687769889831543
                        },
                        "cpu_energy": {
                            "process_3": 0.0014517634571257073,
                            "process_1": 0.0014727923663394905,
                            "process_2": 0.001347467346062331,
                            "process_0": 0.0013896918168520638
                        },
                        "gpu_energy": {
                            "process_3": 0.010889864545220007,
                            "process_1": 0.01084970117975459,
                            "process_2": 0.010511903131737554,
                            "process_0": 0.010791427244246421
                        },
                        "ram_energy": {
                            "process_3": 8.386982039692484e-06,
                            "process_1": 8.54932692027936e-06,
                            "process_2": 8.271179644287465e-06,
                            "process_0": 7.91973523969317e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.012350014984385404,
                            "process_1": 0.012331042873014356,
                            "process_2": 0.011867641657444175,
                            "process_0": 0.01218903879633818
                        },
                        "total_energy_joules": {
                            "process_3": 44460.053943787454,
                            "process_1": 44391.75434285168,
                            "process_2": 42723.50996679903,
                            "process_0": 43880.53966681745
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 776.9448048164959,
                        "ram_power_avg": 0.7080481052398682,
                        "cpu_energy_total": 0.005661714986379593,
                        "gpu_energy_total": 0.04304289610095857,
                        "ram_energy_total": 3.3127223843952485e-05,
                        "total_energy_kwh": 0.04873773831118212,
                        "total_energy_joules": 175455.85792025563
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09337961236635654,
                        "joules_per_token": 10.708975703140602,
                        "flops_per_joule": 300010401.09352255,
                        "joules_per_flop": 3.3332177696341566e-09
                    },
                    "per-process_emissions": [
                        0.00470473820830162,
                        0.004697510782474819,
                        0.004520978089403359,
                        0.00464341432946503
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0543": {
            "setup": {
                "experiment_id": "0543",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 03:20:28 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 4,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 133.16418913204689,
                        "average_latency_ms_per_batch": 4161.380910376465,
                        "throughput_queries_per_sec": 0.9612193851387025,
                        "throughput_tokens_per_sec": 123.03608129775392
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38004588544,
                        "gpu_max_memory_reserved_bytes": 38004588544
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            51.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.7,
                        "cpu_memory_usage_bytes": 1962975232
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0543",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_1": 148.42847367749246,
                            "process_3": 152.7691114932203,
                            "process_0": 563.6244964412934,
                            "process_2": 88.55683112312293
                        },
                        "ram_power": {
                            "process_1": 0.7044596672058105,
                            "process_3": 0.7060089111328125,
                            "process_0": 0.6843852996826172,
                            "process_2": 0.7215571403503418
                        },
                        "cpu_energy": {
                            "process_1": 0.003454070234502068,
                            "process_3": 0.003521012388146119,
                            "process_0": 0.0038467065727254535,
                            "process_2": 0.0035319954026545026
                        },
                        "gpu_energy": {
                            "process_1": 0.025232659352779407,
                            "process_3": 0.025212882948070003,
                            "process_0": 0.0268732789986057,
                            "process_2": 0.025190390152298292
                        },
                        "ram_energy": {
                            "process_1": 2.1229179880661666e-05,
                            "process_3": 2.0972002663213158e-05,
                            "process_0": 2.2330036032447582e-05,
                            "process_2": 2.1532311096416457e-05
                        },
                        "total_energy_kwh": {
                            "process_1": 0.028707958767162152,
                            "process_3": 0.028754867338879324,
                            "process_0": 0.03074231560736359,
                            "process_2": 0.028743917866049216
                        },
                        "total_energy_joules": {
                            "process_1": 103348.65156178374,
                            "process_3": 103517.52241996556,
                            "process_0": 110672.33618650892,
                            "process_2": 103478.10431777718
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 238.34472818378228,
                        "ram_power_avg": 0.7041027545928955,
                        "cpu_energy_total": 0.014353784598028144,
                        "gpu_energy_total": 0.1025092114517534,
                        "ram_energy_total": 8.606352967273886e-05,
                        "total_energy_kwh": 0.11694905957945428,
                        "total_energy_joules": 421016.6144860354
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.03891532884040954,
                        "joules_per_token": 25.696814849001186,
                        "flops_per_joule": 125027327.89565471,
                        "joules_per_flop": 7.998251396963229e-09
                    },
                    "per-process_emissions": [
                        0.010936296892350423,
                        0.010954166712746079,
                        0.01171128513062516,
                        0.01094999551107145
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0543": {
            "setup": {
                "experiment_id": "0543",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 03:20:28 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 4,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 133.16418913204689,
                        "average_latency_ms_per_batch": 4161.380910376465,
                        "throughput_queries_per_sec": 0.9612193851387025,
                        "throughput_tokens_per_sec": 123.03608129775392
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38004588544,
                        "gpu_max_memory_reserved_bytes": 38004588544
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            51.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.7,
                        "cpu_memory_usage_bytes": 1962975232
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0543",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_1": 148.42847367749246,
                            "process_3": 152.7691114932203,
                            "process_0": 563.6244964412934,
                            "process_2": 88.55683112312293
                        },
                        "ram_power": {
                            "process_1": 0.7044596672058105,
                            "process_3": 0.7060089111328125,
                            "process_0": 0.6843852996826172,
                            "process_2": 0.7215571403503418
                        },
                        "cpu_energy": {
                            "process_1": 0.003454070234502068,
                            "process_3": 0.003521012388146119,
                            "process_0": 0.0038467065727254535,
                            "process_2": 0.0035319954026545026
                        },
                        "gpu_energy": {
                            "process_1": 0.025232659352779407,
                            "process_3": 0.025212882948070003,
                            "process_0": 0.0268732789986057,
                            "process_2": 0.025190390152298292
                        },
                        "ram_energy": {
                            "process_1": 2.1229179880661666e-05,
                            "process_3": 2.0972002663213158e-05,
                            "process_0": 2.2330036032447582e-05,
                            "process_2": 2.1532311096416457e-05
                        },
                        "total_energy_kwh": {
                            "process_1": 0.028707958767162152,
                            "process_3": 0.028754867338879324,
                            "process_0": 0.03074231560736359,
                            "process_2": 0.028743917866049216
                        },
                        "total_energy_joules": {
                            "process_1": 103348.65156178374,
                            "process_3": 103517.52241996556,
                            "process_0": 110672.33618650892,
                            "process_2": 103478.10431777718
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 238.34472818378228,
                        "ram_power_avg": 0.7041027545928955,
                        "cpu_energy_total": 0.014353784598028144,
                        "gpu_energy_total": 0.1025092114517534,
                        "ram_energy_total": 8.606352967273886e-05,
                        "total_energy_kwh": 0.11694905957945428,
                        "total_energy_joules": 421016.6144860354
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.03891532884040954,
                        "joules_per_token": 25.696814849001186,
                        "flops_per_joule": 125027327.89565471,
                        "joules_per_flop": 7.998251396963229e-09
                    },
                    "per-process_emissions": [
                        0.010936296892350423,
                        0.010954166712746079,
                        0.01171128513062516,
                        0.01094999551107145
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0544": {
            "setup": {
                "experiment_id": "0544",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 03:22:15 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_burst_0.05_0.1_2.0_5",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.05,
                    "delay_max": 0.1,
                    "simulate_burst": true,
                    "burst_interval": 2.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 53.772685479954816,
                        "average_latency_ms_per_batch": 6721.585684994352,
                        "throughput_queries_per_sec": 2.3803906919939,
                        "throughput_tokens_per_sec": 304.6900085752192
                    }
                },
                "compute_metrics": {
                    "flops": 0.0,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25714405888,
                        "gpu_max_memory_allocated_bytes": 25714405888,
                        "gpu_current_memory_reserved_bytes": 38327549952,
                        "gpu_max_memory_reserved_bytes": 38327549952
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            22.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 5.0,
                        "cpu_memory_usage_bytes": 1971597312
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0544",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 736.4443213999763,
                            "process_2": 963.0336101289349,
                            "process_3": 837.3912362590992,
                            "process_1": 847.3925372282431
                        },
                        "ram_power": {
                            "process_0": 0.6859216690063477,
                            "process_2": 0.7309398651123047,
                            "process_3": 0.7229347229003906,
                            "process_1": 0.7298240661621095
                        },
                        "cpu_energy": {
                            "process_0": 0.0015149779264720567,
                            "process_2": 0.001172462823007663,
                            "process_3": 0.0011946314909655486,
                            "process_1": 0.0011347437871609143
                        },
                        "gpu_energy": {
                            "process_0": 0.011507629206096937,
                            "process_2": 0.009934432669762572,
                            "process_3": 0.010016660791096754,
                            "process_1": 0.009999849110984727
                        },
                        "ram_energy": {
                            "process_0": 8.764714156277778e-06,
                            "process_2": 7.682259572644824e-06,
                            "process_3": 6.965031600717199e-06,
                            "process_1": 7.399920626769229e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.013031371846725277,
                            "process_2": 0.011114577752342886,
                            "process_3": 0.011218257313663025,
                            "process_1": 0.011141992818772407
                        },
                        "total_energy_joules": {
                            "process_0": 46912.938648211,
                            "process_2": 40012.47990843439,
                            "process_3": 40385.72632918689,
                            "process_1": 40111.17414758066
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 846.0654262540634,
                        "ram_power_avg": 0.7174050807952881,
                        "cpu_energy_total": 0.005016816027606182,
                        "gpu_energy_total": 0.04145857177794099,
                        "ram_energy_total": 3.081192595640903e-05,
                        "total_energy_kwh": 0.0465061997315036,
                        "total_energy_joules": 167422.31903341296
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09786030975195247,
                        "joules_per_token": 10.218647401941709,
                        "flops_per_joule": 0.0,
                        "joules_per_flop": 0
                    },
                    "per-process_emissions": [
                        0.004964301105009995,
                        0.0042340983947550224,
                        0.00427359512363993,
                        0.004244542164311349
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0544": {
            "setup": {
                "experiment_id": "0544",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 03:22:15 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_burst_0.05_0.1_2.0_5",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.05,
                    "delay_max": 0.1,
                    "simulate_burst": true,
                    "burst_interval": 2.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 53.772685479954816,
                        "average_latency_ms_per_batch": 6721.585684994352,
                        "throughput_queries_per_sec": 2.3803906919939,
                        "throughput_tokens_per_sec": 304.6900085752192
                    }
                },
                "compute_metrics": {
                    "flops": 0.0,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25714405888,
                        "gpu_max_memory_allocated_bytes": 25714405888,
                        "gpu_current_memory_reserved_bytes": 38327549952,
                        "gpu_max_memory_reserved_bytes": 38327549952
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            22.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 5.0,
                        "cpu_memory_usage_bytes": 1971597312
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0544",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 736.4443213999763,
                            "process_2": 963.0336101289349,
                            "process_3": 837.3912362590992,
                            "process_1": 847.3925372282431
                        },
                        "ram_power": {
                            "process_0": 0.6859216690063477,
                            "process_2": 0.7309398651123047,
                            "process_3": 0.7229347229003906,
                            "process_1": 0.7298240661621095
                        },
                        "cpu_energy": {
                            "process_0": 0.0015149779264720567,
                            "process_2": 0.001172462823007663,
                            "process_3": 0.0011946314909655486,
                            "process_1": 0.0011347437871609143
                        },
                        "gpu_energy": {
                            "process_0": 0.011507629206096937,
                            "process_2": 0.009934432669762572,
                            "process_3": 0.010016660791096754,
                            "process_1": 0.009999849110984727
                        },
                        "ram_energy": {
                            "process_0": 8.764714156277778e-06,
                            "process_2": 7.682259572644824e-06,
                            "process_3": 6.965031600717199e-06,
                            "process_1": 7.399920626769229e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.013031371846725277,
                            "process_2": 0.011114577752342886,
                            "process_3": 0.011218257313663025,
                            "process_1": 0.011141992818772407
                        },
                        "total_energy_joules": {
                            "process_0": 46912.938648211,
                            "process_2": 40012.47990843439,
                            "process_3": 40385.72632918689,
                            "process_1": 40111.17414758066
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 846.0654262540634,
                        "ram_power_avg": 0.7174050807952881,
                        "cpu_energy_total": 0.005016816027606182,
                        "gpu_energy_total": 0.04145857177794099,
                        "ram_energy_total": 3.081192595640903e-05,
                        "total_energy_kwh": 0.0465061997315036,
                        "total_energy_joules": 167422.31903341296
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09786030975195247,
                        "joules_per_token": 10.218647401941709,
                        "flops_per_joule": 0.0,
                        "joules_per_flop": 0
                    },
                    "per-process_emissions": [
                        0.004964301105009995,
                        0.0042340983947550224,
                        0.00427359512363993,
                        0.004244542164311349
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0545": {
            "setup": {
                "experiment_id": "0545",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 03:23:59 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_20_temp_1.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 1.2,
                    "decoder_top_k": 20,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 51.27657594101038,
                        "average_latency_ms_per_batch": 6409.571992626297,
                        "throughput_queries_per_sec": 2.496266524255711,
                        "throughput_tokens_per_sec": 319.52211510473103
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38182846464,
                        "gpu_max_memory_reserved_bytes": 38182846464
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            44.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.6,
                        "cpu_memory_usage_bytes": 1957728256
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0545",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 94.31793741790202,
                            "process_2": 6189.881920894438,
                            "process_1": 895.447522507353,
                            "process_3": 944.4921068644298
                        },
                        "ram_power": {
                            "process_0": 0.6825442314147949,
                            "process_2": 0.733107089996338,
                            "process_1": 0.7329440116882324,
                            "process_3": 0.7320671081542969
                        },
                        "cpu_energy": {
                            "process_0": 0.0015484289627420368,
                            "process_2": 0.001185756376657082,
                            "process_1": 0.0011922827934140517,
                            "process_3": 0.0012000134470290505
                        },
                        "gpu_energy": {
                            "process_0": 0.011368241594585271,
                            "process_2": 0.00981477701848199,
                            "process_1": 0.009856649551981178,
                            "process_3": 0.009860463443919443
                        },
                        "ram_energy": {
                            "process_0": 8.247155180174253e-06,
                            "process_2": 6.651885253073871e-06,
                            "process_1": 6.689599410204822e-06,
                            "process_3": 6.7463895124196595e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.012924917712507486,
                            "process_2": 0.011007185280392146,
                            "process_1": 0.011055621944805432,
                            "process_3": 0.011067223280460914
                        },
                        "total_energy_joules": {
                            "process_0": 46529.70376502695,
                            "process_2": 39625.867009411726,
                            "process_1": 39800.23900129955,
                            "process_3": 39842.00380965929
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 2031.0348719210306,
                        "ram_power_avg": 0.7201656103134155,
                        "cpu_energy_total": 0.0051264815798422216,
                        "gpu_energy_total": 0.04090013160896788,
                        "ram_energy_total": 2.8335029355872604e-05,
                        "total_energy_kwh": 0.04605494821816598,
                        "total_energy_joules": 165797.8135853975
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09881915596891205,
                        "joules_per_token": 10.119495458093109,
                        "flops_per_joule": 317486589.06018347,
                        "joules_per_flop": 3.1497393416212544e-09
                    },
                    "per-process_emissions": [
                        0.004923747402579727,
                        0.004193187232565388,
                        0.0042116391798736295,
                        0.004216058708691585
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0545": {
            "setup": {
                "experiment_id": "0545",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 03:23:59 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_20_temp_1.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 1.2,
                    "decoder_top_k": 20,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 51.27657594101038,
                        "average_latency_ms_per_batch": 6409.571992626297,
                        "throughput_queries_per_sec": 2.496266524255711,
                        "throughput_tokens_per_sec": 319.52211510473103
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38182846464,
                        "gpu_max_memory_reserved_bytes": 38182846464
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            44.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.6,
                        "cpu_memory_usage_bytes": 1957728256
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0545",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 94.31793741790202,
                            "process_2": 6189.881920894438,
                            "process_1": 895.447522507353,
                            "process_3": 944.4921068644298
                        },
                        "ram_power": {
                            "process_0": 0.6825442314147949,
                            "process_2": 0.733107089996338,
                            "process_1": 0.7329440116882324,
                            "process_3": 0.7320671081542969
                        },
                        "cpu_energy": {
                            "process_0": 0.0015484289627420368,
                            "process_2": 0.001185756376657082,
                            "process_1": 0.0011922827934140517,
                            "process_3": 0.0012000134470290505
                        },
                        "gpu_energy": {
                            "process_0": 0.011368241594585271,
                            "process_2": 0.00981477701848199,
                            "process_1": 0.009856649551981178,
                            "process_3": 0.009860463443919443
                        },
                        "ram_energy": {
                            "process_0": 8.247155180174253e-06,
                            "process_2": 6.651885253073871e-06,
                            "process_1": 6.689599410204822e-06,
                            "process_3": 6.7463895124196595e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.012924917712507486,
                            "process_2": 0.011007185280392146,
                            "process_1": 0.011055621944805432,
                            "process_3": 0.011067223280460914
                        },
                        "total_energy_joules": {
                            "process_0": 46529.70376502695,
                            "process_2": 39625.867009411726,
                            "process_1": 39800.23900129955,
                            "process_3": 39842.00380965929
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 2031.0348719210306,
                        "ram_power_avg": 0.7201656103134155,
                        "cpu_energy_total": 0.0051264815798422216,
                        "gpu_energy_total": 0.04090013160896788,
                        "ram_energy_total": 2.8335029355872604e-05,
                        "total_energy_kwh": 0.04605494821816598,
                        "total_energy_joules": 165797.8135853975
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09881915596891205,
                        "joules_per_token": 10.119495458093109,
                        "flops_per_joule": 317486589.06018347,
                        "joules_per_flop": 3.1497393416212544e-09
                    },
                    "per-process_emissions": [
                        0.004923747402579727,
                        0.004193187232565388,
                        0.0042116391798736295,
                        0.004216058708691585
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0546": {
            "setup": {
                "experiment_id": "0546",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 03:25:32 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_20_temp_0.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 0.2,
                    "decoder_top_k": 20,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.498330284957774,
                        "average_latency_ms_per_batch": 4812.291285619722,
                        "throughput_queries_per_sec": 3.324819519510764,
                        "throughput_tokens_per_sec": 425.5768984973778
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38182846464,
                        "gpu_max_memory_reserved_bytes": 38182846464
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            18.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.6,
                        "cpu_memory_usage_bytes": 1976172544
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0546",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_3": 155.15156203294524,
                            "process_1": 177.43134316590528,
                            "process_2": 185.9694961843353,
                            "process_0": 182.18571356345646
                        },
                        "ram_power": {
                            "process_3": 0.7093477249145509,
                            "process_1": 0.732297420501709,
                            "process_2": 0.7325735092163086,
                            "process_0": 0.6890101432800293
                        },
                        "cpu_energy": {
                            "process_3": 0.0012259252512230884,
                            "process_1": 0.0011548348905289457,
                            "process_2": 0.0012494641929642965,
                            "process_0": 0.0012193929065215342
                        },
                        "gpu_energy": {
                            "process_3": 0.010258587929085294,
                            "process_1": 0.01025499987065892,
                            "process_2": 0.010257484317090615,
                            "process_0": 0.010258587929085294
                        },
                        "ram_energy": {
                            "process_3": 7.427388168596528e-06,
                            "process_1": 7.245728168491428e-06,
                            "process_2": 7.413799714632471e-06,
                            "process_0": 6.820965739832449e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.01149194056847698,
                            "process_1": 0.011417080489356359,
                            "process_2": 0.011514362309769547,
                            "process_0": 0.011484801801346662
                        },
                        "total_energy_joules": {
                            "process_3": 41370.98604651713,
                            "process_1": 41101.48976168289,
                            "process_2": 41451.70431517037,
                            "process_0": 41345.28648484798
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 175.18452873666058,
                        "ram_power_avg": 0.7158071994781494,
                        "cpu_energy_total": 0.004849617241237865,
                        "gpu_energy_total": 0.041029660045920124,
                        "ram_energy_total": 2.8907881791552875e-05,
                        "total_energy_kwh": 0.04590818516894954,
                        "total_energy_joules": 165269.46660821838
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09913506914643404,
                        "joules_per_token": 10.087247717786767,
                        "flops_per_joule": 318501556.2109065,
                        "joules_per_flop": 3.139702084651092e-09
                    },
                    "per-process_emissions": [
                        0.004377854759561306,
                        0.004349336812420305,
                        0.0043863963219067095,
                        0.004375135246223011
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0546": {
            "setup": {
                "experiment_id": "0546",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 03:25:32 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_20_temp_0.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 0.2,
                    "decoder_top_k": 20,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.498330284957774,
                        "average_latency_ms_per_batch": 4812.291285619722,
                        "throughput_queries_per_sec": 3.324819519510764,
                        "throughput_tokens_per_sec": 425.5768984973778
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38182846464,
                        "gpu_max_memory_reserved_bytes": 38182846464
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            18.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.6,
                        "cpu_memory_usage_bytes": 1976172544
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0546",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_3": 155.15156203294524,
                            "process_1": 177.43134316590528,
                            "process_2": 185.9694961843353,
                            "process_0": 182.18571356345646
                        },
                        "ram_power": {
                            "process_3": 0.7093477249145509,
                            "process_1": 0.732297420501709,
                            "process_2": 0.7325735092163086,
                            "process_0": 0.6890101432800293
                        },
                        "cpu_energy": {
                            "process_3": 0.0012259252512230884,
                            "process_1": 0.0011548348905289457,
                            "process_2": 0.0012494641929642965,
                            "process_0": 0.0012193929065215342
                        },
                        "gpu_energy": {
                            "process_3": 0.010258587929085294,
                            "process_1": 0.01025499987065892,
                            "process_2": 0.010257484317090615,
                            "process_0": 0.010258587929085294
                        },
                        "ram_energy": {
                            "process_3": 7.427388168596528e-06,
                            "process_1": 7.245728168491428e-06,
                            "process_2": 7.413799714632471e-06,
                            "process_0": 6.820965739832449e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.01149194056847698,
                            "process_1": 0.011417080489356359,
                            "process_2": 0.011514362309769547,
                            "process_0": 0.011484801801346662
                        },
                        "total_energy_joules": {
                            "process_3": 41370.98604651713,
                            "process_1": 41101.48976168289,
                            "process_2": 41451.70431517037,
                            "process_0": 41345.28648484798
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 175.18452873666058,
                        "ram_power_avg": 0.7158071994781494,
                        "cpu_energy_total": 0.004849617241237865,
                        "gpu_energy_total": 0.041029660045920124,
                        "ram_energy_total": 2.8907881791552875e-05,
                        "total_energy_kwh": 0.04590818516894954,
                        "total_energy_joules": 165269.46660821838
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09913506914643404,
                        "joules_per_token": 10.087247717786767,
                        "flops_per_joule": 318501556.2109065,
                        "joules_per_flop": 3.139702084651092e-09
                    },
                    "per-process_emissions": [
                        0.004377854759561306,
                        0.004349336812420305,
                        0.0043863963219067095,
                        0.004375135246223011
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0547": {
            "setup": {
                "experiment_id": "0547",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 03:27:06 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_burst_0.05_0.1_2.0_8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.05,
                    "delay_max": 0.1,
                    "simulate_burst": true,
                    "burst_interval": 2.0,
                    "burst_size": 8
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 40.81896623398643,
                        "average_latency_ms_per_batch": 5102.370779248304,
                        "throughput_queries_per_sec": 3.135797199426022,
                        "throughput_tokens_per_sec": 401.3820415265308
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38352715776,
                        "gpu_max_memory_reserved_bytes": 38352715776
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            6.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.8,
                        "cpu_memory_usage_bytes": 1973235712
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0547",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 2717.519874319969,
                            "process_3": 622.4799948196635,
                            "process_2": 3326.70506454989,
                            "process_1": 2018.7978598815093
                        },
                        "ram_power": {
                            "process_0": 0.6879658699035645,
                            "process_3": 0.7235569953918457,
                            "process_2": 0.7234382629394531,
                            "process_1": 0.7307968139648438
                        },
                        "cpu_energy": {
                            "process_0": 0.0011199675256575573,
                            "process_3": 0.001108487642746695,
                            "process_2": 0.0011572032816911813,
                            "process_1": 0.001100555394190451
                        },
                        "gpu_energy": {
                            "process_0": 0.010349764113138704,
                            "process_3": 0.010386714420477006,
                            "process_2": 0.010339554938303408,
                            "process_1": 0.010374329132787352
                        },
                        "ram_energy": {
                            "process_0": 6.938438559059701e-06,
                            "process_3": 6.89754221142116e-06,
                            "process_2": 7.523856375725555e-06,
                            "process_1": 7.241479634423726e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.011476670077355316,
                            "process_3": 0.011502099605435122,
                            "process_2": 0.011504282076370321,
                            "process_1": 0.011482126006612227
                        },
                        "total_energy_joules": {
                            "process_0": 41316.01227847914,
                            "process_3": 41407.55857956644,
                            "process_2": 41415.41547493316,
                            "process_1": 41335.65362380401
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 2171.375698392758,
                        "ram_power_avg": 0.7164394855499268,
                        "cpu_energy_total": 0.0044862138442858844,
                        "gpu_energy_total": 0.04145036260470647,
                        "ram_energy_total": 2.8601316780630142e-05,
                        "total_energy_kwh": 0.04596517776577299,
                        "total_energy_joules": 165474.63995678275
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09901215076992483,
                        "joules_per_token": 10.099770505174728,
                        "flops_per_joule": 318106643.547384,
                        "joules_per_flop": 3.143599859620799e-09
                    },
                    "per-process_emissions": [
                        0.004372037465968508,
                        0.00438172484469051,
                        0.004382556256993274,
                        0.004374115902218928
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0547": {
            "setup": {
                "experiment_id": "0547",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 03:27:06 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_burst_0.05_0.1_2.0_8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.05,
                    "delay_max": 0.1,
                    "simulate_burst": true,
                    "burst_interval": 2.0,
                    "burst_size": 8
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 40.81896623398643,
                        "average_latency_ms_per_batch": 5102.370779248304,
                        "throughput_queries_per_sec": 3.135797199426022,
                        "throughput_tokens_per_sec": 401.3820415265308
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38352715776,
                        "gpu_max_memory_reserved_bytes": 38352715776
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            6.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.8,
                        "cpu_memory_usage_bytes": 1973235712
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0547",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 2717.519874319969,
                            "process_3": 622.4799948196635,
                            "process_2": 3326.70506454989,
                            "process_1": 2018.7978598815093
                        },
                        "ram_power": {
                            "process_0": 0.6879658699035645,
                            "process_3": 0.7235569953918457,
                            "process_2": 0.7234382629394531,
                            "process_1": 0.7307968139648438
                        },
                        "cpu_energy": {
                            "process_0": 0.0011199675256575573,
                            "process_3": 0.001108487642746695,
                            "process_2": 0.0011572032816911813,
                            "process_1": 0.001100555394190451
                        },
                        "gpu_energy": {
                            "process_0": 0.010349764113138704,
                            "process_3": 0.010386714420477006,
                            "process_2": 0.010339554938303408,
                            "process_1": 0.010374329132787352
                        },
                        "ram_energy": {
                            "process_0": 6.938438559059701e-06,
                            "process_3": 6.89754221142116e-06,
                            "process_2": 7.523856375725555e-06,
                            "process_1": 7.241479634423726e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.011476670077355316,
                            "process_3": 0.011502099605435122,
                            "process_2": 0.011504282076370321,
                            "process_1": 0.011482126006612227
                        },
                        "total_energy_joules": {
                            "process_0": 41316.01227847914,
                            "process_3": 41407.55857956644,
                            "process_2": 41415.41547493316,
                            "process_1": 41335.65362380401
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 2171.375698392758,
                        "ram_power_avg": 0.7164394855499268,
                        "cpu_energy_total": 0.0044862138442858844,
                        "gpu_energy_total": 0.04145036260470647,
                        "ram_energy_total": 2.8601316780630142e-05,
                        "total_energy_kwh": 0.04596517776577299,
                        "total_energy_joules": 165474.63995678275
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09901215076992483,
                        "joules_per_token": 10.099770505174728,
                        "flops_per_joule": 318106643.547384,
                        "joules_per_flop": 3.143599859620799e-09
                    },
                    "per-process_emissions": [
                        0.004372037465968508,
                        0.00438172484469051,
                        0.004382556256993274,
                        0.004374115902218928
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0548": {
            "setup": {
                "experiment_id": "0548",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 03:32:25 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_burst_0.1_0.2_6.0_10",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.1,
                    "delay_max": 0.2,
                    "simulate_burst": true,
                    "burst_interval": 6.0,
                    "burst_size": 10
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 235.0922280480736,
                        "average_latency_ms_per_batch": 29386.5285060092,
                        "throughput_queries_per_sec": 0.5444671695987564,
                        "throughput_tokens_per_sec": 69.69179770864082
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 37138464768,
                        "gpu_max_memory_reserved_bytes": 37138464768
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            10.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 99.9,
                        "cpu_memory_usage_bytes": 1974906880
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0548",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 275.9991269726657,
                            "process_3": 86.4963268401966,
                            "process_2": 374.79941728129296,
                            "process_1": 346.92364264436594
                        },
                        "ram_power": {
                            "process_0": 0.6885666847229004,
                            "process_3": 0.7081489562988281,
                            "process_2": 0.7319326400756836,
                            "process_1": 0.706505298614502
                        },
                        "cpu_energy": {
                            "process_0": 0.007457028796934543,
                            "process_3": 0.006652270151780616,
                            "process_2": 0.007414711069706754,
                            "process_1": 0.006567565336401456
                        },
                        "gpu_energy": {
                            "process_0": 0.02002866518958868,
                            "process_3": 0.018501945634881167,
                            "process_2": 0.019351660203540177,
                            "process_1": 0.01881512421875442
                        },
                        "ram_energy": {
                            "process_0": 3.858488135348269e-05,
                            "process_3": 3.730820798386538e-05,
                            "process_2": 4.0888575308517946e-05,
                            "process_1": 3.603313680255446e-05
                        },
                        "total_energy_kwh": {
                            "process_0": 0.02752427886787669,
                            "process_3": 0.02519152399464563,
                            "process_2": 0.026807259848555434,
                            "process_1": 0.025418722691958436
                        },
                        "total_energy_joules": {
                            "process_0": 99087.40392435608,
                            "process_3": 90689.48638072427,
                            "process_2": 96506.13545479956,
                            "process_1": 91507.40169105037
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 271.05462843463033,
                        "ram_power_avg": 0.7087883949279785,
                        "cpu_energy_total": 0.02809157535482337,
                        "gpu_energy_total": 0.07669739524676444,
                        "ram_energy_total": 0.00015281480144842046,
                        "total_energy_kwh": 0.1049417854030362,
                        "total_energy_joules": 377790.4274509303
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.043367959613344234,
                        "joules_per_token": 23.05849776922182,
                        "flops_per_joule": 139332758.28091493,
                        "joules_per_flop": 7.1770631137858895e-09
                    },
                    "per-process_emissions": [
                        0.010485374034717625,
                        0.009596711065760252,
                        0.010212225639307192,
                        0.009683262409501566
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0548": {
            "setup": {
                "experiment_id": "0548",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 03:32:25 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_burst_0.1_0.2_6.0_10",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.1,
                    "delay_max": 0.2,
                    "simulate_burst": true,
                    "burst_interval": 6.0,
                    "burst_size": 10
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 235.0922280480736,
                        "average_latency_ms_per_batch": 29386.5285060092,
                        "throughput_queries_per_sec": 0.5444671695987564,
                        "throughput_tokens_per_sec": 69.69179770864082
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 37138464768,
                        "gpu_max_memory_reserved_bytes": 37138464768
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            10.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 99.9,
                        "cpu_memory_usage_bytes": 1974906880
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0548",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 275.9991269726657,
                            "process_3": 86.4963268401966,
                            "process_2": 374.79941728129296,
                            "process_1": 346.92364264436594
                        },
                        "ram_power": {
                            "process_0": 0.6885666847229004,
                            "process_3": 0.7081489562988281,
                            "process_2": 0.7319326400756836,
                            "process_1": 0.706505298614502
                        },
                        "cpu_energy": {
                            "process_0": 0.007457028796934543,
                            "process_3": 0.006652270151780616,
                            "process_2": 0.007414711069706754,
                            "process_1": 0.006567565336401456
                        },
                        "gpu_energy": {
                            "process_0": 0.02002866518958868,
                            "process_3": 0.018501945634881167,
                            "process_2": 0.019351660203540177,
                            "process_1": 0.01881512421875442
                        },
                        "ram_energy": {
                            "process_0": 3.858488135348269e-05,
                            "process_3": 3.730820798386538e-05,
                            "process_2": 4.0888575308517946e-05,
                            "process_1": 3.603313680255446e-05
                        },
                        "total_energy_kwh": {
                            "process_0": 0.02752427886787669,
                            "process_3": 0.02519152399464563,
                            "process_2": 0.026807259848555434,
                            "process_1": 0.025418722691958436
                        },
                        "total_energy_joules": {
                            "process_0": 99087.40392435608,
                            "process_3": 90689.48638072427,
                            "process_2": 96506.13545479956,
                            "process_1": 91507.40169105037
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 271.05462843463033,
                        "ram_power_avg": 0.7087883949279785,
                        "cpu_energy_total": 0.02809157535482337,
                        "gpu_energy_total": 0.07669739524676444,
                        "ram_energy_total": 0.00015281480144842046,
                        "total_energy_kwh": 0.1049417854030362,
                        "total_energy_joules": 377790.4274509303
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.043367959613344234,
                        "joules_per_token": 23.05849776922182,
                        "flops_per_joule": 139332758.28091493,
                        "joules_per_flop": 7.1770631137858895e-09
                    },
                    "per-process_emissions": [
                        0.010485374034717625,
                        0.009596711065760252,
                        0.010212225639307192,
                        0.009683262409501566
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0549": {
            "setup": {
                "experiment_id": "0549",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 03:37:39 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_burst_0.2_0.4_2.0_5",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.2,
                    "delay_max": 0.4,
                    "simulate_burst": true,
                    "burst_interval": 2.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 146.7018237389857,
                        "average_latency_ms_per_batch": 18337.727967373212,
                        "throughput_queries_per_sec": 0.8725181237537968,
                        "throughput_tokens_per_sec": 111.68231984048599
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 36568039424,
                        "gpu_max_memory_reserved_bytes": 36568039424
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            17.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 100.0,
                        "cpu_memory_usage_bytes": 1970421760
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0549",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_2": 780.1054933969497,
                            "process_0": 399.3606603577989,
                            "process_1": 723.8210903634752,
                            "process_3": 545.7070416714965
                        },
                        "ram_power": {
                            "process_2": 0.7060933113098145,
                            "process_0": 0.6869945526123047,
                            "process_1": 0.7068285942077637,
                            "process_3": 0.7302331924438477
                        },
                        "cpu_energy": {
                            "process_2": 0.004425941846466229,
                            "process_0": 0.0044404925818671455,
                            "process_1": 0.004575253272490954,
                            "process_3": 0.004586466968437889
                        },
                        "gpu_energy": {
                            "process_2": 0.015077909006761203,
                            "process_0": 0.015992720849723696,
                            "process_1": 0.01606019895926103,
                            "process_3": 0.015975368891396613
                        },
                        "ram_energy": {
                            "process_2": 2.264785952324157e-05,
                            "process_0": 2.3566682482249626e-05,
                            "process_1": 2.4071702875385966e-05,
                            "process_3": 2.363130108529558e-05
                        },
                        "total_energy_kwh": {
                            "process_2": 0.01952649871275069,
                            "process_0": 0.02045678011407308,
                            "process_1": 0.020659523934627378,
                            "process_3": 0.0205854671609198
                        },
                        "total_energy_joules": {
                            "process_2": 70295.39536590248,
                            "process_0": 73644.40841066309,
                            "process_1": 74374.28616465857,
                            "process_3": 74107.68177931129
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 612.2485714474301,
                        "ram_power_avg": 0.7075374126434326,
                        "cpu_energy_total": 0.01802815466926222,
                        "gpu_energy_total": 0.06310619770714254,
                        "ram_energy_total": 9.391754596617273e-05,
                        "total_energy_kwh": 0.08122826992237095,
                        "total_energy_joules": 292421.7717205354
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.05602865991680683,
                        "joules_per_token": 17.84800852786471,
                        "flops_per_joule": 180009108.074108,
                        "joules_per_flop": 5.555274456380894e-09
                    },
                    "per-process_emissions": [
                        0.007438619684622376,
                        0.007793010384456141,
                        0.0078702456428963,
                        0.0078420337149524
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0549": {
            "setup": {
                "experiment_id": "0549",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 03:37:39 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_burst_0.2_0.4_2.0_5",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.2,
                    "delay_max": 0.4,
                    "simulate_burst": true,
                    "burst_interval": 2.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 146.7018237389857,
                        "average_latency_ms_per_batch": 18337.727967373212,
                        "throughput_queries_per_sec": 0.8725181237537968,
                        "throughput_tokens_per_sec": 111.68231984048599
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 36568039424,
                        "gpu_max_memory_reserved_bytes": 36568039424
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            17.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 100.0,
                        "cpu_memory_usage_bytes": 1970421760
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0549",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_2": 780.1054933969497,
                            "process_0": 399.3606603577989,
                            "process_1": 723.8210903634752,
                            "process_3": 545.7070416714965
                        },
                        "ram_power": {
                            "process_2": 0.7060933113098145,
                            "process_0": 0.6869945526123047,
                            "process_1": 0.7068285942077637,
                            "process_3": 0.7302331924438477
                        },
                        "cpu_energy": {
                            "process_2": 0.004425941846466229,
                            "process_0": 0.0044404925818671455,
                            "process_1": 0.004575253272490954,
                            "process_3": 0.004586466968437889
                        },
                        "gpu_energy": {
                            "process_2": 0.015077909006761203,
                            "process_0": 0.015992720849723696,
                            "process_1": 0.01606019895926103,
                            "process_3": 0.015975368891396613
                        },
                        "ram_energy": {
                            "process_2": 2.264785952324157e-05,
                            "process_0": 2.3566682482249626e-05,
                            "process_1": 2.4071702875385966e-05,
                            "process_3": 2.363130108529558e-05
                        },
                        "total_energy_kwh": {
                            "process_2": 0.01952649871275069,
                            "process_0": 0.02045678011407308,
                            "process_1": 0.020659523934627378,
                            "process_3": 0.0205854671609198
                        },
                        "total_energy_joules": {
                            "process_2": 70295.39536590248,
                            "process_0": 73644.40841066309,
                            "process_1": 74374.28616465857,
                            "process_3": 74107.68177931129
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 612.2485714474301,
                        "ram_power_avg": 0.7075374126434326,
                        "cpu_energy_total": 0.01802815466926222,
                        "gpu_energy_total": 0.06310619770714254,
                        "ram_energy_total": 9.391754596617273e-05,
                        "total_energy_kwh": 0.08122826992237095,
                        "total_energy_joules": 292421.7717205354
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.05602865991680683,
                        "joules_per_token": 17.84800852786471,
                        "flops_per_joule": 180009108.074108,
                        "joules_per_flop": 5.555274456380894e-09
                    },
                    "per-process_emissions": [
                        0.007438619684622376,
                        0.007793010384456141,
                        0.0078702456428963,
                        0.0078420337149524
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0550": {
            "setup": {
                "experiment_id": "0550",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 03:42:17 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_greedy_temp_0.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "greedy",
                    "decoder_temperature": 0.4,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 122.71682443993632,
                        "average_latency_ms_per_batch": 15339.60305499204,
                        "throughput_queries_per_sec": 1.0430517623331228,
                        "throughput_tokens_per_sec": 133.51062557863972
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38101057536,
                        "gpu_max_memory_reserved_bytes": 38101057536
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            26.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 100.0,
                        "cpu_memory_usage_bytes": 1973469184
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0550",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 349.0233624174783,
                            "process_1": 447.48570449663106,
                            "process_2": 87.43031273357346,
                            "process_3": 107.17330535678619
                        },
                        "ram_power": {
                            "process_0": 0.688084602355957,
                            "process_1": 0.7165918350219728,
                            "process_2": 0.7093248367309571,
                            "process_3": 0.7299742698669434
                        },
                        "cpu_energy": {
                            "process_0": 0.0036843635450131834,
                            "process_1": 0.0036624680705681376,
                            "process_2": 0.0033458627283489487,
                            "process_3": 0.0034382377992078516
                        },
                        "gpu_energy": {
                            "process_0": 0.015153721011857968,
                            "process_1": 0.014257907239651146,
                            "process_2": 0.014724723446439825,
                            "process_3": 0.014292753378638778
                        },
                        "ram_energy": {
                            "process_0": 1.944788150440258e-05,
                            "process_1": 1.835705599864783e-05,
                            "process_2": 1.7246919710322637e-05,
                            "process_3": 1.8007675173086286e-05
                        },
                        "total_energy_kwh": {
                            "process_0": 0.01885753243837554,
                            "process_1": 0.017938732366217947,
                            "process_2": 0.01808783309449909,
                            "process_3": 0.017748998853019712
                        },
                        "total_energy_joules": {
                            "process_0": 67887.11677815195,
                            "process_1": 64579.43651838461,
                            "process_2": 65116.199140196724,
                            "process_3": 63896.39587087096
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 247.77817125111724,
                        "ram_power_avg": 0.7109938859939575,
                        "cpu_energy_total": 0.01413093214313812,
                        "gpu_energy_total": 0.05842910507658772,
                        "ram_energy_total": 7.305953238645933e-05,
                        "total_energy_kwh": 0.07263309675211228,
                        "total_energy_joules": 261479.14830760425
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.06265891603993544,
                        "joules_per_token": 15.959420673071547,
                        "flops_per_joule": 201310822.10402465,
                        "joules_per_flop": 4.967442830685294e-09
                    },
                    "per-process_emissions": [
                        0.007183776982399162,
                        0.006833760094910727,
                        0.006890560017349429,
                        0.00676148111305786
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0550": {
            "setup": {
                "experiment_id": "0550",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 03:42:17 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_greedy_temp_0.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "greedy",
                    "decoder_temperature": 0.4,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 122.71682443993632,
                        "average_latency_ms_per_batch": 15339.60305499204,
                        "throughput_queries_per_sec": 1.0430517623331228,
                        "throughput_tokens_per_sec": 133.51062557863972
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38101057536,
                        "gpu_max_memory_reserved_bytes": 38101057536
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            26.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 100.0,
                        "cpu_memory_usage_bytes": 1973469184
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0550",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 349.0233624174783,
                            "process_1": 447.48570449663106,
                            "process_2": 87.43031273357346,
                            "process_3": 107.17330535678619
                        },
                        "ram_power": {
                            "process_0": 0.688084602355957,
                            "process_1": 0.7165918350219728,
                            "process_2": 0.7093248367309571,
                            "process_3": 0.7299742698669434
                        },
                        "cpu_energy": {
                            "process_0": 0.0036843635450131834,
                            "process_1": 0.0036624680705681376,
                            "process_2": 0.0033458627283489487,
                            "process_3": 0.0034382377992078516
                        },
                        "gpu_energy": {
                            "process_0": 0.015153721011857968,
                            "process_1": 0.014257907239651146,
                            "process_2": 0.014724723446439825,
                            "process_3": 0.014292753378638778
                        },
                        "ram_energy": {
                            "process_0": 1.944788150440258e-05,
                            "process_1": 1.835705599864783e-05,
                            "process_2": 1.7246919710322637e-05,
                            "process_3": 1.8007675173086286e-05
                        },
                        "total_energy_kwh": {
                            "process_0": 0.01885753243837554,
                            "process_1": 0.017938732366217947,
                            "process_2": 0.01808783309449909,
                            "process_3": 0.017748998853019712
                        },
                        "total_energy_joules": {
                            "process_0": 67887.11677815195,
                            "process_1": 64579.43651838461,
                            "process_2": 65116.199140196724,
                            "process_3": 63896.39587087096
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 247.77817125111724,
                        "ram_power_avg": 0.7109938859939575,
                        "cpu_energy_total": 0.01413093214313812,
                        "gpu_energy_total": 0.05842910507658772,
                        "ram_energy_total": 7.305953238645933e-05,
                        "total_energy_kwh": 0.07263309675211228,
                        "total_energy_joules": 261479.14830760425
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.06265891603993544,
                        "joules_per_token": 15.959420673071547,
                        "flops_per_joule": 201310822.10402465,
                        "joules_per_flop": 4.967442830685294e-09
                    },
                    "per-process_emissions": [
                        0.007183776982399162,
                        0.006833760094910727,
                        0.006890560017349429,
                        0.00676148111305786
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0551": {
            "setup": {
                "experiment_id": "0551",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 04:00:57 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_1",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 1,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 990.2499784908141,
                        "average_latency_ms_per_batch": 7736.327956959485,
                        "throughput_queries_per_sec": 0.12926029061376787,
                        "throughput_tokens_per_sec": 16.545317198562287
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38044434432,
                        "gpu_max_memory_reserved_bytes": 38044434432
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 53.1,
                        "cpu_memory_usage_bytes": 1959411712
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0551",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 408.5273441959399,
                            "process_1": 931.3501183065351,
                            "process_2": 604.5045049228225,
                            "process_3": 78.37430296004521
                        },
                        "ram_power": {
                            "process_0": 0.6831851005554199,
                            "process_1": 0.7190165519714355,
                            "process_2": 0.710515022277832,
                            "process_3": 0.7268071174621583
                        },
                        "cpu_energy": {
                            "process_0": 0.030492678942511658,
                            "process_1": 0.03074064861880105,
                            "process_2": 0.03000826744625556,
                            "process_3": 0.030014860066386945
                        },
                        "gpu_energy": {
                            "process_0": 0.08722613200307627,
                            "process_1": 0.08703259684824616,
                            "process_2": 0.08754936559499349,
                            "process_3": 0.08700172487910507
                        },
                        "ram_energy": {
                            "process_0": 0.00016899454991934425,
                            "process_1": 0.0001675652656034206,
                            "process_2": 0.00017118803992029446,
                            "process_3": 0.00017266543097764391
                        },
                        "total_energy_kwh": {
                            "process_0": 0.11788780549550741,
                            "process_1": 0.11794081073265061,
                            "process_2": 0.11772882108116939,
                            "process_3": 0.11718925037646986
                        },
                        "total_energy_joules": {
                            "process_0": 424396.0997838267,
                            "process_1": 424586.9186375422,
                            "process_2": 423823.7558922098,
                            "process_3": 421881.3013552915
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 505.68906759633575,
                        "ram_power_avg": 0.7098809480667114,
                        "cpu_energy_total": 0.1212564550739552,
                        "gpu_energy_total": 0.348809819325421,
                        "ram_energy_total": 0.0006804132864207032,
                        "total_energy_kwh": 0.4707466876857973,
                        "total_energy_joules": 1694688.0756688705
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.009667855834492407,
                        "joules_per_token": 103.43555149346133,
                        "flops_per_joule": 31060926.824594706,
                        "joules_per_flop": 3.2194789474478224e-08
                    },
                    "per-process_emissions": [
                        0.044909359503513546,
                        0.044929551848603255,
                        0.04484879439087148,
                        0.0446432449309162
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0551": {
            "setup": {
                "experiment_id": "0551",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 04:00:57 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_1",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 1,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 990.2499784908141,
                        "average_latency_ms_per_batch": 7736.327956959485,
                        "throughput_queries_per_sec": 0.12926029061376787,
                        "throughput_tokens_per_sec": 16.545317198562287
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38044434432,
                        "gpu_max_memory_reserved_bytes": 38044434432
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 53.1,
                        "cpu_memory_usage_bytes": 1959411712
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0551",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 408.5273441959399,
                            "process_1": 931.3501183065351,
                            "process_2": 604.5045049228225,
                            "process_3": 78.37430296004521
                        },
                        "ram_power": {
                            "process_0": 0.6831851005554199,
                            "process_1": 0.7190165519714355,
                            "process_2": 0.710515022277832,
                            "process_3": 0.7268071174621583
                        },
                        "cpu_energy": {
                            "process_0": 0.030492678942511658,
                            "process_1": 0.03074064861880105,
                            "process_2": 0.03000826744625556,
                            "process_3": 0.030014860066386945
                        },
                        "gpu_energy": {
                            "process_0": 0.08722613200307627,
                            "process_1": 0.08703259684824616,
                            "process_2": 0.08754936559499349,
                            "process_3": 0.08700172487910507
                        },
                        "ram_energy": {
                            "process_0": 0.00016899454991934425,
                            "process_1": 0.0001675652656034206,
                            "process_2": 0.00017118803992029446,
                            "process_3": 0.00017266543097764391
                        },
                        "total_energy_kwh": {
                            "process_0": 0.11788780549550741,
                            "process_1": 0.11794081073265061,
                            "process_2": 0.11772882108116939,
                            "process_3": 0.11718925037646986
                        },
                        "total_energy_joules": {
                            "process_0": 424396.0997838267,
                            "process_1": 424586.9186375422,
                            "process_2": 423823.7558922098,
                            "process_3": 421881.3013552915
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 505.68906759633575,
                        "ram_power_avg": 0.7098809480667114,
                        "cpu_energy_total": 0.1212564550739552,
                        "gpu_energy_total": 0.348809819325421,
                        "ram_energy_total": 0.0006804132864207032,
                        "total_energy_kwh": 0.4707466876857973,
                        "total_energy_joules": 1694688.0756688705
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.009667855834492407,
                        "joules_per_token": 103.43555149346133,
                        "flops_per_joule": 31060926.824594706,
                        "joules_per_flop": 3.2194789474478224e-08
                    },
                    "per-process_emissions": [
                        0.044909359503513546,
                        0.044929551848603255,
                        0.04484879439087148,
                        0.0446432449309162
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0552": {
            "setup": {
                "experiment_id": "0552",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 04:02:47 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_5_temp_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 0.8,
                    "decoder_top_k": 5,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 43.52476919913897,
                        "average_latency_ms_per_batch": 5440.596149892372,
                        "throughput_queries_per_sec": 2.9408541930311296,
                        "throughput_tokens_per_sec": 376.4293367079846
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38449184768,
                        "gpu_max_memory_reserved_bytes": 38449184768
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 43.1,
                        "cpu_memory_usage_bytes": 1957318656
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0552",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_3": 790.9733828826087,
                            "process_0": 767.5918271788453,
                            "process_2": 197.69645988891284,
                            "process_1": 652.0961309799782
                        },
                        "ram_power": {
                            "process_3": 0.7326993942260742,
                            "process_0": 0.6814742088317871,
                            "process_2": 0.7331972122192383,
                            "process_1": 0.7101931571960449
                        },
                        "cpu_energy": {
                            "process_3": 0.0013081373275963413,
                            "process_0": 0.0014160123858255244,
                            "process_2": 0.0013974878612516477,
                            "process_1": 0.0013607890475323074
                        },
                        "gpu_energy": {
                            "process_3": 0.01064551184973439,
                            "process_0": 0.010660184083693736,
                            "process_2": 0.010579296796761284,
                            "process_1": 0.010673482705449544
                        },
                        "ram_energy": {
                            "process_3": 7.964893891117773e-06,
                            "process_0": 7.350824404610514e-06,
                            "process_2": 7.8786564750598e-06,
                            "process_1": 7.647989994169636e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.011961614071221848,
                            "process_0": 0.012083547293923868,
                            "process_2": 0.01198466331448799,
                            "process_1": 0.012041919742976017
                        },
                        "total_energy_joules": {
                            "process_3": 43061.81065639865,
                            "process_0": 43500.77025812592,
                            "process_2": 43144.78793215677,
                            "process_1": 43350.911074713666
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 602.0894502325862,
                        "ram_power_avg": 0.7143909931182861,
                        "cpu_energy_total": 0.005482426622205821,
                        "gpu_energy_total": 0.042558475435638954,
                        "ram_energy_total": 3.084236476495773e-05,
                        "total_energy_kwh": 0.048071744422609725,
                        "total_energy_joules": 173058.279921395
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09467330894217714,
                        "joules_per_token": 10.562639155358582,
                        "flops_per_joule": 304166794.75130016,
                        "joules_per_flop": 3.287669848438397e-09
                    },
                    "per-process_emissions": [
                        0.004556776880431964,
                        0.004603227341620298,
                        0.0045655574896542005,
                        0.004587369326086714
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0552": {
            "setup": {
                "experiment_id": "0552",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 04:02:47 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_5_temp_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 0.8,
                    "decoder_top_k": 5,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 43.52476919913897,
                        "average_latency_ms_per_batch": 5440.596149892372,
                        "throughput_queries_per_sec": 2.9408541930311296,
                        "throughput_tokens_per_sec": 376.4293367079846
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38449184768,
                        "gpu_max_memory_reserved_bytes": 38449184768
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 43.1,
                        "cpu_memory_usage_bytes": 1957318656
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0552",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_3": 790.9733828826087,
                            "process_0": 767.5918271788453,
                            "process_2": 197.69645988891284,
                            "process_1": 652.0961309799782
                        },
                        "ram_power": {
                            "process_3": 0.7326993942260742,
                            "process_0": 0.6814742088317871,
                            "process_2": 0.7331972122192383,
                            "process_1": 0.7101931571960449
                        },
                        "cpu_energy": {
                            "process_3": 0.0013081373275963413,
                            "process_0": 0.0014160123858255244,
                            "process_2": 0.0013974878612516477,
                            "process_1": 0.0013607890475323074
                        },
                        "gpu_energy": {
                            "process_3": 0.01064551184973439,
                            "process_0": 0.010660184083693736,
                            "process_2": 0.010579296796761284,
                            "process_1": 0.010673482705449544
                        },
                        "ram_energy": {
                            "process_3": 7.964893891117773e-06,
                            "process_0": 7.350824404610514e-06,
                            "process_2": 7.8786564750598e-06,
                            "process_1": 7.647989994169636e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.011961614071221848,
                            "process_0": 0.012083547293923868,
                            "process_2": 0.01198466331448799,
                            "process_1": 0.012041919742976017
                        },
                        "total_energy_joules": {
                            "process_3": 43061.81065639865,
                            "process_0": 43500.77025812592,
                            "process_2": 43144.78793215677,
                            "process_1": 43350.911074713666
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 602.0894502325862,
                        "ram_power_avg": 0.7143909931182861,
                        "cpu_energy_total": 0.005482426622205821,
                        "gpu_energy_total": 0.042558475435638954,
                        "ram_energy_total": 3.084236476495773e-05,
                        "total_energy_kwh": 0.048071744422609725,
                        "total_energy_joules": 173058.279921395
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09467330894217714,
                        "joules_per_token": 10.562639155358582,
                        "flops_per_joule": 304166794.75130016,
                        "joules_per_flop": 3.287669848438397e-09
                    },
                    "per-process_emissions": [
                        0.004556776880431964,
                        0.004603227341620298,
                        0.0045655574896542005,
                        0.004587369326086714
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0553": {
            "setup": {
                "experiment_id": "0553",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 04:04:32 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_5_temp_0.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 0.2,
                    "decoder_top_k": 5,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 42.57087161805248,
                        "average_latency_ms_per_batch": 5321.35895225656,
                        "throughput_queries_per_sec": 3.006750746106892,
                        "throughput_tokens_per_sec": 384.8640955016822
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38449184768,
                        "gpu_max_memory_reserved_bytes": 38449184768
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            24.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 43.1,
                        "cpu_memory_usage_bytes": 1975066624
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0553",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 89.77234043426827,
                            "process_3": 496.2673882389853,
                            "process_2": 934.0166846205359,
                            "process_1": 73.61855990792321
                        },
                        "ram_power": {
                            "process_0": 0.6886038780212402,
                            "process_3": 0.7248787879943848,
                            "process_2": 0.7260475158691406,
                            "process_1": 0.7177591323852539
                        },
                        "cpu_energy": {
                            "process_0": 0.00128761302840212,
                            "process_3": 0.001311612619665539,
                            "process_2": 0.0012266656184365273,
                            "process_1": 0.001290663109128218
                        },
                        "gpu_energy": {
                            "process_0": 0.010664247975833874,
                            "process_3": 0.010701209949845136,
                            "process_2": 0.010452361972989266,
                            "process_1": 0.010654182690004177
                        },
                        "ram_energy": {
                            "process_0": 7.103665660984469e-06,
                            "process_3": 7.567955461898803e-06,
                            "process_2": 7.122588487258545e-06,
                            "process_1": 7.378829681973931e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.01195896466989698,
                            "process_3": 0.01202039052497258,
                            "process_2": 0.011686150179913046,
                            "process_1": 0.011952224628814366
                        },
                        "total_energy_joules": {
                            "process_0": 43052.27281162913,
                            "process_3": 43273.40588990129,
                            "process_2": 42070.14064768697,
                            "process_1": 43028.008663731714
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 398.4187433004281,
                        "ram_power_avg": 0.7143223285675049,
                        "cpu_energy_total": 0.005116554375632404,
                        "gpu_energy_total": 0.04247200258867245,
                        "ram_energy_total": 2.9173039292115748e-05,
                        "total_energy_kwh": 0.047617730003596974,
                        "total_energy_joules": 171423.8280129491
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09557597791342273,
                        "joules_per_token": 10.462880127743475,
                        "flops_per_joule": 307066893.3194501,
                        "joules_per_flop": 3.2566193938715256e-09
                    },
                    "per-process_emissions": [
                        0.004555767590997255,
                        0.0045791677704883045,
                        0.004451838911037875,
                        0.004553199972346833
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0553": {
            "setup": {
                "experiment_id": "0553",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 04:04:32 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_5_temp_0.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 0.2,
                    "decoder_top_k": 5,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 42.57087161805248,
                        "average_latency_ms_per_batch": 5321.35895225656,
                        "throughput_queries_per_sec": 3.006750746106892,
                        "throughput_tokens_per_sec": 384.8640955016822
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38449184768,
                        "gpu_max_memory_reserved_bytes": 38449184768
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            24.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 43.1,
                        "cpu_memory_usage_bytes": 1975066624
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0553",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 89.77234043426827,
                            "process_3": 496.2673882389853,
                            "process_2": 934.0166846205359,
                            "process_1": 73.61855990792321
                        },
                        "ram_power": {
                            "process_0": 0.6886038780212402,
                            "process_3": 0.7248787879943848,
                            "process_2": 0.7260475158691406,
                            "process_1": 0.7177591323852539
                        },
                        "cpu_energy": {
                            "process_0": 0.00128761302840212,
                            "process_3": 0.001311612619665539,
                            "process_2": 0.0012266656184365273,
                            "process_1": 0.001290663109128218
                        },
                        "gpu_energy": {
                            "process_0": 0.010664247975833874,
                            "process_3": 0.010701209949845136,
                            "process_2": 0.010452361972989266,
                            "process_1": 0.010654182690004177
                        },
                        "ram_energy": {
                            "process_0": 7.103665660984469e-06,
                            "process_3": 7.567955461898803e-06,
                            "process_2": 7.122588487258545e-06,
                            "process_1": 7.378829681973931e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.01195896466989698,
                            "process_3": 0.01202039052497258,
                            "process_2": 0.011686150179913046,
                            "process_1": 0.011952224628814366
                        },
                        "total_energy_joules": {
                            "process_0": 43052.27281162913,
                            "process_3": 43273.40588990129,
                            "process_2": 42070.14064768697,
                            "process_1": 43028.008663731714
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 398.4187433004281,
                        "ram_power_avg": 0.7143223285675049,
                        "cpu_energy_total": 0.005116554375632404,
                        "gpu_energy_total": 0.04247200258867245,
                        "ram_energy_total": 2.9173039292115748e-05,
                        "total_energy_kwh": 0.047617730003596974,
                        "total_energy_joules": 171423.8280129491
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09557597791342273,
                        "joules_per_token": 10.462880127743475,
                        "flops_per_joule": 307066893.3194501,
                        "joules_per_flop": 3.2566193938715256e-09
                    },
                    "per-process_emissions": [
                        0.004555767590997255,
                        0.0045791677704883045,
                        0.004451838911037875,
                        0.004553199972346833
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0554": {
            "setup": {
                "experiment_id": "0554",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 04:06:19 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_topp_0.8_temp_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_p",
                    "decoder_temperature": 0.8,
                    "decoder_top_k": null,
                    "decoder_top_p": 0.8
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 43.72131526505109,
                        "average_latency_ms_per_batch": 5465.164408131386,
                        "throughput_queries_per_sec": 2.927633791985156,
                        "throughput_tokens_per_sec": 374.73712537409995
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38403047424,
                        "gpu_max_memory_reserved_bytes": 38403047424
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            27.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 42.8,
                        "cpu_memory_usage_bytes": 1980010496
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0554",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 584.7710170065853,
                            "process_3": 217.23498011212112,
                            "process_2": 820.9567978011424,
                            "process_1": 487.0779975756861
                        },
                        "ram_power": {
                            "process_0": 0.6889786720275879,
                            "process_3": 0.7101860046386719,
                            "process_2": 0.7088155746459962,
                            "process_1": 0.7318668365478516
                        },
                        "cpu_energy": {
                            "process_0": 0.001178924555693811,
                            "process_3": 0.0013407164134987394,
                            "process_2": 0.0013084579400583609,
                            "process_1": 0.001271217166871793
                        },
                        "gpu_energy": {
                            "process_0": 0.010750668600527646,
                            "process_3": 0.01059891347912334,
                            "process_2": 0.01066066936186516,
                            "process_1": 0.010890367878955587
                        },
                        "ram_energy": {
                            "process_0": 7.226159942970771e-06,
                            "process_3": 7.733846200717445e-06,
                            "process_2": 7.841938522627873e-06,
                            "process_1": 8.182476784495464e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.011936819316164426,
                            "process_3": 0.011947363738822799,
                            "process_2": 0.01197696924044615,
                            "process_1": 0.012169767522611877
                        },
                        "total_energy_joules": {
                            "process_0": 42972.54953819193,
                            "process_3": 43010.50945976208,
                            "process_2": 43117.08926560614,
                            "process_1": 43811.16308140276
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 527.5101981238837,
                        "ram_power_avg": 0.7099617719650269,
                        "cpu_energy_total": 0.005099316076122704,
                        "gpu_energy_total": 0.04290061932047173,
                        "ram_energy_total": 3.0984421450811546e-05,
                        "total_energy_kwh": 0.048030919818045255,
                        "total_energy_joules": 172911.31134496292
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09475377794870493,
                        "joules_per_token": 10.55366890533221,
                        "flops_per_joule": 304425325.9050737,
                        "joules_per_flop": 3.284877817004691e-09
                    },
                    "per-process_emissions": [
                        0.004547331318492838,
                        0.004551348216304546,
                        0.004562626432147961,
                        0.004636072937738995
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0554": {
            "setup": {
                "experiment_id": "0554",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 04:06:19 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_topp_0.8_temp_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_p",
                    "decoder_temperature": 0.8,
                    "decoder_top_k": null,
                    "decoder_top_p": 0.8
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 43.72131526505109,
                        "average_latency_ms_per_batch": 5465.164408131386,
                        "throughput_queries_per_sec": 2.927633791985156,
                        "throughput_tokens_per_sec": 374.73712537409995
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38403047424,
                        "gpu_max_memory_reserved_bytes": 38403047424
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            27.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 42.8,
                        "cpu_memory_usage_bytes": 1980010496
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0554",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 584.7710170065853,
                            "process_3": 217.23498011212112,
                            "process_2": 820.9567978011424,
                            "process_1": 487.0779975756861
                        },
                        "ram_power": {
                            "process_0": 0.6889786720275879,
                            "process_3": 0.7101860046386719,
                            "process_2": 0.7088155746459962,
                            "process_1": 0.7318668365478516
                        },
                        "cpu_energy": {
                            "process_0": 0.001178924555693811,
                            "process_3": 0.0013407164134987394,
                            "process_2": 0.0013084579400583609,
                            "process_1": 0.001271217166871793
                        },
                        "gpu_energy": {
                            "process_0": 0.010750668600527646,
                            "process_3": 0.01059891347912334,
                            "process_2": 0.01066066936186516,
                            "process_1": 0.010890367878955587
                        },
                        "ram_energy": {
                            "process_0": 7.226159942970771e-06,
                            "process_3": 7.733846200717445e-06,
                            "process_2": 7.841938522627873e-06,
                            "process_1": 8.182476784495464e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.011936819316164426,
                            "process_3": 0.011947363738822799,
                            "process_2": 0.01197696924044615,
                            "process_1": 0.012169767522611877
                        },
                        "total_energy_joules": {
                            "process_0": 42972.54953819193,
                            "process_3": 43010.50945976208,
                            "process_2": 43117.08926560614,
                            "process_1": 43811.16308140276
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 527.5101981238837,
                        "ram_power_avg": 0.7099617719650269,
                        "cpu_energy_total": 0.005099316076122704,
                        "gpu_energy_total": 0.04290061932047173,
                        "ram_energy_total": 3.0984421450811546e-05,
                        "total_energy_kwh": 0.048030919818045255,
                        "total_energy_joules": 172911.31134496292
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09475377794870493,
                        "joules_per_token": 10.55366890533221,
                        "flops_per_joule": 304425325.9050737,
                        "joules_per_flop": 3.284877817004691e-09
                    },
                    "per-process_emissions": [
                        0.004547331318492838,
                        0.004551348216304546,
                        0.004562626432147961,
                        0.004636072937738995
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0555": {
            "setup": {
                "experiment_id": "0555",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 04:08:04 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_200_temp_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 200,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 41.96109683299437,
                        "average_latency_ms_per_batch": 5245.1371041242965,
                        "throughput_queries_per_sec": 3.050444570346705,
                        "throughput_tokens_per_sec": 390.45690500437826
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38451281920,
                        "gpu_max_memory_reserved_bytes": 38451281920
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 42.7,
                        "cpu_memory_usage_bytes": 1953906688
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0555",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_3": 978.3336352444412,
                            "process_1": 2137.0822567207842,
                            "process_2": 963.87966961564,
                            "process_0": 0.0
                        },
                        "ram_power": {
                            "process_3": 0.72454833984375,
                            "process_1": 0.7089815139770508,
                            "process_2": 0.710789680480957,
                            "process_0": 0.6812481880187988
                        },
                        "cpu_energy": {
                            "process_3": 0.0012412846904371693,
                            "process_1": 0.0012817184465111497,
                            "process_2": 0.001211689931591536,
                            "process_0": 0.0012880497118767382
                        },
                        "gpu_energy": {
                            "process_3": 0.010522479806866158,
                            "process_1": 0.01063305406199433,
                            "process_2": 0.010542423711713766,
                            "process_0": 0.01063169294979538
                        },
                        "ram_energy": {
                            "process_3": 7.288896903702662e-06,
                            "process_1": 7.238389236589914e-06,
                            "process_2": 6.955398868800684e-06,
                            "process_0": 7.062425932157209e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.01177105339420703,
                            "process_1": 0.011922010897742061,
                            "process_2": 0.0117610690421741,
                            "process_0": 0.011926805087604275
                        },
                        "total_energy_joules": {
                            "process_3": 42375.79221914531,
                            "process_1": 42919.23923187142,
                            "process_2": 42339.84855182676,
                            "process_0": 42936.49831537539
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1019.8238903952164,
                        "ram_power_avg": 0.7063919305801392,
                        "cpu_energy_total": 0.0050227427804165935,
                        "gpu_energy_total": 0.042329650530369634,
                        "ram_energy_total": 2.8545110941250468e-05,
                        "total_energy_kwh": 0.04738093842172746,
                        "total_energy_joules": 170571.37831821886
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09605362963904719,
                        "joules_per_token": 10.41085072743035,
                        "flops_per_joule": 308601494.73999786,
                        "joules_per_flop": 3.24042500455974e-09
                    },
                    "per-process_emissions": [
                        0.004484182790523168,
                        0.004541690051494838,
                        0.004480379251616223,
                        0.004543516398122849
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0555": {
            "setup": {
                "experiment_id": "0555",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 04:08:04 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_200_temp_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 200,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 41.96109683299437,
                        "average_latency_ms_per_batch": 5245.1371041242965,
                        "throughput_queries_per_sec": 3.050444570346705,
                        "throughput_tokens_per_sec": 390.45690500437826
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38451281920,
                        "gpu_max_memory_reserved_bytes": 38451281920
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 42.7,
                        "cpu_memory_usage_bytes": 1953906688
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0555",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_3": 978.3336352444412,
                            "process_1": 2137.0822567207842,
                            "process_2": 963.87966961564,
                            "process_0": 0.0
                        },
                        "ram_power": {
                            "process_3": 0.72454833984375,
                            "process_1": 0.7089815139770508,
                            "process_2": 0.710789680480957,
                            "process_0": 0.6812481880187988
                        },
                        "cpu_energy": {
                            "process_3": 0.0012412846904371693,
                            "process_1": 0.0012817184465111497,
                            "process_2": 0.001211689931591536,
                            "process_0": 0.0012880497118767382
                        },
                        "gpu_energy": {
                            "process_3": 0.010522479806866158,
                            "process_1": 0.01063305406199433,
                            "process_2": 0.010542423711713766,
                            "process_0": 0.01063169294979538
                        },
                        "ram_energy": {
                            "process_3": 7.288896903702662e-06,
                            "process_1": 7.238389236589914e-06,
                            "process_2": 6.955398868800684e-06,
                            "process_0": 7.062425932157209e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.01177105339420703,
                            "process_1": 0.011922010897742061,
                            "process_2": 0.0117610690421741,
                            "process_0": 0.011926805087604275
                        },
                        "total_energy_joules": {
                            "process_3": 42375.79221914531,
                            "process_1": 42919.23923187142,
                            "process_2": 42339.84855182676,
                            "process_0": 42936.49831537539
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1019.8238903952164,
                        "ram_power_avg": 0.7063919305801392,
                        "cpu_energy_total": 0.0050227427804165935,
                        "gpu_energy_total": 0.042329650530369634,
                        "ram_energy_total": 2.8545110941250468e-05,
                        "total_energy_kwh": 0.04738093842172746,
                        "total_energy_joules": 170571.37831821886
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09605362963904719,
                        "joules_per_token": 10.41085072743035,
                        "flops_per_joule": 308601494.73999786,
                        "joules_per_flop": 3.24042500455974e-09
                    },
                    "per-process_emissions": [
                        0.004484182790523168,
                        0.004541690051494838,
                        0.004480379251616223,
                        0.004543516398122849
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0556": {
            "setup": {
                "experiment_id": "0556",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 04:09:50 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_200_temp_1.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 1.2,
                    "decoder_top_k": 200,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 41.68463337799767,
                        "average_latency_ms_per_batch": 5210.579172249709,
                        "throughput_queries_per_sec": 3.0706759212511634,
                        "throughput_tokens_per_sec": 393.0465179201489
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38403047424,
                        "gpu_max_memory_reserved_bytes": 38403047424
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            16.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 42.7,
                        "cpu_memory_usage_bytes": 1975566336
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0556",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_3": 869.2998565990049,
                            "process_1": 971.9237439265746,
                            "process_2": 0.0,
                            "process_0": 957.9367724217955
                        },
                        "ram_power": {
                            "process_3": 0.7089500427246094,
                            "process_1": 0.7089014053344727,
                            "process_2": 0.7089486122131349,
                            "process_0": 0.688788414001465
                        },
                        "cpu_energy": {
                            "process_3": 0.0012243050085344294,
                            "process_1": 0.0012221039422602186,
                            "process_2": 0.0012711229695687503,
                            "process_0": 0.001165282319934704
                        },
                        "gpu_energy": {
                            "process_3": 0.010546939826433288,
                            "process_1": 0.010470998654568575,
                            "process_2": 0.010621981275354742,
                            "process_0": 0.010532916759659727
                        },
                        "ram_energy": {
                            "process_3": 7.374659016449804e-06,
                            "process_1": 7.347160048970707e-06,
                            "process_2": 7.609128584039189e-06,
                            "process_0": 7.204632205239551e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.011778619493984168,
                            "process_1": 0.011700449756877765,
                            "process_2": 0.011900713373507532,
                            "process_0": 0.01170540371179967
                        },
                        "total_energy_joules": {
                            "process_3": 42403.030178343004,
                            "process_1": 42121.619124759956,
                            "process_2": 42842.56814462712,
                            "process_0": 42139.45336247881
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 699.7900932368437,
                        "ram_power_avg": 0.7038971185684204,
                        "cpu_energy_total": 0.004882814240298103,
                        "gpu_energy_total": 0.04217283651601633,
                        "ram_energy_total": 2.953557985469925e-05,
                        "total_energy_kwh": 0.047085186336169135,
                        "total_energy_joules": 169506.6708102089
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09665696294834693,
                        "joules_per_token": 10.345866138318414,
                        "flops_per_joule": 310539886.46736926,
                        "joules_per_flop": 3.2201982533573106e-09
                    },
                    "per-process_emissions": [
                        0.004487065096233269,
                        0.004457286334882585,
                        0.004533576759637695,
                        0.004459173544010084
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0556": {
            "setup": {
                "experiment_id": "0556",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 04:09:50 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_200_temp_1.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 1.2,
                    "decoder_top_k": 200,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 41.68463337799767,
                        "average_latency_ms_per_batch": 5210.579172249709,
                        "throughput_queries_per_sec": 3.0706759212511634,
                        "throughput_tokens_per_sec": 393.0465179201489
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38403047424,
                        "gpu_max_memory_reserved_bytes": 38403047424
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            16.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 42.7,
                        "cpu_memory_usage_bytes": 1975566336
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0556",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_3": 869.2998565990049,
                            "process_1": 971.9237439265746,
                            "process_2": 0.0,
                            "process_0": 957.9367724217955
                        },
                        "ram_power": {
                            "process_3": 0.7089500427246094,
                            "process_1": 0.7089014053344727,
                            "process_2": 0.7089486122131349,
                            "process_0": 0.688788414001465
                        },
                        "cpu_energy": {
                            "process_3": 0.0012243050085344294,
                            "process_1": 0.0012221039422602186,
                            "process_2": 0.0012711229695687503,
                            "process_0": 0.001165282319934704
                        },
                        "gpu_energy": {
                            "process_3": 0.010546939826433288,
                            "process_1": 0.010470998654568575,
                            "process_2": 0.010621981275354742,
                            "process_0": 0.010532916759659727
                        },
                        "ram_energy": {
                            "process_3": 7.374659016449804e-06,
                            "process_1": 7.347160048970707e-06,
                            "process_2": 7.609128584039189e-06,
                            "process_0": 7.204632205239551e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.011778619493984168,
                            "process_1": 0.011700449756877765,
                            "process_2": 0.011900713373507532,
                            "process_0": 0.01170540371179967
                        },
                        "total_energy_joules": {
                            "process_3": 42403.030178343004,
                            "process_1": 42121.619124759956,
                            "process_2": 42842.56814462712,
                            "process_0": 42139.45336247881
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 699.7900932368437,
                        "ram_power_avg": 0.7038971185684204,
                        "cpu_energy_total": 0.004882814240298103,
                        "gpu_energy_total": 0.04217283651601633,
                        "ram_energy_total": 2.953557985469925e-05,
                        "total_energy_kwh": 0.047085186336169135,
                        "total_energy_joules": 169506.6708102089
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09665696294834693,
                        "joules_per_token": 10.345866138318414,
                        "flops_per_joule": 310539886.46736926,
                        "joules_per_flop": 3.2201982533573106e-09
                    },
                    "per-process_emissions": [
                        0.004487065096233269,
                        0.004457286334882585,
                        0.004533576759637695,
                        0.004459173544010084
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0557": {
            "setup": {
                "experiment_id": "0557",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 04:11:38 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_topp_0.3_temp_1.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_p",
                    "decoder_temperature": 1.4,
                    "decoder_top_k": null,
                    "decoder_top_p": 0.3
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 42.908264875935856,
                        "average_latency_ms_per_batch": 5363.533109491982,
                        "throughput_queries_per_sec": 2.9831082745969053,
                        "throughput_tokens_per_sec": 381.8378591484039
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38403047424,
                        "gpu_max_memory_reserved_bytes": 38403047424
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            6.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 42.7,
                        "cpu_memory_usage_bytes": 1980981248
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0557",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_1": 187.98299563569276,
                            "process_0": 196.95162322564497,
                            "process_2": 217.10863032800987,
                            "process_3": 735.6093718978527
                        },
                        "ram_power": {
                            "process_1": 0.7266497611999512,
                            "process_0": 0.6906781196594238,
                            "process_2": 0.7264823913574219,
                            "process_3": 0.7344517707824707
                        },
                        "cpu_energy": {
                            "process_1": 0.0013295018602839266,
                            "process_0": 0.0012438324821596324,
                            "process_2": 0.0013450381259681307,
                            "process_3": 0.0012960027633816935
                        },
                        "gpu_energy": {
                            "process_1": 0.010698906892454119,
                            "process_0": 0.010715509683516089,
                            "process_2": 0.010716960240230833,
                            "process_3": 0.01090618011382638
                        },
                        "ram_energy": {
                            "process_1": 7.81500432435159e-06,
                            "process_0": 7.440137510015358e-06,
                            "process_2": 7.568693852777979e-06,
                            "process_3": 8.12262935216359e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.012036223757062398,
                            "process_0": 0.011966782303185734,
                            "process_2": 0.01206956706005174,
                            "process_3": 0.012210305506560241
                        },
                        "total_energy_joules": {
                            "process_1": 43330.405525424634,
                            "process_0": 43080.41629146864,
                            "process_2": 43450.44141618626,
                            "process_3": 43957.09982361687
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 334.41315527180006,
                        "ram_power_avg": 0.7195655107498169,
                        "cpu_energy_total": 0.005214375231793383,
                        "gpu_energy_total": 0.04303755693002742,
                        "ram_energy_total": 3.094646503930852e-05,
                        "total_energy_kwh": 0.048282878626860114,
                        "total_energy_joules": 173818.36305669637
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09425931594267654,
                        "joules_per_token": 10.60903094828469,
                        "flops_per_joule": 302836716.3467893,
                        "joules_per_flop": 3.302109506612348e-09
                    },
                    "per-process_emissions": [
                        0.004585199440252921,
                        0.004558745718398606,
                        0.00459790157152671,
                        0.004651515882724124
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0557": {
            "setup": {
                "experiment_id": "0557",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 04:11:38 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_topp_0.3_temp_1.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_p",
                    "decoder_temperature": 1.4,
                    "decoder_top_k": null,
                    "decoder_top_p": 0.3
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 42.908264875935856,
                        "average_latency_ms_per_batch": 5363.533109491982,
                        "throughput_queries_per_sec": 2.9831082745969053,
                        "throughput_tokens_per_sec": 381.8378591484039
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38403047424,
                        "gpu_max_memory_reserved_bytes": 38403047424
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            6.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 42.7,
                        "cpu_memory_usage_bytes": 1980981248
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0557",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_1": 187.98299563569276,
                            "process_0": 196.95162322564497,
                            "process_2": 217.10863032800987,
                            "process_3": 735.6093718978527
                        },
                        "ram_power": {
                            "process_1": 0.7266497611999512,
                            "process_0": 0.6906781196594238,
                            "process_2": 0.7264823913574219,
                            "process_3": 0.7344517707824707
                        },
                        "cpu_energy": {
                            "process_1": 0.0013295018602839266,
                            "process_0": 0.0012438324821596324,
                            "process_2": 0.0013450381259681307,
                            "process_3": 0.0012960027633816935
                        },
                        "gpu_energy": {
                            "process_1": 0.010698906892454119,
                            "process_0": 0.010715509683516089,
                            "process_2": 0.010716960240230833,
                            "process_3": 0.01090618011382638
                        },
                        "ram_energy": {
                            "process_1": 7.81500432435159e-06,
                            "process_0": 7.440137510015358e-06,
                            "process_2": 7.568693852777979e-06,
                            "process_3": 8.12262935216359e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.012036223757062398,
                            "process_0": 0.011966782303185734,
                            "process_2": 0.01206956706005174,
                            "process_3": 0.012210305506560241
                        },
                        "total_energy_joules": {
                            "process_1": 43330.405525424634,
                            "process_0": 43080.41629146864,
                            "process_2": 43450.44141618626,
                            "process_3": 43957.09982361687
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 334.41315527180006,
                        "ram_power_avg": 0.7195655107498169,
                        "cpu_energy_total": 0.005214375231793383,
                        "gpu_energy_total": 0.04303755693002742,
                        "ram_energy_total": 3.094646503930852e-05,
                        "total_energy_kwh": 0.048282878626860114,
                        "total_energy_joules": 173818.36305669637
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09425931594267654,
                        "joules_per_token": 10.60903094828469,
                        "flops_per_joule": 302836716.3467893,
                        "joules_per_flop": 3.302109506612348e-09
                    },
                    "per-process_emissions": [
                        0.004585199440252921,
                        0.004558745718398606,
                        0.00459790157152671,
                        0.004651515882724124
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0558": {
            "setup": {
                "experiment_id": "0558",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 04:13:24 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_topp_0.1_temp_0.6",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_p",
                    "decoder_temperature": 0.6,
                    "decoder_top_k": null,
                    "decoder_top_p": 0.1
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 43.99584133696044,
                        "average_latency_ms_per_batch": 5499.480167120055,
                        "throughput_queries_per_sec": 2.909365888008796,
                        "throughput_tokens_per_sec": 372.3988336651259
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38403047424,
                        "gpu_max_memory_reserved_bytes": 38403047424
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            24.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 42.4,
                        "cpu_memory_usage_bytes": 1958174720
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0558",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_3": 248.57366372728958,
                            "process_1": 438.9790022199862,
                            "process_2": 900.535913808947,
                            "process_0": 0.0
                        },
                        "ram_power": {
                            "process_3": 0.7344789505004883,
                            "process_1": 0.7099771499633789,
                            "process_2": 0.7344846725463867,
                            "process_0": 0.6816787719726562
                        },
                        "cpu_energy": {
                            "process_3": 0.001378129540438749,
                            "process_1": 0.0013405856860899796,
                            "process_2": 0.0013322647620934733,
                            "process_0": 0.0012929466074656374
                        },
                        "gpu_energy": {
                            "process_3": 0.01065653908077735,
                            "process_1": 0.010852586182062751,
                            "process_2": 0.0107156510725126,
                            "process_0": 0.010846069232400168
                        },
                        "ram_energy": {
                            "process_3": 7.830486081596675e-06,
                            "process_1": 7.905197457046609e-06,
                            "process_2": 7.910319348615085e-06,
                            "process_0": 7.270902382049576e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.012042499107297695,
                            "process_1": 0.012201077065609781,
                            "process_2": 0.012055826153954686,
                            "process_0": 0.01214628674224786
                        },
                        "total_energy_joules": {
                            "process_3": 43352.9967862717,
                            "process_1": 43923.87743619521,
                            "process_2": 43400.97415423687,
                            "process_0": 43726.6322720923
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 397.0221449390557,
                        "ram_power_avg": 0.7151548862457275,
                        "cpu_energy_total": 0.005343926596087839,
                        "gpu_energy_total": 0.04307084556775287,
                        "ram_energy_total": 3.091690526930794e-05,
                        "total_energy_kwh": 0.04844568906911002,
                        "total_energy_joules": 174404.48064879607
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09394254057608181,
                        "joules_per_token": 10.64480472709937,
                        "flops_per_joule": 301818979.15148187,
                        "joules_per_flop": 3.3132442592289853e-09
                    },
                    "per-process_emissions": [
                        0.004587590034925057,
                        0.004648000308144046,
                        0.004592666973349038,
                        0.004627127934459322
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0558": {
            "setup": {
                "experiment_id": "0558",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 04:13:24 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_topp_0.1_temp_0.6",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_p",
                    "decoder_temperature": 0.6,
                    "decoder_top_k": null,
                    "decoder_top_p": 0.1
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 43.99584133696044,
                        "average_latency_ms_per_batch": 5499.480167120055,
                        "throughput_queries_per_sec": 2.909365888008796,
                        "throughput_tokens_per_sec": 372.3988336651259
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38403047424,
                        "gpu_max_memory_reserved_bytes": 38403047424
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            24.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 42.4,
                        "cpu_memory_usage_bytes": 1958174720
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0558",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_3": 248.57366372728958,
                            "process_1": 438.9790022199862,
                            "process_2": 900.535913808947,
                            "process_0": 0.0
                        },
                        "ram_power": {
                            "process_3": 0.7344789505004883,
                            "process_1": 0.7099771499633789,
                            "process_2": 0.7344846725463867,
                            "process_0": 0.6816787719726562
                        },
                        "cpu_energy": {
                            "process_3": 0.001378129540438749,
                            "process_1": 0.0013405856860899796,
                            "process_2": 0.0013322647620934733,
                            "process_0": 0.0012929466074656374
                        },
                        "gpu_energy": {
                            "process_3": 0.01065653908077735,
                            "process_1": 0.010852586182062751,
                            "process_2": 0.0107156510725126,
                            "process_0": 0.010846069232400168
                        },
                        "ram_energy": {
                            "process_3": 7.830486081596675e-06,
                            "process_1": 7.905197457046609e-06,
                            "process_2": 7.910319348615085e-06,
                            "process_0": 7.270902382049576e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.012042499107297695,
                            "process_1": 0.012201077065609781,
                            "process_2": 0.012055826153954686,
                            "process_0": 0.01214628674224786
                        },
                        "total_energy_joules": {
                            "process_3": 43352.9967862717,
                            "process_1": 43923.87743619521,
                            "process_2": 43400.97415423687,
                            "process_0": 43726.6322720923
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 397.0221449390557,
                        "ram_power_avg": 0.7151548862457275,
                        "cpu_energy_total": 0.005343926596087839,
                        "gpu_energy_total": 0.04307084556775287,
                        "ram_energy_total": 3.091690526930794e-05,
                        "total_energy_kwh": 0.04844568906911002,
                        "total_energy_joules": 174404.48064879607
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09394254057608181,
                        "joules_per_token": 10.64480472709937,
                        "flops_per_joule": 301818979.15148187,
                        "joules_per_flop": 3.3132442592289853e-09
                    },
                    "per-process_emissions": [
                        0.004587590034925057,
                        0.004648000308144046,
                        0.004592666973349038,
                        0.004627127934459322
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0559": {
            "setup": {
                "experiment_id": "0559",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 04:15:09 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_50_temp_0.6",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 0.6,
                    "decoder_top_k": 50,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 41.54796123399865,
                        "average_latency_ms_per_batch": 5193.495154249831,
                        "throughput_queries_per_sec": 3.080776918970882,
                        "throughput_tokens_per_sec": 394.33944562827287
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38449184768,
                        "gpu_max_memory_reserved_bytes": 38449184768
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            25.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 42.8,
                        "cpu_memory_usage_bytes": 1978953728
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0559",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_1": 0.0,
                            "process_2": 893.0135889564275,
                            "process_3": 902.1121387796609,
                            "process_0": 977.2312166735524
                        },
                        "ram_power": {
                            "process_1": 0.7252821922302246,
                            "process_2": 0.7323875427246094,
                            "process_3": 0.7323946952819824,
                            "process_0": 0.689051628112793
                        },
                        "cpu_energy": {
                            "process_1": 0.0012455951588981405,
                            "process_2": 0.001199326623283923,
                            "process_3": 0.001146078898062115,
                            "process_0": 0.0011904712482155447
                        },
                        "gpu_energy": {
                            "process_1": 0.01065150713230878,
                            "process_2": 0.010584518467608106,
                            "process_3": 0.010585456801692317,
                            "process_0": 0.010540603710254004
                        },
                        "ram_energy": {
                            "process_1": 7.576447583358436e-06,
                            "process_2": 7.847304927908964e-06,
                            "process_3": 7.946765683711538e-06,
                            "process_0": 6.962349168832051e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.011904678738790276,
                            "process_2": 0.011791692395819942,
                            "process_3": 0.011739482465438142,
                            "process_0": 0.011738037307638375
                        },
                        "total_energy_joules": {
                            "process_1": 42856.843459644995,
                            "process_2": 42450.092624951794,
                            "process_3": 42262.13687557731,
                            "process_0": 42256.93430749815
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 693.0892361024102,
                        "ram_power_avg": 0.7197790145874023,
                        "cpu_energy_total": 0.004781471928459723,
                        "gpu_energy_total": 0.042362086111863206,
                        "ram_energy_total": 3.033286736381099e-05,
                        "total_energy_kwh": 0.047173890907686734,
                        "total_energy_joules": 169826.00726767225
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09647521168048344,
                        "joules_per_token": 10.365356888896011,
                        "flops_per_joule": 309955955.24952424,
                        "joules_per_flop": 3.2262648388057866e-09
                    },
                    "per-process_emissions": [
                        0.004535087365542156,
                        0.004492045218187607,
                        0.00447215584520866,
                        0.004471605312344839
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0559": {
            "setup": {
                "experiment_id": "0559",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 04:15:09 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_50_temp_0.6",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 0.6,
                    "decoder_top_k": 50,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 41.54796123399865,
                        "average_latency_ms_per_batch": 5193.495154249831,
                        "throughput_queries_per_sec": 3.080776918970882,
                        "throughput_tokens_per_sec": 394.33944562827287
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38449184768,
                        "gpu_max_memory_reserved_bytes": 38449184768
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            25.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 42.8,
                        "cpu_memory_usage_bytes": 1978953728
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0559",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_1": 0.0,
                            "process_2": 893.0135889564275,
                            "process_3": 902.1121387796609,
                            "process_0": 977.2312166735524
                        },
                        "ram_power": {
                            "process_1": 0.7252821922302246,
                            "process_2": 0.7323875427246094,
                            "process_3": 0.7323946952819824,
                            "process_0": 0.689051628112793
                        },
                        "cpu_energy": {
                            "process_1": 0.0012455951588981405,
                            "process_2": 0.001199326623283923,
                            "process_3": 0.001146078898062115,
                            "process_0": 0.0011904712482155447
                        },
                        "gpu_energy": {
                            "process_1": 0.01065150713230878,
                            "process_2": 0.010584518467608106,
                            "process_3": 0.010585456801692317,
                            "process_0": 0.010540603710254004
                        },
                        "ram_energy": {
                            "process_1": 7.576447583358436e-06,
                            "process_2": 7.847304927908964e-06,
                            "process_3": 7.946765683711538e-06,
                            "process_0": 6.962349168832051e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.011904678738790276,
                            "process_2": 0.011791692395819942,
                            "process_3": 0.011739482465438142,
                            "process_0": 0.011738037307638375
                        },
                        "total_energy_joules": {
                            "process_1": 42856.843459644995,
                            "process_2": 42450.092624951794,
                            "process_3": 42262.13687557731,
                            "process_0": 42256.93430749815
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 693.0892361024102,
                        "ram_power_avg": 0.7197790145874023,
                        "cpu_energy_total": 0.004781471928459723,
                        "gpu_energy_total": 0.042362086111863206,
                        "ram_energy_total": 3.033286736381099e-05,
                        "total_energy_kwh": 0.047173890907686734,
                        "total_energy_joules": 169826.00726767225
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09647521168048344,
                        "joules_per_token": 10.365356888896011,
                        "flops_per_joule": 309955955.24952424,
                        "joules_per_flop": 3.2262648388057866e-09
                    },
                    "per-process_emissions": [
                        0.004535087365542156,
                        0.004492045218187607,
                        0.00447215584520866,
                        0.004471605312344839
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0560": {
            "setup": {
                "experiment_id": "0560",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 04:16:56 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_topp_0.7_temp_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_p",
                    "decoder_temperature": 0.8,
                    "decoder_top_k": null,
                    "decoder_top_p": 0.7
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 43.805997670046054,
                        "average_latency_ms_per_batch": 5475.749708755757,
                        "throughput_queries_per_sec": 2.921974314205031,
                        "throughput_tokens_per_sec": 374.01271221824396
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38403047424,
                        "gpu_max_memory_reserved_bytes": 38403047424
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            15.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 42.7,
                        "cpu_memory_usage_bytes": 1980858368
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0560",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_2": 597.782042410579,
                            "process_0": 644.7719263306649,
                            "process_1": 252.61401224621173,
                            "process_3": 1292.9550611727584
                        },
                        "ram_power": {
                            "process_2": 0.7269716262817383,
                            "process_0": 0.690608024597168,
                            "process_1": 0.733095645904541,
                            "process_3": 0.7326478958129883
                        },
                        "cpu_energy": {
                            "process_2": 0.0013459841761523418,
                            "process_0": 0.0013005714908376831,
                            "process_1": 0.0013780266458434198,
                            "process_3": 0.0013917625387166483
                        },
                        "gpu_energy": {
                            "process_2": 0.01090803094863979,
                            "process_0": 0.010798907528009494,
                            "process_1": 0.010669703257978114,
                            "process_3": 0.010677330764080395
                        },
                        "ram_energy": {
                            "process_2": 7.948317601965998e-06,
                            "process_0": 7.233153847616019e-06,
                            "process_1": 7.907664302863446e-06,
                            "process_3": 8.329500879800448e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.0122619634423941,
                            "process_0": 0.012106712172694791,
                            "process_1": 0.012055637568124394,
                            "process_3": 0.012077422803676845
                        },
                        "total_energy_joules": {
                            "process_2": 44143.068392618756,
                            "process_0": 43584.16382170125,
                            "process_1": 43400.29524524782,
                            "process_3": 43478.72209323664
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 697.0307605400535,
                        "ram_power_avg": 0.7208307981491089,
                        "cpu_energy_total": 0.005416344851550093,
                        "gpu_energy_total": 0.04305397249870779,
                        "ram_energy_total": 3.1418636632245906e-05,
                        "total_energy_kwh": 0.04850173598689013,
                        "total_energy_joules": 174606.24955280445
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09383398384629496,
                        "joules_per_token": 10.657119723681912,
                        "flops_per_joule": 301470207.6453743,
                        "joules_per_flop": 3.3170773583581463e-09
                    },
                    "per-process_emissions": [
                        0.004671194973380032,
                        0.004612052002188081,
                        0.004592595131576988,
                        0.004600894217060694
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0560": {
            "setup": {
                "experiment_id": "0560",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 04:16:56 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_topp_0.7_temp_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_p",
                    "decoder_temperature": 0.8,
                    "decoder_top_k": null,
                    "decoder_top_p": 0.7
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 43.805997670046054,
                        "average_latency_ms_per_batch": 5475.749708755757,
                        "throughput_queries_per_sec": 2.921974314205031,
                        "throughput_tokens_per_sec": 374.01271221824396
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38403047424,
                        "gpu_max_memory_reserved_bytes": 38403047424
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            15.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 42.7,
                        "cpu_memory_usage_bytes": 1980858368
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0560",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_2": 597.782042410579,
                            "process_0": 644.7719263306649,
                            "process_1": 252.61401224621173,
                            "process_3": 1292.9550611727584
                        },
                        "ram_power": {
                            "process_2": 0.7269716262817383,
                            "process_0": 0.690608024597168,
                            "process_1": 0.733095645904541,
                            "process_3": 0.7326478958129883
                        },
                        "cpu_energy": {
                            "process_2": 0.0013459841761523418,
                            "process_0": 0.0013005714908376831,
                            "process_1": 0.0013780266458434198,
                            "process_3": 0.0013917625387166483
                        },
                        "gpu_energy": {
                            "process_2": 0.01090803094863979,
                            "process_0": 0.010798907528009494,
                            "process_1": 0.010669703257978114,
                            "process_3": 0.010677330764080395
                        },
                        "ram_energy": {
                            "process_2": 7.948317601965998e-06,
                            "process_0": 7.233153847616019e-06,
                            "process_1": 7.907664302863446e-06,
                            "process_3": 8.329500879800448e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.0122619634423941,
                            "process_0": 0.012106712172694791,
                            "process_1": 0.012055637568124394,
                            "process_3": 0.012077422803676845
                        },
                        "total_energy_joules": {
                            "process_2": 44143.068392618756,
                            "process_0": 43584.16382170125,
                            "process_1": 43400.29524524782,
                            "process_3": 43478.72209323664
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 697.0307605400535,
                        "ram_power_avg": 0.7208307981491089,
                        "cpu_energy_total": 0.005416344851550093,
                        "gpu_energy_total": 0.04305397249870779,
                        "ram_energy_total": 3.1418636632245906e-05,
                        "total_energy_kwh": 0.04850173598689013,
                        "total_energy_joules": 174606.24955280445
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09383398384629496,
                        "joules_per_token": 10.657119723681912,
                        "flops_per_joule": 301470207.6453743,
                        "joules_per_flop": 3.3170773583581463e-09
                    },
                    "per-process_emissions": [
                        0.004671194973380032,
                        0.004612052002188081,
                        0.004592595131576988,
                        0.004600894217060694
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0561": {
            "setup": {
                "experiment_id": "0561",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 04:18:51 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_burst_0.2_0.4_6.0_8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.2,
                    "delay_max": 0.4,
                    "simulate_burst": true,
                    "burst_interval": 6.0,
                    "burst_size": 8
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 51.51507205114467,
                        "average_latency_ms_per_batch": 6439.384006393084,
                        "throughput_queries_per_sec": 2.4847097151086257,
                        "throughput_tokens_per_sec": 318.0428435339041
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38352715776,
                        "gpu_max_memory_reserved_bytes": 38352715776
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            28.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 43.2,
                        "cpu_memory_usage_bytes": 1971351552
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0561",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_3": 450.94959556233835,
                            "process_1": 184.26161166125118,
                            "process_2": 582.1554816070031,
                            "process_0": 173.15917474895988
                        },
                        "ram_power": {
                            "process_3": 0.7236299514770509,
                            "process_1": 0.7297968864440919,
                            "process_2": 0.7061119079589844,
                            "process_0": 0.6859459877014161
                        },
                        "cpu_energy": {
                            "process_3": 0.0014235401572113915,
                            "process_1": 0.0015030036589050726,
                            "process_2": 0.0014329238842801712,
                            "process_0": 0.0014945601086565146
                        },
                        "gpu_energy": {
                            "process_3": 0.011197860347168742,
                            "process_1": 0.011159528094285776,
                            "process_2": 0.011190031729794825,
                            "process_0": 0.011160915595395338
                        },
                        "ram_energy": {
                            "process_3": 9.340914398037306e-06,
                            "process_1": 8.871102711692603e-06,
                            "process_2": 9.207339859696077e-06,
                            "process_0": 8.995445663456007e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.012630741418778173,
                            "process_1": 0.012671402855902542,
                            "process_2": 0.012632162953934694,
                            "process_0": 0.012664471149715311
                        },
                        "total_energy_joules": {
                            "process_3": 45470.669107601425,
                            "process_1": 45617.05028124915,
                            "process_2": 45475.7866341649,
                            "process_0": 45592.096138975125
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 347.63146589488815,
                        "ram_power_avg": 0.7113711833953857,
                        "cpu_energy_total": 0.00585402780905315,
                        "gpu_energy_total": 0.04470833576664468,
                        "ram_energy_total": 3.641480263288199e-05,
                        "total_energy_kwh": 0.05059877837833072,
                        "total_energy_joules": 182155.6021619906
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.08994507885313208,
                        "joules_per_token": 11.117895639769934,
                        "flops_per_joule": 288975917.7543857,
                        "joules_per_flop": 3.46049597409687e-09
                    },
                    "per-process_emissions": [
                        0.004811680943483545,
                        0.0048271709179560736,
                        0.004812222477301422,
                        0.004824530284484048
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0561": {
            "setup": {
                "experiment_id": "0561",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 04:18:51 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_burst_0.2_0.4_6.0_8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.2,
                    "delay_max": 0.4,
                    "simulate_burst": true,
                    "burst_interval": 6.0,
                    "burst_size": 8
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 51.51507205114467,
                        "average_latency_ms_per_batch": 6439.384006393084,
                        "throughput_queries_per_sec": 2.4847097151086257,
                        "throughput_tokens_per_sec": 318.0428435339041
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38352715776,
                        "gpu_max_memory_reserved_bytes": 38352715776
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            28.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 43.2,
                        "cpu_memory_usage_bytes": 1971351552
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0561",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_3": 450.94959556233835,
                            "process_1": 184.26161166125118,
                            "process_2": 582.1554816070031,
                            "process_0": 173.15917474895988
                        },
                        "ram_power": {
                            "process_3": 0.7236299514770509,
                            "process_1": 0.7297968864440919,
                            "process_2": 0.7061119079589844,
                            "process_0": 0.6859459877014161
                        },
                        "cpu_energy": {
                            "process_3": 0.0014235401572113915,
                            "process_1": 0.0015030036589050726,
                            "process_2": 0.0014329238842801712,
                            "process_0": 0.0014945601086565146
                        },
                        "gpu_energy": {
                            "process_3": 0.011197860347168742,
                            "process_1": 0.011159528094285776,
                            "process_2": 0.011190031729794825,
                            "process_0": 0.011160915595395338
                        },
                        "ram_energy": {
                            "process_3": 9.340914398037306e-06,
                            "process_1": 8.871102711692603e-06,
                            "process_2": 9.207339859696077e-06,
                            "process_0": 8.995445663456007e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.012630741418778173,
                            "process_1": 0.012671402855902542,
                            "process_2": 0.012632162953934694,
                            "process_0": 0.012664471149715311
                        },
                        "total_energy_joules": {
                            "process_3": 45470.669107601425,
                            "process_1": 45617.05028124915,
                            "process_2": 45475.7866341649,
                            "process_0": 45592.096138975125
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 347.63146589488815,
                        "ram_power_avg": 0.7113711833953857,
                        "cpu_energy_total": 0.00585402780905315,
                        "gpu_energy_total": 0.04470833576664468,
                        "ram_energy_total": 3.641480263288199e-05,
                        "total_energy_kwh": 0.05059877837833072,
                        "total_energy_joules": 182155.6021619906
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.08994507885313208,
                        "joules_per_token": 11.117895639769934,
                        "flops_per_joule": 288975917.7543857,
                        "joules_per_flop": 3.46049597409687e-09
                    },
                    "per-process_emissions": [
                        0.004811680943483545,
                        0.0048271709179560736,
                        0.004812222477301422,
                        0.004824530284484048
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0562": {
            "setup": {
                "experiment_id": "0562",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 04:20:35 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_200_temp_0.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 0.4,
                    "decoder_top_k": 200,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 41.28623305802466,
                        "average_latency_ms_per_batch": 5160.779132253083,
                        "throughput_queries_per_sec": 3.1003070641030805,
                        "throughput_tokens_per_sec": 396.8393042051943
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38449184768,
                        "gpu_max_memory_reserved_bytes": 38449184768
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 39.9,
                        "cpu_memory_usage_bytes": 1980538880
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0562",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 1010.1949469098661,
                            "process_3": 1298.7804809844229,
                            "process_2": 782.125449674452,
                            "process_1": 680.3686902886607
                        },
                        "ram_power": {
                            "process_0": 0.6905064582824707,
                            "process_3": 0.7091631889343262,
                            "process_2": 0.7326779365539551,
                            "process_1": 0.7262020111083985
                        },
                        "cpu_energy": {
                            "process_0": 0.001129420993196618,
                            "process_3": 0.0012095188658240658,
                            "process_2": 0.0011794633500285272,
                            "process_1": 0.0011870570284172575
                        },
                        "gpu_energy": {
                            "process_0": 0.010500552567103583,
                            "process_3": 0.010457861421839354,
                            "process_2": 0.010526990643811018,
                            "process_1": 0.010531825092122915
                        },
                        "ram_energy": {
                            "process_0": 7.332725746995924e-06,
                            "process_3": 7.719900395034427e-06,
                            "process_2": 7.737138350566793e-06,
                            "process_1": 7.75688671424638e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.011637306286047202,
                            "process_3": 0.011675100188058448,
                            "process_2": 0.011714191132190108,
                            "process_1": 0.011726639007254416
                        },
                        "total_energy_joules": {
                            "process_0": 41894.302629769925,
                            "process_3": 42030.360677010416,
                            "process_2": 42171.08807588439,
                            "process_1": 42215.9004261159
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 942.8673919643505,
                        "ram_power_avg": 0.7146373987197876,
                        "cpu_energy_total": 0.004705460237466468,
                        "gpu_energy_total": 0.04201722972487687,
                        "ram_energy_total": 3.0546651206843524e-05,
                        "total_energy_kwh": 0.04675323661355017,
                        "total_energy_joules": 168311.65180878062
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09734323098803589,
                        "joules_per_token": 10.27292796684452,
                        "flops_per_joule": 312744731.2362358,
                        "joules_per_flop": 3.197495913191378e-09
                    },
                    "per-process_emissions": [
                        0.004433231829669681,
                        0.004447629416640866,
                        0.004462521111807822,
                        0.0044672631298135704
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0562": {
            "setup": {
                "experiment_id": "0562",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 04:20:35 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_200_temp_0.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 0.4,
                    "decoder_top_k": 200,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 41.28623305802466,
                        "average_latency_ms_per_batch": 5160.779132253083,
                        "throughput_queries_per_sec": 3.1003070641030805,
                        "throughput_tokens_per_sec": 396.8393042051943
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38449184768,
                        "gpu_max_memory_reserved_bytes": 38449184768
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 39.9,
                        "cpu_memory_usage_bytes": 1980538880
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0562",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 1010.1949469098661,
                            "process_3": 1298.7804809844229,
                            "process_2": 782.125449674452,
                            "process_1": 680.3686902886607
                        },
                        "ram_power": {
                            "process_0": 0.6905064582824707,
                            "process_3": 0.7091631889343262,
                            "process_2": 0.7326779365539551,
                            "process_1": 0.7262020111083985
                        },
                        "cpu_energy": {
                            "process_0": 0.001129420993196618,
                            "process_3": 0.0012095188658240658,
                            "process_2": 0.0011794633500285272,
                            "process_1": 0.0011870570284172575
                        },
                        "gpu_energy": {
                            "process_0": 0.010500552567103583,
                            "process_3": 0.010457861421839354,
                            "process_2": 0.010526990643811018,
                            "process_1": 0.010531825092122915
                        },
                        "ram_energy": {
                            "process_0": 7.332725746995924e-06,
                            "process_3": 7.719900395034427e-06,
                            "process_2": 7.737138350566793e-06,
                            "process_1": 7.75688671424638e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.011637306286047202,
                            "process_3": 0.011675100188058448,
                            "process_2": 0.011714191132190108,
                            "process_1": 0.011726639007254416
                        },
                        "total_energy_joules": {
                            "process_0": 41894.302629769925,
                            "process_3": 42030.360677010416,
                            "process_2": 42171.08807588439,
                            "process_1": 42215.9004261159
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 942.8673919643505,
                        "ram_power_avg": 0.7146373987197876,
                        "cpu_energy_total": 0.004705460237466468,
                        "gpu_energy_total": 0.04201722972487687,
                        "ram_energy_total": 3.0546651206843524e-05,
                        "total_energy_kwh": 0.04675323661355017,
                        "total_energy_joules": 168311.65180878062
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09734323098803589,
                        "joules_per_token": 10.27292796684452,
                        "flops_per_joule": 312744731.2362358,
                        "joules_per_flop": 3.197495913191378e-09
                    },
                    "per-process_emissions": [
                        0.004433231829669681,
                        0.004447629416640866,
                        0.004462521111807822,
                        0.0044672631298135704
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0563": {
            "setup": {
                "experiment_id": "0563",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 04:22:19 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_topp_0.7_temp_0.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_p",
                    "decoder_temperature": 0.2,
                    "decoder_top_k": null,
                    "decoder_top_p": 0.7
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 41.81198785902234,
                        "average_latency_ms_per_batch": 5226.498482377792,
                        "throughput_queries_per_sec": 3.061322997403954,
                        "throughput_tokens_per_sec": 391.84934366770614
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38403047424,
                        "gpu_max_memory_reserved_bytes": 38403047424
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 39.0,
                        "cpu_memory_usage_bytes": 1978089472
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0563",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_2": 952.4441633539344,
                            "process_0": 960.7403242267883,
                            "process_3": 626.5915205392209,
                            "process_1": 916.3475210017554
                        },
                        "ram_power": {
                            "process_2": 0.7337794303894043,
                            "process_0": 0.6896538734436035,
                            "process_3": 0.7325448989868164,
                            "process_1": 0.733278751373291
                        },
                        "cpu_energy": {
                            "process_2": 0.0011979903136561914,
                            "process_0": 0.001146658960286004,
                            "process_3": 0.0012147670630074574,
                            "process_1": 0.0012095087250636428
                        },
                        "gpu_energy": {
                            "process_2": 0.01062577294505651,
                            "process_0": 0.01062577294505651,
                            "process_3": 0.010721853299699546,
                            "process_1": 0.010633076839791755
                        },
                        "ram_energy": {
                            "process_2": 7.870891600475954e-06,
                            "process_0": 7.064969098297762e-06,
                            "process_3": 7.95388476519311e-06,
                            "process_1": 7.886500358595194e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.011831634150313184,
                            "process_0": 0.011779496874440815,
                            "process_3": 0.0119445742474722,
                            "process_1": 0.01185047206521399
                        },
                        "total_energy_joules": {
                            "process_2": 42593.882941127464,
                            "process_0": 42406.18874798693,
                            "process_3": 43000.46729089992,
                            "process_1": 42661.699434770366
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 864.0308822804249,
                        "ram_power_avg": 0.7223142385482788,
                        "cpu_energy_total": 0.004768925062013295,
                        "gpu_energy_total": 0.04260647602960432,
                        "ram_energy_total": 3.077624582256202e-05,
                        "total_energy_kwh": 0.04740617733744019,
                        "total_energy_joules": 170662.2384147847
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09600249095631593,
                        "joules_per_token": 10.416396387621136,
                        "flops_per_joule": 308437196.1706548,
                        "joules_per_flop": 3.24215111671133e-09
                    },
                    "per-process_emissions": [
                        0.004507261029561808,
                        0.004487399334318229,
                        0.0045502855595745345,
                        0.004514437333243269
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0563": {
            "setup": {
                "experiment_id": "0563",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 04:22:19 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_topp_0.7_temp_0.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_p",
                    "decoder_temperature": 0.2,
                    "decoder_top_k": null,
                    "decoder_top_p": 0.7
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 41.81198785902234,
                        "average_latency_ms_per_batch": 5226.498482377792,
                        "throughput_queries_per_sec": 3.061322997403954,
                        "throughput_tokens_per_sec": 391.84934366770614
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38403047424,
                        "gpu_max_memory_reserved_bytes": 38403047424
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 39.0,
                        "cpu_memory_usage_bytes": 1978089472
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0563",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_2": 952.4441633539344,
                            "process_0": 960.7403242267883,
                            "process_3": 626.5915205392209,
                            "process_1": 916.3475210017554
                        },
                        "ram_power": {
                            "process_2": 0.7337794303894043,
                            "process_0": 0.6896538734436035,
                            "process_3": 0.7325448989868164,
                            "process_1": 0.733278751373291
                        },
                        "cpu_energy": {
                            "process_2": 0.0011979903136561914,
                            "process_0": 0.001146658960286004,
                            "process_3": 0.0012147670630074574,
                            "process_1": 0.0012095087250636428
                        },
                        "gpu_energy": {
                            "process_2": 0.01062577294505651,
                            "process_0": 0.01062577294505651,
                            "process_3": 0.010721853299699546,
                            "process_1": 0.010633076839791755
                        },
                        "ram_energy": {
                            "process_2": 7.870891600475954e-06,
                            "process_0": 7.064969098297762e-06,
                            "process_3": 7.95388476519311e-06,
                            "process_1": 7.886500358595194e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.011831634150313184,
                            "process_0": 0.011779496874440815,
                            "process_3": 0.0119445742474722,
                            "process_1": 0.01185047206521399
                        },
                        "total_energy_joules": {
                            "process_2": 42593.882941127464,
                            "process_0": 42406.18874798693,
                            "process_3": 43000.46729089992,
                            "process_1": 42661.699434770366
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 864.0308822804249,
                        "ram_power_avg": 0.7223142385482788,
                        "cpu_energy_total": 0.004768925062013295,
                        "gpu_energy_total": 0.04260647602960432,
                        "ram_energy_total": 3.077624582256202e-05,
                        "total_energy_kwh": 0.04740617733744019,
                        "total_energy_joules": 170662.2384147847
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09600249095631593,
                        "joules_per_token": 10.416396387621136,
                        "flops_per_joule": 308437196.1706548,
                        "joules_per_flop": 3.24215111671133e-09
                    },
                    "per-process_emissions": [
                        0.004507261029561808,
                        0.004487399334318229,
                        0.0045502855595745345,
                        0.004514437333243269
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0565": {
            "setup": {
                "experiment_id": "0565",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 04:25:27 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_20_temp_0.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 0.4,
                    "decoder_top_k": 20,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 40.232405804039445,
                        "average_latency_ms_per_batch": 5029.050725504931,
                        "throughput_queries_per_sec": 3.1815149365775297,
                        "throughput_tokens_per_sec": 407.2339118819238
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38449184768,
                        "gpu_max_memory_reserved_bytes": 38449184768
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            27.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 32.7,
                        "cpu_memory_usage_bytes": 1974595584
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0565",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_2": 1308.2587301261765,
                            "process_3": 83.24865034020121,
                            "process_0": 76.54676610569999,
                            "process_1": 80.95871598993875
                        },
                        "ram_power": {
                            "process_2": 0.7091531753540039,
                            "process_3": 0.7248787879943848,
                            "process_0": 0.6884336471557617,
                            "process_1": 0.7323102951049805
                        },
                        "cpu_energy": {
                            "process_2": 0.0012689883816874498,
                            "process_3": 0.0013109770518422012,
                            "process_0": 0.0012776809885635886,
                            "process_1": 0.0012899031157576249
                        },
                        "gpu_energy": {
                            "process_2": 0.01041718416707127,
                            "process_3": 0.01044063779694504,
                            "process_0": 0.010443132798942045,
                            "process_1": 0.01045057947156458
                        },
                        "ram_energy": {
                            "process_2": 6.679257331000267e-06,
                            "process_3": 7.128168977530873e-06,
                            "process_0": 6.520420525299061e-06,
                            "process_1": 7.044432379997371e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.011692851806089716,
                            "process_3": 0.011758743017764773,
                            "process_0": 0.011727334208030935,
                            "process_1": 0.011747527019702199
                        },
                        "total_energy_joules": {
                            "process_2": 42094.26650192298,
                            "process_3": 42331.47486395318,
                            "process_0": 42218.40314891137,
                            "process_1": 42291.097270927916
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 387.25321564050415,
                        "ram_power_avg": 0.7136939764022827,
                        "cpu_energy_total": 0.005147549537850865,
                        "gpu_energy_total": 0.041751534234522936,
                        "ram_energy_total": 2.7372279213827574e-05,
                        "total_energy_kwh": 0.04692645605158762,
                        "total_energy_joules": 168935.24178571545
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09698390831193264,
                        "joules_per_token": 10.310988878522672,
                        "flops_per_joule": 311590297.87066567,
                        "joules_per_flop": 3.209342546394298e-09
                    },
                    "per-process_emissions": [
                        0.004454391895529878,
                        0.00447949315261749,
                        0.004467527966549385,
                        0.004475220418155553
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0565": {
            "setup": {
                "experiment_id": "0565",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 04:25:27 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_20_temp_0.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 0.4,
                    "decoder_top_k": 20,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 40.232405804039445,
                        "average_latency_ms_per_batch": 5029.050725504931,
                        "throughput_queries_per_sec": 3.1815149365775297,
                        "throughput_tokens_per_sec": 407.2339118819238
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38449184768,
                        "gpu_max_memory_reserved_bytes": 38449184768
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            27.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 32.7,
                        "cpu_memory_usage_bytes": 1974595584
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0565",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_2": 1308.2587301261765,
                            "process_3": 83.24865034020121,
                            "process_0": 76.54676610569999,
                            "process_1": 80.95871598993875
                        },
                        "ram_power": {
                            "process_2": 0.7091531753540039,
                            "process_3": 0.7248787879943848,
                            "process_0": 0.6884336471557617,
                            "process_1": 0.7323102951049805
                        },
                        "cpu_energy": {
                            "process_2": 0.0012689883816874498,
                            "process_3": 0.0013109770518422012,
                            "process_0": 0.0012776809885635886,
                            "process_1": 0.0012899031157576249
                        },
                        "gpu_energy": {
                            "process_2": 0.01041718416707127,
                            "process_3": 0.01044063779694504,
                            "process_0": 0.010443132798942045,
                            "process_1": 0.01045057947156458
                        },
                        "ram_energy": {
                            "process_2": 6.679257331000267e-06,
                            "process_3": 7.128168977530873e-06,
                            "process_0": 6.520420525299061e-06,
                            "process_1": 7.044432379997371e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.011692851806089716,
                            "process_3": 0.011758743017764773,
                            "process_0": 0.011727334208030935,
                            "process_1": 0.011747527019702199
                        },
                        "total_energy_joules": {
                            "process_2": 42094.26650192298,
                            "process_3": 42331.47486395318,
                            "process_0": 42218.40314891137,
                            "process_1": 42291.097270927916
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 387.25321564050415,
                        "ram_power_avg": 0.7136939764022827,
                        "cpu_energy_total": 0.005147549537850865,
                        "gpu_energy_total": 0.041751534234522936,
                        "ram_energy_total": 2.7372279213827574e-05,
                        "total_energy_kwh": 0.04692645605158762,
                        "total_energy_joules": 168935.24178571545
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09698390831193264,
                        "joules_per_token": 10.310988878522672,
                        "flops_per_joule": 311590297.87066567,
                        "joules_per_flop": 3.209342546394298e-09
                    },
                    "per-process_emissions": [
                        0.004454391895529878,
                        0.00447949315261749,
                        0.004467527966549385,
                        0.004475220418155553
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0566": {
            "setup": {
                "experiment_id": "0566",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 04:27:06 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_20_temp_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 20,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 39.75235354201868,
                        "average_latency_ms_per_batch": 4969.044192752335,
                        "throughput_queries_per_sec": 3.2199351382982284,
                        "throughput_tokens_per_sec": 412.15169770217324
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38451281920,
                        "gpu_max_memory_reserved_bytes": 38451281920
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 29.7,
                        "cpu_memory_usage_bytes": 1978048512
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0566",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_1": 1008.3806571708318,
                            "process_2": 900.1024403832176,
                            "process_3": 1058.9746710080944,
                            "process_0": 1039.2440304701804
                        },
                        "ram_power": {
                            "process_1": 0.7324190139770509,
                            "process_2": 0.732081413269043,
                            "process_3": 0.7087225914001465,
                            "process_0": 0.6896452903747559
                        },
                        "cpu_energy": {
                            "process_1": 0.0012267243233436604,
                            "process_2": 0.0012204932987897339,
                            "process_3": 0.001219885056720159,
                            "process_0": 0.0012201341838426743
                        },
                        "gpu_energy": {
                            "process_1": 0.01026006265249002,
                            "process_2": 0.010344250497617224,
                            "process_3": 0.01039302025885469,
                            "process_0": 0.01039302025885469
                        },
                        "ram_energy": {
                            "process_1": 7.141880673550721e-06,
                            "process_2": 6.9842413091092865e-06,
                            "process_3": 6.801206839532211e-06,
                            "process_0": 6.593091174955047e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.01149392885650723,
                            "process_2": 0.011571728037716073,
                            "process_3": 0.01161970652241438,
                            "process_0": 0.011619747533872321
                        },
                        "total_energy_joules": {
                            "process_1": 41378.143883426026,
                            "process_2": 41658.22093577786,
                            "process_3": 41830.94348069176,
                            "process_0": 41831.09112194036
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1001.6754497580811,
                        "ram_power_avg": 0.715717077255249,
                        "cpu_energy_total": 0.0048872368626962275,
                        "gpu_energy_total": 0.041390353667816626,
                        "ram_energy_total": 2.7520419997147264e-05,
                        "total_energy_kwh": 0.04630511095051,
                        "total_energy_joules": 166698.399421836
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09828528682233913,
                        "joules_per_token": 10.17446285533667,
                        "flops_per_joule": 315771372.07934594,
                        "joules_per_flop": 3.1668481959432452e-09
                    },
                    "per-process_emissions": [
                        0.004378612197886429,
                        0.004408249795967939,
                        0.004426527199713758,
                        0.004426542823028661
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0566": {
            "setup": {
                "experiment_id": "0566",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 04:27:06 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_20_temp_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 20,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 39.75235354201868,
                        "average_latency_ms_per_batch": 4969.044192752335,
                        "throughput_queries_per_sec": 3.2199351382982284,
                        "throughput_tokens_per_sec": 412.15169770217324
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38451281920,
                        "gpu_max_memory_reserved_bytes": 38451281920
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 29.7,
                        "cpu_memory_usage_bytes": 1978048512
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0566",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_1": 1008.3806571708318,
                            "process_2": 900.1024403832176,
                            "process_3": 1058.9746710080944,
                            "process_0": 1039.2440304701804
                        },
                        "ram_power": {
                            "process_1": 0.7324190139770509,
                            "process_2": 0.732081413269043,
                            "process_3": 0.7087225914001465,
                            "process_0": 0.6896452903747559
                        },
                        "cpu_energy": {
                            "process_1": 0.0012267243233436604,
                            "process_2": 0.0012204932987897339,
                            "process_3": 0.001219885056720159,
                            "process_0": 0.0012201341838426743
                        },
                        "gpu_energy": {
                            "process_1": 0.01026006265249002,
                            "process_2": 0.010344250497617224,
                            "process_3": 0.01039302025885469,
                            "process_0": 0.01039302025885469
                        },
                        "ram_energy": {
                            "process_1": 7.141880673550721e-06,
                            "process_2": 6.9842413091092865e-06,
                            "process_3": 6.801206839532211e-06,
                            "process_0": 6.593091174955047e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.01149392885650723,
                            "process_2": 0.011571728037716073,
                            "process_3": 0.01161970652241438,
                            "process_0": 0.011619747533872321
                        },
                        "total_energy_joules": {
                            "process_1": 41378.143883426026,
                            "process_2": 41658.22093577786,
                            "process_3": 41830.94348069176,
                            "process_0": 41831.09112194036
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1001.6754497580811,
                        "ram_power_avg": 0.715717077255249,
                        "cpu_energy_total": 0.0048872368626962275,
                        "gpu_energy_total": 0.041390353667816626,
                        "ram_energy_total": 2.7520419997147264e-05,
                        "total_energy_kwh": 0.04630511095051,
                        "total_energy_joules": 166698.399421836
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09828528682233913,
                        "joules_per_token": 10.17446285533667,
                        "flops_per_joule": 315771372.07934594,
                        "joules_per_flop": 3.1668481959432452e-09
                    },
                    "per-process_emissions": [
                        0.004378612197886429,
                        0.004408249795967939,
                        0.004426527199713758,
                        0.004426542823028661
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0567": {
            "setup": {
                "experiment_id": "0567",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 04:28:44 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_500_temp_1.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 1.2,
                    "decoder_top_k": 500,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 39.26512783294311,
                        "average_latency_ms_per_batch": 4908.1409791178885,
                        "throughput_queries_per_sec": 3.259890061853029,
                        "throughput_tokens_per_sec": 417.2659279171877
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38449184768,
                        "gpu_max_memory_reserved_bytes": 38449184768
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 28.1,
                        "cpu_memory_usage_bytes": 1975410688
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0567",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_3": 960.1746801962795,
                            "process_1": 883.166864749739,
                            "process_2": 1183.7140450402112,
                            "process_0": 1040.7579788870357
                        },
                        "ram_power": {
                            "process_3": 0.7331972122192383,
                            "process_1": 0.7324776649475098,
                            "process_2": 0.733212947845459,
                            "process_0": 0.6887168884277344
                        },
                        "cpu_energy": {
                            "process_3": 0.0011085492029706072,
                            "process_1": 0.0010961850182793567,
                            "process_2": 0.0011541762587148699,
                            "process_0": 0.001103729831629608
                        },
                        "gpu_energy": {
                            "process_3": 0.010377348301874534,
                            "process_1": 0.010317405476142127,
                            "process_2": 0.010378797191922473,
                            "process_0": 0.010294728235777484
                        },
                        "ram_energy": {
                            "process_3": 7.499829294020194e-06,
                            "process_1": 7.411254715057811e-06,
                            "process_2": 7.421597687561092e-06,
                            "process_0": 7.001484171625719e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.011493397334139161,
                            "process_1": 0.011421001749136541,
                            "process_2": 0.011540395048324907,
                            "process_0": 0.011405459551578715
                        },
                        "total_energy_joules": {
                            "process_3": 41376.23040290098,
                            "process_1": 41115.60629689155,
                            "process_2": 41545.422173969666,
                            "process_0": 41059.654385683374
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1016.9533922183164,
                        "ram_power_avg": 0.7219011783599854,
                        "cpu_energy_total": 0.004462640311594442,
                        "gpu_energy_total": 0.04136827920571662,
                        "ram_energy_total": 2.9334165868264816e-05,
                        "total_energy_kwh": 0.04586025368317932,
                        "total_energy_joules": 165096.91325944557
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09923868155095646,
                        "joules_per_token": 10.076715897182957,
                        "flops_per_joule": 318834442.56855255,
                        "joules_per_flop": 3.1364240072180726e-09
                    },
                    "per-process_emissions": [
                        0.004378409714440314,
                        0.004350830616333566,
                        0.004396313493659374,
                        0.0043449098161739115
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0567": {
            "setup": {
                "experiment_id": "0567",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 04:28:44 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_500_temp_1.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 1.2,
                    "decoder_top_k": 500,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 39.26512783294311,
                        "average_latency_ms_per_batch": 4908.1409791178885,
                        "throughput_queries_per_sec": 3.259890061853029,
                        "throughput_tokens_per_sec": 417.2659279171877
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38449184768,
                        "gpu_max_memory_reserved_bytes": 38449184768
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 28.1,
                        "cpu_memory_usage_bytes": 1975410688
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0567",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_3": 960.1746801962795,
                            "process_1": 883.166864749739,
                            "process_2": 1183.7140450402112,
                            "process_0": 1040.7579788870357
                        },
                        "ram_power": {
                            "process_3": 0.7331972122192383,
                            "process_1": 0.7324776649475098,
                            "process_2": 0.733212947845459,
                            "process_0": 0.6887168884277344
                        },
                        "cpu_energy": {
                            "process_3": 0.0011085492029706072,
                            "process_1": 0.0010961850182793567,
                            "process_2": 0.0011541762587148699,
                            "process_0": 0.001103729831629608
                        },
                        "gpu_energy": {
                            "process_3": 0.010377348301874534,
                            "process_1": 0.010317405476142127,
                            "process_2": 0.010378797191922473,
                            "process_0": 0.010294728235777484
                        },
                        "ram_energy": {
                            "process_3": 7.499829294020194e-06,
                            "process_1": 7.411254715057811e-06,
                            "process_2": 7.421597687561092e-06,
                            "process_0": 7.001484171625719e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.011493397334139161,
                            "process_1": 0.011421001749136541,
                            "process_2": 0.011540395048324907,
                            "process_0": 0.011405459551578715
                        },
                        "total_energy_joules": {
                            "process_3": 41376.23040290098,
                            "process_1": 41115.60629689155,
                            "process_2": 41545.422173969666,
                            "process_0": 41059.654385683374
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1016.9533922183164,
                        "ram_power_avg": 0.7219011783599854,
                        "cpu_energy_total": 0.004462640311594442,
                        "gpu_energy_total": 0.04136827920571662,
                        "ram_energy_total": 2.9334165868264816e-05,
                        "total_energy_kwh": 0.04586025368317932,
                        "total_energy_joules": 165096.91325944557
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09923868155095646,
                        "joules_per_token": 10.076715897182957,
                        "flops_per_joule": 318834442.56855255,
                        "joules_per_flop": 3.1364240072180726e-09
                    },
                    "per-process_emissions": [
                        0.004378409714440314,
                        0.004350830616333566,
                        0.004396313493659374,
                        0.0043449098161739115
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0568": {
            "setup": {
                "experiment_id": "0568",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 04:30:20 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_topp_0.8_temp_0.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_p",
                    "decoder_temperature": 0.0,
                    "decoder_top_k": null,
                    "decoder_top_p": 0.8
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.99374179501319,
                        "average_latency_ms_per_batch": 4874.217724376649,
                        "throughput_queries_per_sec": 3.282578026825053,
                        "throughput_tokens_per_sec": 420.16998743360676
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38325452800,
                        "gpu_max_memory_reserved_bytes": 38325452800
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 25.8,
                        "cpu_memory_usage_bytes": 1944649728
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0568",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_3": 877.2320956732724,
                            "process_2": 244.04704523041644,
                            "process_0": 180388.34955300012,
                            "process_1": 427.84463794734916
                        },
                        "ram_power": {
                            "process_3": 0.7207059860229492,
                            "process_2": 0.7201151847839355,
                            "process_0": 0.6779608726501465,
                            "process_1": 0.706113338470459
                        },
                        "cpu_energy": {
                            "process_3": 0.0012161939052166416,
                            "process_2": 0.0012607302494943727,
                            "process_0": 0.001186436697116733,
                            "process_1": 0.0012293822895971967
                        },
                        "gpu_energy": {
                            "process_3": 0.010279308223438477,
                            "process_2": 0.010182193145750418,
                            "process_0": 0.010265005712000175,
                            "process_1": 0.010288677397598534
                        },
                        "ram_energy": {
                            "process_3": 6.911612209302356e-06,
                            "process_2": 7.303307593551212e-06,
                            "process_0": 6.386972633344442e-06,
                            "process_1": 6.7706189188350155e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.011502413740864423,
                            "process_2": 0.011450226702838342,
                            "process_0": 0.011457829381750254,
                            "process_1": 0.011524830306114566
                        },
                        "total_energy_joules": {
                            "process_3": 41408.68946711192,
                            "process_2": 41220.81613021803,
                            "process_0": 41248.18577430092,
                            "process_1": 41489.389102012436
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 45484.3683329628,
                        "ram_power_avg": 0.7062238454818726,
                        "cpu_energy_total": 0.004892743141424944,
                        "gpu_energy_total": 0.0410151844787876,
                        "ram_energy_total": 2.7372511355033026e-05,
                        "total_energy_kwh": 0.04593530013156758,
                        "total_energy_joules": 165367.08047364332
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09907655110722795,
                        "joules_per_token": 10.093205595315144,
                        "flops_per_joule": 318313549.2148553,
                        "joules_per_flop": 3.1415565013383077e-09
                    },
                    "per-process_emissions": [
                        0.004381844514582302,
                        0.004361963862446266,
                        0.004364860102977759,
                        0.004390384105114344
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0568": {
            "setup": {
                "experiment_id": "0568",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 04:30:20 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_topp_0.8_temp_0.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_p",
                    "decoder_temperature": 0.0,
                    "decoder_top_k": null,
                    "decoder_top_p": 0.8
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.99374179501319,
                        "average_latency_ms_per_batch": 4874.217724376649,
                        "throughput_queries_per_sec": 3.282578026825053,
                        "throughput_tokens_per_sec": 420.16998743360676
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38325452800,
                        "gpu_max_memory_reserved_bytes": 38325452800
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 25.8,
                        "cpu_memory_usage_bytes": 1944649728
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0568",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_3": 877.2320956732724,
                            "process_2": 244.04704523041644,
                            "process_0": 180388.34955300012,
                            "process_1": 427.84463794734916
                        },
                        "ram_power": {
                            "process_3": 0.7207059860229492,
                            "process_2": 0.7201151847839355,
                            "process_0": 0.6779608726501465,
                            "process_1": 0.706113338470459
                        },
                        "cpu_energy": {
                            "process_3": 0.0012161939052166416,
                            "process_2": 0.0012607302494943727,
                            "process_0": 0.001186436697116733,
                            "process_1": 0.0012293822895971967
                        },
                        "gpu_energy": {
                            "process_3": 0.010279308223438477,
                            "process_2": 0.010182193145750418,
                            "process_0": 0.010265005712000175,
                            "process_1": 0.010288677397598534
                        },
                        "ram_energy": {
                            "process_3": 6.911612209302356e-06,
                            "process_2": 7.303307593551212e-06,
                            "process_0": 6.386972633344442e-06,
                            "process_1": 6.7706189188350155e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.011502413740864423,
                            "process_2": 0.011450226702838342,
                            "process_0": 0.011457829381750254,
                            "process_1": 0.011524830306114566
                        },
                        "total_energy_joules": {
                            "process_3": 41408.68946711192,
                            "process_2": 41220.81613021803,
                            "process_0": 41248.18577430092,
                            "process_1": 41489.389102012436
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 45484.3683329628,
                        "ram_power_avg": 0.7062238454818726,
                        "cpu_energy_total": 0.004892743141424944,
                        "gpu_energy_total": 0.0410151844787876,
                        "ram_energy_total": 2.7372511355033026e-05,
                        "total_energy_kwh": 0.04593530013156758,
                        "total_energy_joules": 165367.08047364332
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09907655110722795,
                        "joules_per_token": 10.093205595315144,
                        "flops_per_joule": 318313549.2148553,
                        "joules_per_flop": 3.1415565013383077e-09
                    },
                    "per-process_emissions": [
                        0.004381844514582302,
                        0.004361963862446266,
                        0.004364860102977759,
                        0.004390384105114344
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0569": {
            "setup": {
                "experiment_id": "0569",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 04:31:56 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_50_temp_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 0.8,
                    "decoder_top_k": 50,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.965195305936504,
                        "average_latency_ms_per_batch": 4870.649413242063,
                        "throughput_queries_per_sec": 3.2849828929382703,
                        "throughput_tokens_per_sec": 420.4778102960986
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38449184768,
                        "gpu_max_memory_reserved_bytes": 38449184768
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 24.6,
                        "cpu_memory_usage_bytes": 1975877632
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0569",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_2": 689.2370798088724,
                            "process_0": 0.0,
                            "process_3": 868.5901708441044,
                            "process_1": 807.3897883541049
                        },
                        "ram_power": {
                            "process_2": 0.7080860137939453,
                            "process_0": 0.6889157295227051,
                            "process_3": 0.7327837944030762,
                            "process_1": 0.7322344779968262
                        },
                        "cpu_energy": {
                            "process_2": 0.0012328314209644306,
                            "process_0": 0.0012443903163457434,
                            "process_3": 0.0012227817277562284,
                            "process_1": 0.001222066779908346
                        },
                        "gpu_energy": {
                            "process_2": 0.010318927699579206,
                            "process_0": 0.010270038771579948,
                            "process_3": 0.010312645750108373,
                            "process_1": 0.010312645750108373
                        },
                        "ram_energy": {
                            "process_2": 6.8815037969186884e-06,
                            "process_0": 6.76389351022943e-06,
                            "process_3": 7.140039011273994e-06,
                            "process_1": 7.087510216458324e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.011558640624340553,
                            "process_0": 0.011521192981435918,
                            "process_3": 0.011542567516875872,
                            "process_1": 0.011541800040233176
                        },
                        "total_energy_joules": {
                            "process_2": 41611.10624762599,
                            "process_0": 41476.29473316931,
                            "process_3": 41553.24306075314,
                            "process_1": 41550.480144839436
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 591.3042597517704,
                        "ram_power_avg": 0.7155050039291382,
                        "cpu_energy_total": 0.0049220702449747486,
                        "gpu_energy_total": 0.0412142579713759,
                        "ram_energy_total": 2.7872946534880437e-05,
                        "total_energy_kwh": 0.04616420116288552,
                        "total_energy_joules": 166191.12418638787
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09858528895697762,
                        "joules_per_token": 10.143501232079338,
                        "flops_per_joule": 316735220.1663213,
                        "joules_per_flop": 3.1572112487991977e-09
                    },
                    "per-process_emissions": [
                        0.004403264145842534,
                        0.0043889984662780135,
                        0.004397141095553864,
                        0.004396848725326828
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0569": {
            "setup": {
                "experiment_id": "0569",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 04:31:56 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_50_temp_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 0.8,
                    "decoder_top_k": 50,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.965195305936504,
                        "average_latency_ms_per_batch": 4870.649413242063,
                        "throughput_queries_per_sec": 3.2849828929382703,
                        "throughput_tokens_per_sec": 420.4778102960986
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38449184768,
                        "gpu_max_memory_reserved_bytes": 38449184768
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 24.6,
                        "cpu_memory_usage_bytes": 1975877632
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0569",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_2": 689.2370798088724,
                            "process_0": 0.0,
                            "process_3": 868.5901708441044,
                            "process_1": 807.3897883541049
                        },
                        "ram_power": {
                            "process_2": 0.7080860137939453,
                            "process_0": 0.6889157295227051,
                            "process_3": 0.7327837944030762,
                            "process_1": 0.7322344779968262
                        },
                        "cpu_energy": {
                            "process_2": 0.0012328314209644306,
                            "process_0": 0.0012443903163457434,
                            "process_3": 0.0012227817277562284,
                            "process_1": 0.001222066779908346
                        },
                        "gpu_energy": {
                            "process_2": 0.010318927699579206,
                            "process_0": 0.010270038771579948,
                            "process_3": 0.010312645750108373,
                            "process_1": 0.010312645750108373
                        },
                        "ram_energy": {
                            "process_2": 6.8815037969186884e-06,
                            "process_0": 6.76389351022943e-06,
                            "process_3": 7.140039011273994e-06,
                            "process_1": 7.087510216458324e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.011558640624340553,
                            "process_0": 0.011521192981435918,
                            "process_3": 0.011542567516875872,
                            "process_1": 0.011541800040233176
                        },
                        "total_energy_joules": {
                            "process_2": 41611.10624762599,
                            "process_0": 41476.29473316931,
                            "process_3": 41553.24306075314,
                            "process_1": 41550.480144839436
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 591.3042597517704,
                        "ram_power_avg": 0.7155050039291382,
                        "cpu_energy_total": 0.0049220702449747486,
                        "gpu_energy_total": 0.0412142579713759,
                        "ram_energy_total": 2.7872946534880437e-05,
                        "total_energy_kwh": 0.04616420116288552,
                        "total_energy_joules": 166191.12418638787
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09858528895697762,
                        "joules_per_token": 10.143501232079338,
                        "flops_per_joule": 316735220.1663213,
                        "joules_per_flop": 3.1572112487991977e-09
                    },
                    "per-process_emissions": [
                        0.004403264145842534,
                        0.0043889984662780135,
                        0.004397141095553864,
                        0.004396848725326828
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0570": {
            "setup": {
                "experiment_id": "0570",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 04:33:33 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_topp_0.98_temp_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_p",
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": 0.98
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 39.63128742406843,
                        "average_latency_ms_per_batch": 4953.910928008554,
                        "throughput_queries_per_sec": 3.229771433624043,
                        "throughput_tokens_per_sec": 413.4107435038775
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38403047424,
                        "gpu_max_memory_reserved_bytes": 38403047424
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            10.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 22.5,
                        "cpu_memory_usage_bytes": 1981915136
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0570",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_3": 864.0046559167854,
                            "process_2": 1016.5482822672287,
                            "process_0": 1259.626028246879,
                            "process_1": 1134.206302168801
                        },
                        "ram_power": {
                            "process_3": 0.7092289924621582,
                            "process_2": 0.7263665199279785,
                            "process_0": 0.6910171508789064,
                            "process_1": 0.7104835510253906
                        },
                        "cpu_energy": {
                            "process_3": 0.0011472348268107455,
                            "process_2": 0.0011926124103974868,
                            "process_0": 0.001190649396337904,
                            "process_1": 0.0011384827800320638
                        },
                        "gpu_energy": {
                            "process_3": 0.01048298533082459,
                            "process_2": 0.01038355386238976,
                            "process_0": 0.010479701717086343,
                            "process_1": 0.010477358381878688
                        },
                        "ram_energy": {
                            "process_3": 7.196222429753271e-06,
                            "process_2": 7.275538877449094e-06,
                            "process_0": 6.97121961774133e-06,
                            "process_1": 7.165747299803208e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.011637416380065095,
                            "process_2": 0.011583441811664695,
                            "process_0": 0.011677322333041983,
                            "process_1": 0.011623006909210561
                        },
                        "total_energy_joules": {
                            "process_3": 41894.69896823434,
                            "process_2": 41700.3905219929,
                            "process_0": 42038.36039895114,
                            "process_1": 41842.82487315802
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1068.5963171499236,
                        "ram_power_avg": 0.7092740535736084,
                        "cpu_energy_total": 0.0046689794135782,
                        "gpu_energy_total": 0.04182359929217938,
                        "ram_energy_total": 2.8608728224746904e-05,
                        "total_energy_kwh": 0.04652118743398233,
                        "total_energy_joules": 167476.2747623364
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09782878215586262,
                        "joules_per_token": 10.221940598287134,
                        "flops_per_joule": 314304712.0170471,
                        "joules_per_flop": 3.181625861039469e-09
                    },
                    "per-process_emissions": [
                        0.004433273769985798,
                        0.004412712158153666,
                        0.004448475942772344,
                        0.004427784482063764
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0570": {
            "setup": {
                "experiment_id": "0570",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 04:33:33 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_topp_0.98_temp_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_p",
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": 0.98
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 39.63128742406843,
                        "average_latency_ms_per_batch": 4953.910928008554,
                        "throughput_queries_per_sec": 3.229771433624043,
                        "throughput_tokens_per_sec": 413.4107435038775
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38403047424,
                        "gpu_max_memory_reserved_bytes": 38403047424
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            10.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 22.5,
                        "cpu_memory_usage_bytes": 1981915136
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0570",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_3": 864.0046559167854,
                            "process_2": 1016.5482822672287,
                            "process_0": 1259.626028246879,
                            "process_1": 1134.206302168801
                        },
                        "ram_power": {
                            "process_3": 0.7092289924621582,
                            "process_2": 0.7263665199279785,
                            "process_0": 0.6910171508789064,
                            "process_1": 0.7104835510253906
                        },
                        "cpu_energy": {
                            "process_3": 0.0011472348268107455,
                            "process_2": 0.0011926124103974868,
                            "process_0": 0.001190649396337904,
                            "process_1": 0.0011384827800320638
                        },
                        "gpu_energy": {
                            "process_3": 0.01048298533082459,
                            "process_2": 0.01038355386238976,
                            "process_0": 0.010479701717086343,
                            "process_1": 0.010477358381878688
                        },
                        "ram_energy": {
                            "process_3": 7.196222429753271e-06,
                            "process_2": 7.275538877449094e-06,
                            "process_0": 6.97121961774133e-06,
                            "process_1": 7.165747299803208e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.011637416380065095,
                            "process_2": 0.011583441811664695,
                            "process_0": 0.011677322333041983,
                            "process_1": 0.011623006909210561
                        },
                        "total_energy_joules": {
                            "process_3": 41894.69896823434,
                            "process_2": 41700.3905219929,
                            "process_0": 42038.36039895114,
                            "process_1": 41842.82487315802
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1068.5963171499236,
                        "ram_power_avg": 0.7092740535736084,
                        "cpu_energy_total": 0.0046689794135782,
                        "gpu_energy_total": 0.04182359929217938,
                        "ram_energy_total": 2.8608728224746904e-05,
                        "total_energy_kwh": 0.04652118743398233,
                        "total_energy_joules": 167476.2747623364
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09782878215586262,
                        "joules_per_token": 10.221940598287134,
                        "flops_per_joule": 314304712.0170471,
                        "joules_per_flop": 3.181625861039469e-09
                    },
                    "per-process_emissions": [
                        0.004433273769985798,
                        0.004412712158153666,
                        0.004448475942772344,
                        0.004427784482063764
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0571": {
            "setup": {
                "experiment_id": "0571",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 04:35:07 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_200_temp_1.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 1.4,
                    "decoder_top_k": 200,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.9566638211254,
                        "average_latency_ms_per_batch": 4869.582977640675,
                        "throughput_queries_per_sec": 3.2857023021203426,
                        "throughput_tokens_per_sec": 420.56989467140386
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38449184768,
                        "gpu_max_memory_reserved_bytes": 38449184768
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            24.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 21.3,
                        "cpu_memory_usage_bytes": 1976299520
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0571",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_2": 931.5045296461741,
                            "process_3": 387.8620235507772,
                            "process_1": 1018.2066287655025,
                            "process_0": 0.0
                        },
                        "ram_power": {
                            "process_2": 0.7256026268005371,
                            "process_3": 0.7094106674194336,
                            "process_1": 0.7325477600097657,
                            "process_0": 0.6890444755554199
                        },
                        "cpu_energy": {
                            "process_2": 0.0012351759787779882,
                            "process_3": 0.0011801801894307574,
                            "process_1": 0.001299668609863147,
                            "process_0": 0.00119536133321526
                        },
                        "gpu_energy": {
                            "process_2": 0.010256892927731798,
                            "process_3": 0.010309344358581995,
                            "process_1": 0.01018715592749686,
                            "process_0": 0.010284371838601203
                        },
                        "ram_energy": {
                            "process_2": 7.131181393236632e-06,
                            "process_3": 6.998067644320305e-06,
                            "process_1": 7.361212357779652e-06,
                            "process_0": 6.894803913072012e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.01149920008790302,
                            "process_3": 0.011496522615657078,
                            "process_1": 0.011494185749717792,
                            "process_0": 0.011486627975729531
                        },
                        "total_energy_joules": {
                            "process_2": 41397.12031645087,
                            "process_3": 41387.48141636548,
                            "process_1": 41379.068698984054,
                            "process_0": 41351.86071262631
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 584.3932954906135,
                        "ram_power_avg": 0.7141513824462891,
                        "cpu_energy_total": 0.004910386111287152,
                        "gpu_energy_total": 0.041037765052411856,
                        "ram_energy_total": 2.83852653084086e-05,
                        "total_energy_kwh": 0.04597653642900742,
                        "total_energy_joules": 165515.5311444267
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09898768947370586,
                        "joules_per_token": 10.102266305201825,
                        "flops_per_joule": 318028054.1946983,
                        "joules_per_flop": 3.144376688818136e-09
                    },
                    "per-process_emissions": [
                        0.004380620273486655,
                        0.0043796002904345635,
                        0.004378710061354993,
                        0.004375830927354165
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0571": {
            "setup": {
                "experiment_id": "0571",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 04:35:07 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_200_temp_1.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 1.4,
                    "decoder_top_k": 200,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.9566638211254,
                        "average_latency_ms_per_batch": 4869.582977640675,
                        "throughput_queries_per_sec": 3.2857023021203426,
                        "throughput_tokens_per_sec": 420.56989467140386
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38449184768,
                        "gpu_max_memory_reserved_bytes": 38449184768
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            24.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 21.3,
                        "cpu_memory_usage_bytes": 1976299520
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0571",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_2": 931.5045296461741,
                            "process_3": 387.8620235507772,
                            "process_1": 1018.2066287655025,
                            "process_0": 0.0
                        },
                        "ram_power": {
                            "process_2": 0.7256026268005371,
                            "process_3": 0.7094106674194336,
                            "process_1": 0.7325477600097657,
                            "process_0": 0.6890444755554199
                        },
                        "cpu_energy": {
                            "process_2": 0.0012351759787779882,
                            "process_3": 0.0011801801894307574,
                            "process_1": 0.001299668609863147,
                            "process_0": 0.00119536133321526
                        },
                        "gpu_energy": {
                            "process_2": 0.010256892927731798,
                            "process_3": 0.010309344358581995,
                            "process_1": 0.01018715592749686,
                            "process_0": 0.010284371838601203
                        },
                        "ram_energy": {
                            "process_2": 7.131181393236632e-06,
                            "process_3": 6.998067644320305e-06,
                            "process_1": 7.361212357779652e-06,
                            "process_0": 6.894803913072012e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.01149920008790302,
                            "process_3": 0.011496522615657078,
                            "process_1": 0.011494185749717792,
                            "process_0": 0.011486627975729531
                        },
                        "total_energy_joules": {
                            "process_2": 41397.12031645087,
                            "process_3": 41387.48141636548,
                            "process_1": 41379.068698984054,
                            "process_0": 41351.86071262631
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 584.3932954906135,
                        "ram_power_avg": 0.7141513824462891,
                        "cpu_energy_total": 0.004910386111287152,
                        "gpu_energy_total": 0.041037765052411856,
                        "ram_energy_total": 2.83852653084086e-05,
                        "total_energy_kwh": 0.04597653642900742,
                        "total_energy_joules": 165515.5311444267
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09898768947370586,
                        "joules_per_token": 10.102266305201825,
                        "flops_per_joule": 318028054.1946983,
                        "joules_per_flop": 3.144376688818136e-09
                    },
                    "per-process_emissions": [
                        0.004380620273486655,
                        0.0043796002904345635,
                        0.004378710061354993,
                        0.004375830927354165
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0572": {
            "setup": {
                "experiment_id": "0572",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 04:36:44 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "num_processes_3",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 3
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 39.14633053401485,
                        "average_latency_ms_per_batch": 4893.291316751856,
                        "throughput_queries_per_sec": 3.2697828443659316,
                        "throughput_tokens_per_sec": 418.53220407883924
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38352715776,
                        "gpu_max_memory_reserved_bytes": 38352715776
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            0.0
                        ],
                        "cpu_usage_percent": 21.0,
                        "cpu_memory_usage_bytes": 1971511296
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0572",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_1": 148.1277429781141,
                            "process_2": 136.58191466537937,
                            "process_0": 0.0
                        },
                        "ram_power": {
                            "process_1": 0.7299814224243164,
                            "process_2": 0.7297282218933105,
                            "process_0": 0.6873478889465332
                        },
                        "cpu_energy": {
                            "process_1": 0.0011907205650550166,
                            "process_2": 0.0012527939679675912,
                            "process_0": 0.00120903483830989
                        },
                        "gpu_energy": {
                            "process_1": 0.008105390650973376,
                            "process_2": 0.008105390650973376,
                            "process_0": 0.008125294000228322
                        },
                        "ram_energy": {
                            "process_1": 7.01494535205011e-06,
                            "process_2": 6.996532390908187e-06,
                            "process_0": 7.021122826067474e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.009303126161380442,
                            "process_2": 0.009365181151331874,
                            "process_0": 0.009341349961364279
                        },
                        "total_energy_joules": {
                            "process_1": 33491.25418096959,
                            "process_2": 33714.652144794745,
                            "process_0": 33628.859860911405
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 94.90321921449782,
                        "ram_power_avg": 0.7156858444213867,
                        "cpu_energy_total": 0.0036525493713324975,
                        "gpu_energy_total": 0.024336075302175075,
                        "ram_energy_total": 2.103260056902577e-05,
                        "total_energy_kwh": 0.028009657274076595,
                        "total_energy_joules": 100834.76618667576
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.16248364150186298,
                        "joules_per_token": 6.154465709635972,
                        "flops_per_joule": 522028108.950181,
                        "joules_per_flop": 1.915605659647407e-09
                    },
                    "per-process_emissions": [
                        0.0035440259111778794,
                        0.0035676657595998776,
                        0.003558587267781722
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0572": {
            "setup": {
                "experiment_id": "0572",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 04:36:44 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "num_processes_3",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 3
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 39.14633053401485,
                        "average_latency_ms_per_batch": 4893.291316751856,
                        "throughput_queries_per_sec": 3.2697828443659316,
                        "throughput_tokens_per_sec": 418.53220407883924
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38352715776,
                        "gpu_max_memory_reserved_bytes": 38352715776
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            0.0
                        ],
                        "cpu_usage_percent": 21.0,
                        "cpu_memory_usage_bytes": 1971511296
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0572",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_1": 148.1277429781141,
                            "process_2": 136.58191466537937,
                            "process_0": 0.0
                        },
                        "ram_power": {
                            "process_1": 0.7299814224243164,
                            "process_2": 0.7297282218933105,
                            "process_0": 0.6873478889465332
                        },
                        "cpu_energy": {
                            "process_1": 0.0011907205650550166,
                            "process_2": 0.0012527939679675912,
                            "process_0": 0.00120903483830989
                        },
                        "gpu_energy": {
                            "process_1": 0.008105390650973376,
                            "process_2": 0.008105390650973376,
                            "process_0": 0.008125294000228322
                        },
                        "ram_energy": {
                            "process_1": 7.01494535205011e-06,
                            "process_2": 6.996532390908187e-06,
                            "process_0": 7.021122826067474e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.009303126161380442,
                            "process_2": 0.009365181151331874,
                            "process_0": 0.009341349961364279
                        },
                        "total_energy_joules": {
                            "process_1": 33491.25418096959,
                            "process_2": 33714.652144794745,
                            "process_0": 33628.859860911405
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 94.90321921449782,
                        "ram_power_avg": 0.7156858444213867,
                        "cpu_energy_total": 0.0036525493713324975,
                        "gpu_energy_total": 0.024336075302175075,
                        "ram_energy_total": 2.103260056902577e-05,
                        "total_energy_kwh": 0.028009657274076595,
                        "total_energy_joules": 100834.76618667576
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.16248364150186298,
                        "joules_per_token": 6.154465709635972,
                        "flops_per_joule": 522028108.950181,
                        "joules_per_flop": 1.915605659647407e-09
                    },
                    "per-process_emissions": [
                        0.0035440259111778794,
                        0.0035676657595998776,
                        0.003558587267781722
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0573": {
            "setup": {
                "experiment_id": "0573",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 04:38:27 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_burst_0.1_0.2_6.0_8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.1,
                    "delay_max": 0.2,
                    "simulate_burst": true,
                    "burst_interval": 6.0,
                    "burst_size": 8
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 47.59151821304113,
                        "average_latency_ms_per_batch": 5948.939776630141,
                        "throughput_queries_per_sec": 2.689554878812947,
                        "throughput_tokens_per_sec": 344.2630244880572
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38352715776,
                        "gpu_max_memory_reserved_bytes": 38352715776
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            7.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 19.5,
                        "cpu_memory_usage_bytes": 1951961088
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0573",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_0": 136.7605962987375,
                            "process_3": 1095.895130909863,
                            "process_1": 136.91168085215068,
                            "process_2": 1190.0413000434837
                        },
                        "ram_power": {
                            "process_0": 0.6794342994689941,
                            "process_3": 0.7061748504638673,
                            "process_1": 0.7307410240173341,
                            "process_2": 0.7058529853820801
                        },
                        "cpu_energy": {
                            "process_0": 0.0015430745776393454,
                            "process_3": 0.0014298353605263399,
                            "process_1": 0.0015106268476556578,
                            "process_2": 0.001422157957256786
                        },
                        "gpu_energy": {
                            "process_0": 0.010789698631752387,
                            "process_3": 0.010462724759060293,
                            "process_1": 0.010789698631752387,
                            "process_2": 0.010441968909121968
                        },
                        "ram_energy": {
                            "process_0": 8.096879980578626e-06,
                            "process_3": 7.775010060149195e-06,
                            "process_1": 8.525041542369936e-06,
                            "process_2": 7.725832133997514e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.012340870089372302,
                            "process_3": 0.01190033512964678,
                            "process_1": 0.012308850520950416,
                            "process_2": 0.011871852698512752
                        },
                        "total_energy_joules": {
                            "process_0": 44427.13232174028,
                            "process_3": 42841.20646672841,
                            "process_1": 44311.8618754215,
                            "process_2": 42738.669714645905
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 639.9021770260588,
                        "ram_power_avg": 0.7055507898330688,
                        "cpu_energy_total": 0.005905694743078129,
                        "gpu_energy_total": 0.042484090931687035,
                        "ram_energy_total": 3.2122763717095277e-05,
                        "total_energy_kwh": 0.04842190843848225,
                        "total_energy_joules": 174318.87037853606
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09398867698271506,
                        "joules_per_token": 10.639579490877445,
                        "flops_per_joule": 301967206.3876878,
                        "joules_per_flop": 3.3116178805062894e-09
                    },
                    "per-process_emissions": [
                        0.004701254460546378,
                        0.00453343266763894,
                        0.004689056605956061,
                        0.004522582285498433
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0573": {
            "setup": {
                "experiment_id": "0573",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 04:38:27 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_burst_0.1_0.2_6.0_8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.1,
                    "delay_max": 0.2,
                    "simulate_burst": true,
                    "burst_interval": 6.0,
                    "burst_size": 8
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 47.59151821304113,
                        "average_latency_ms_per_batch": 5948.939776630141,
                        "throughput_queries_per_sec": 2.689554878812947,
                        "throughput_tokens_per_sec": 344.2630244880572
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38352715776,
                        "gpu_max_memory_reserved_bytes": 38352715776
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            7.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 19.5,
                        "cpu_memory_usage_bytes": 1951961088
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0573",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_0": 136.7605962987375,
                            "process_3": 1095.895130909863,
                            "process_1": 136.91168085215068,
                            "process_2": 1190.0413000434837
                        },
                        "ram_power": {
                            "process_0": 0.6794342994689941,
                            "process_3": 0.7061748504638673,
                            "process_1": 0.7307410240173341,
                            "process_2": 0.7058529853820801
                        },
                        "cpu_energy": {
                            "process_0": 0.0015430745776393454,
                            "process_3": 0.0014298353605263399,
                            "process_1": 0.0015106268476556578,
                            "process_2": 0.001422157957256786
                        },
                        "gpu_energy": {
                            "process_0": 0.010789698631752387,
                            "process_3": 0.010462724759060293,
                            "process_1": 0.010789698631752387,
                            "process_2": 0.010441968909121968
                        },
                        "ram_energy": {
                            "process_0": 8.096879980578626e-06,
                            "process_3": 7.775010060149195e-06,
                            "process_1": 8.525041542369936e-06,
                            "process_2": 7.725832133997514e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.012340870089372302,
                            "process_3": 0.01190033512964678,
                            "process_1": 0.012308850520950416,
                            "process_2": 0.011871852698512752
                        },
                        "total_energy_joules": {
                            "process_0": 44427.13232174028,
                            "process_3": 42841.20646672841,
                            "process_1": 44311.8618754215,
                            "process_2": 42738.669714645905
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 639.9021770260588,
                        "ram_power_avg": 0.7055507898330688,
                        "cpu_energy_total": 0.005905694743078129,
                        "gpu_energy_total": 0.042484090931687035,
                        "ram_energy_total": 3.2122763717095277e-05,
                        "total_energy_kwh": 0.04842190843848225,
                        "total_energy_joules": 174318.87037853606
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09398867698271506,
                        "joules_per_token": 10.639579490877445,
                        "flops_per_joule": 301967206.3876878,
                        "joules_per_flop": 3.3116178805062894e-09
                    },
                    "per-process_emissions": [
                        0.004701254460546378,
                        0.00453343266763894,
                        0.004689056605956061,
                        0.004522582285498433
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0574": {
            "setup": {
                "experiment_id": "0574",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 04:40:01 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_topp_0.7_temp_0.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_p",
                    "decoder_temperature": 0.0,
                    "decoder_top_k": null,
                    "decoder_top_p": 0.7
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.564810890879016,
                        "average_latency_ms_per_batch": 4820.601361359877,
                        "throughput_queries_per_sec": 3.3190879727682874,
                        "throughput_tokens_per_sec": 424.8432605143408
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38325452800,
                        "gpu_max_memory_reserved_bytes": 38325452800
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            12.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 19.4,
                        "cpu_memory_usage_bytes": 1963831296
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0574",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_0": 1086.889948203496,
                            "process_3": 860.3104779171322,
                            "process_1": 874.9935151415473,
                            "process_2": 12.549926001624609
                        },
                        "ram_power": {
                            "process_0": 0.6846799850463867,
                            "process_3": 0.7294664382934571,
                            "process_1": 0.7048172950744629,
                            "process_2": 0.7049546241760254
                        },
                        "cpu_energy": {
                            "process_0": 0.0010770531243160801,
                            "process_3": 0.0012223779959949755,
                            "process_1": 0.0011311761195702276,
                            "process_2": 0.0011871933314923807
                        },
                        "gpu_energy": {
                            "process_0": 0.010207711777274753,
                            "process_3": 0.010212861225834757,
                            "process_1": 0.010211609002611866,
                            "process_2": 0.010156734792047928
                        },
                        "ram_energy": {
                            "process_0": 6.921464358168895e-06,
                            "process_3": 7.1676743884624595e-06,
                            "process_1": 7.108350791292396e-06,
                            "process_2": 7.502057177606175e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.011291686365949008,
                            "process_3": 0.011442406896218194,
                            "process_1": 0.01134989347297338,
                            "process_2": 0.011351430180717915
                        },
                        "total_energy_joules": {
                            "process_0": 40650.07091741643,
                            "process_3": 41192.664826385495,
                            "process_1": 40859.61650270417,
                            "process_2": 40865.148650584495
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 708.68596681595,
                        "ram_power_avg": 0.705979585647583,
                        "cpu_energy_total": 0.004617800571373664,
                        "gpu_energy_total": 0.0407889167977693,
                        "ram_energy_total": 2.8699546715529928e-05,
                        "total_energy_kwh": 0.0454354169158585,
                        "total_energy_joules": 163567.5008970906
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.10016659733835565,
                        "joules_per_token": 9.98336797467594,
                        "flops_per_joule": 321815654.21105176,
                        "joules_per_flop": 3.1073690384998625e-09
                    },
                    "per-process_emissions": [
                        0.004301567921108275,
                        0.004358984907114321,
                        0.004323741918529209,
                        0.00432432732734449
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0574": {
            "setup": {
                "experiment_id": "0574",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 04:40:01 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_topp_0.7_temp_0.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_p",
                    "decoder_temperature": 0.0,
                    "decoder_top_k": null,
                    "decoder_top_p": 0.7
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.564810890879016,
                        "average_latency_ms_per_batch": 4820.601361359877,
                        "throughput_queries_per_sec": 3.3190879727682874,
                        "throughput_tokens_per_sec": 424.8432605143408
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38325452800,
                        "gpu_max_memory_reserved_bytes": 38325452800
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            12.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 19.4,
                        "cpu_memory_usage_bytes": 1963831296
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0574",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_0": 1086.889948203496,
                            "process_3": 860.3104779171322,
                            "process_1": 874.9935151415473,
                            "process_2": 12.549926001624609
                        },
                        "ram_power": {
                            "process_0": 0.6846799850463867,
                            "process_3": 0.7294664382934571,
                            "process_1": 0.7048172950744629,
                            "process_2": 0.7049546241760254
                        },
                        "cpu_energy": {
                            "process_0": 0.0010770531243160801,
                            "process_3": 0.0012223779959949755,
                            "process_1": 0.0011311761195702276,
                            "process_2": 0.0011871933314923807
                        },
                        "gpu_energy": {
                            "process_0": 0.010207711777274753,
                            "process_3": 0.010212861225834757,
                            "process_1": 0.010211609002611866,
                            "process_2": 0.010156734792047928
                        },
                        "ram_energy": {
                            "process_0": 6.921464358168895e-06,
                            "process_3": 7.1676743884624595e-06,
                            "process_1": 7.108350791292396e-06,
                            "process_2": 7.502057177606175e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.011291686365949008,
                            "process_3": 0.011442406896218194,
                            "process_1": 0.01134989347297338,
                            "process_2": 0.011351430180717915
                        },
                        "total_energy_joules": {
                            "process_0": 40650.07091741643,
                            "process_3": 41192.664826385495,
                            "process_1": 40859.61650270417,
                            "process_2": 40865.148650584495
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 708.68596681595,
                        "ram_power_avg": 0.705979585647583,
                        "cpu_energy_total": 0.004617800571373664,
                        "gpu_energy_total": 0.0407889167977693,
                        "ram_energy_total": 2.8699546715529928e-05,
                        "total_energy_kwh": 0.0454354169158585,
                        "total_energy_joules": 163567.5008970906
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.10016659733835565,
                        "joules_per_token": 9.98336797467594,
                        "flops_per_joule": 321815654.21105176,
                        "joules_per_flop": 3.1073690384998625e-09
                    },
                    "per-process_emissions": [
                        0.004301567921108275,
                        0.004358984907114321,
                        0.004323741918529209,
                        0.00432432732734449
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0575": {
            "setup": {
                "experiment_id": "0575",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 04:41:34 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_greedy_temp_1.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "greedy",
                    "decoder_temperature": 1.4,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.49409200903028,
                        "average_latency_ms_per_batch": 4811.761501128785,
                        "throughput_queries_per_sec": 3.325185588738465,
                        "throughput_tokens_per_sec": 425.6237553585235
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38405144576,
                        "gpu_max_memory_reserved_bytes": 38405144576
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            4.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 19.3,
                        "cpu_memory_usage_bytes": 1973538816
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0575",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_2": 190.46287439609478,
                            "process_3": 196.48470334764875,
                            "process_0": 186.1427259933737,
                            "process_1": 3689.5955056178664
                        },
                        "ram_power": {
                            "process_2": 0.7296624183654785,
                            "process_3": 0.7311630249023439,
                            "process_0": 0.6880602836608887,
                            "process_1": 0.7231149673461914
                        },
                        "cpu_energy": {
                            "process_2": 0.0011468354734388414,
                            "process_3": 0.001157000340768718,
                            "process_0": 0.0011355715468780543,
                            "process_1": 0.0010550855271922045
                        },
                        "gpu_energy": {
                            "process_2": 0.010233829298167407,
                            "process_3": 0.010245403751872217,
                            "process_0": 0.010233829298167407,
                            "process_1": 0.010245403751872217
                        },
                        "ram_energy": {
                            "process_2": 7.118604595737624e-06,
                            "process_3": 7.223502931225416e-06,
                            "process_0": 6.66322628264005e-06,
                            "process_1": 6.814575555502725e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.011387783376201981,
                            "process_3": 0.011409627595572161,
                            "process_0": 0.011376064071328103,
                            "process_1": 0.011307303854619925
                        },
                        "total_energy_joules": {
                            "process_2": 40996.02015432713,
                            "process_3": 41074.65934405978,
                            "process_0": 40953.830656781174,
                            "process_1": 40706.29387663173
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1065.671452338746,
                        "ram_power_avg": 0.7180001735687256,
                        "cpu_energy_total": 0.004494492888277819,
                        "gpu_energy_total": 0.04095846610007925,
                        "ram_energy_total": 2.7819909365105816e-05,
                        "total_energy_kwh": 0.045480778897722174,
                        "total_energy_joules": 163730.80403179984
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.10006669237889955,
                        "joules_per_token": 9.993335207019033,
                        "flops_per_joule": 321494679.14810044,
                        "joules_per_flop": 3.1104713852490783e-09
                    },
                    "per-process_emissions": [
                        0.004338176077164145,
                        0.0043464976325332146,
                        0.004333711607972441,
                        0.00430751740341746
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0575": {
            "setup": {
                "experiment_id": "0575",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 04:41:34 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_greedy_temp_1.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "greedy",
                    "decoder_temperature": 1.4,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.49409200903028,
                        "average_latency_ms_per_batch": 4811.761501128785,
                        "throughput_queries_per_sec": 3.325185588738465,
                        "throughput_tokens_per_sec": 425.6237553585235
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38405144576,
                        "gpu_max_memory_reserved_bytes": 38405144576
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            4.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 19.3,
                        "cpu_memory_usage_bytes": 1973538816
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0575",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_2": 190.46287439609478,
                            "process_3": 196.48470334764875,
                            "process_0": 186.1427259933737,
                            "process_1": 3689.5955056178664
                        },
                        "ram_power": {
                            "process_2": 0.7296624183654785,
                            "process_3": 0.7311630249023439,
                            "process_0": 0.6880602836608887,
                            "process_1": 0.7231149673461914
                        },
                        "cpu_energy": {
                            "process_2": 0.0011468354734388414,
                            "process_3": 0.001157000340768718,
                            "process_0": 0.0011355715468780543,
                            "process_1": 0.0010550855271922045
                        },
                        "gpu_energy": {
                            "process_2": 0.010233829298167407,
                            "process_3": 0.010245403751872217,
                            "process_0": 0.010233829298167407,
                            "process_1": 0.010245403751872217
                        },
                        "ram_energy": {
                            "process_2": 7.118604595737624e-06,
                            "process_3": 7.223502931225416e-06,
                            "process_0": 6.66322628264005e-06,
                            "process_1": 6.814575555502725e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.011387783376201981,
                            "process_3": 0.011409627595572161,
                            "process_0": 0.011376064071328103,
                            "process_1": 0.011307303854619925
                        },
                        "total_energy_joules": {
                            "process_2": 40996.02015432713,
                            "process_3": 41074.65934405978,
                            "process_0": 40953.830656781174,
                            "process_1": 40706.29387663173
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1065.671452338746,
                        "ram_power_avg": 0.7180001735687256,
                        "cpu_energy_total": 0.004494492888277819,
                        "gpu_energy_total": 0.04095846610007925,
                        "ram_energy_total": 2.7819909365105816e-05,
                        "total_energy_kwh": 0.045480778897722174,
                        "total_energy_joules": 163730.80403179984
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.10006669237889955,
                        "joules_per_token": 9.993335207019033,
                        "flops_per_joule": 321494679.14810044,
                        "joules_per_flop": 3.1104713852490783e-09
                    },
                    "per-process_emissions": [
                        0.004338176077164145,
                        0.0043464976325332146,
                        0.004333711607972441,
                        0.00430751740341746
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0576": {
            "setup": {
                "experiment_id": "0576",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 04:43:07 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_300_temp_0.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 0.2,
                    "decoder_top_k": 300,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.876579407951795,
                        "average_latency_ms_per_batch": 4859.572425993974,
                        "throughput_queries_per_sec": 3.2924707355765706,
                        "throughput_tokens_per_sec": 421.43625415380103
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38449184768,
                        "gpu_max_memory_reserved_bytes": 38449184768
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            27.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 19.5,
                        "cpu_memory_usage_bytes": 1956233216
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0576",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_3": 2299.044437630545,
                            "process_2": 44646.613398135785,
                            "process_0": 2595.879293727357,
                            "process_1": 2301.9933935977897
                        },
                        "ram_power": {
                            "process_3": 0.7093219757080078,
                            "process_2": 0.7317667007446289,
                            "process_0": 0.6811237335205078,
                            "process_1": 0.708977222442627
                        },
                        "cpu_energy": {
                            "process_3": 0.0010535885519111618,
                            "process_2": 0.0010490331369710473,
                            "process_0": 0.0010571919316917045,
                            "process_1": 0.0010644901676641896
                        },
                        "gpu_energy": {
                            "process_3": 0.010267955992135924,
                            "process_2": 0.010224130957075417,
                            "process_0": 0.010267955992135924,
                            "process_1": 0.010267955992135924
                        },
                        "ram_energy": {
                            "process_3": 6.675111519004771e-06,
                            "process_2": 7.178196495962145e-06,
                            "process_0": 6.37993134666547e-06,
                            "process_1": 6.700610233141542e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.01132821965556609,
                            "process_2": 0.01128034229054243,
                            "process_0": 0.011331527855174293,
                            "process_1": 0.01133914677003326
                        },
                        "total_energy_joules": {
                            "process_3": 40781.59076003792,
                            "process_2": 40609.23224595274,
                            "process_0": 40793.500278627456,
                            "process_1": 40820.928372119735
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 12960.882630772869,
                        "ram_power_avg": 0.7077974081039429,
                        "cpu_energy_total": 0.004224303788238103,
                        "gpu_energy_total": 0.04102799893348319,
                        "ram_energy_total": 2.693384959477393e-05,
                        "total_energy_kwh": 0.04527923657131607,
                        "total_energy_joules": 163005.25165673785
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.10051209904882083,
                        "joules_per_token": 9.949051004439566,
                        "flops_per_joule": 322925683.52160925,
                        "joules_per_flop": 3.0966877242301567e-09
                    },
                    "per-process_emissions": [
                        0.004315485277787902,
                        0.0042972463955821385,
                        0.0043167455364286475,
                        0.004319647962044171
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0576": {
            "setup": {
                "experiment_id": "0576",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 04:43:07 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_300_temp_0.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 0.2,
                    "decoder_top_k": 300,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.876579407951795,
                        "average_latency_ms_per_batch": 4859.572425993974,
                        "throughput_queries_per_sec": 3.2924707355765706,
                        "throughput_tokens_per_sec": 421.43625415380103
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38449184768,
                        "gpu_max_memory_reserved_bytes": 38449184768
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            27.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 19.5,
                        "cpu_memory_usage_bytes": 1956233216
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0576",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_3": 2299.044437630545,
                            "process_2": 44646.613398135785,
                            "process_0": 2595.879293727357,
                            "process_1": 2301.9933935977897
                        },
                        "ram_power": {
                            "process_3": 0.7093219757080078,
                            "process_2": 0.7317667007446289,
                            "process_0": 0.6811237335205078,
                            "process_1": 0.708977222442627
                        },
                        "cpu_energy": {
                            "process_3": 0.0010535885519111618,
                            "process_2": 0.0010490331369710473,
                            "process_0": 0.0010571919316917045,
                            "process_1": 0.0010644901676641896
                        },
                        "gpu_energy": {
                            "process_3": 0.010267955992135924,
                            "process_2": 0.010224130957075417,
                            "process_0": 0.010267955992135924,
                            "process_1": 0.010267955992135924
                        },
                        "ram_energy": {
                            "process_3": 6.675111519004771e-06,
                            "process_2": 7.178196495962145e-06,
                            "process_0": 6.37993134666547e-06,
                            "process_1": 6.700610233141542e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.01132821965556609,
                            "process_2": 0.01128034229054243,
                            "process_0": 0.011331527855174293,
                            "process_1": 0.01133914677003326
                        },
                        "total_energy_joules": {
                            "process_3": 40781.59076003792,
                            "process_2": 40609.23224595274,
                            "process_0": 40793.500278627456,
                            "process_1": 40820.928372119735
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 12960.882630772869,
                        "ram_power_avg": 0.7077974081039429,
                        "cpu_energy_total": 0.004224303788238103,
                        "gpu_energy_total": 0.04102799893348319,
                        "ram_energy_total": 2.693384959477393e-05,
                        "total_energy_kwh": 0.04527923657131607,
                        "total_energy_joules": 163005.25165673785
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.10051209904882083,
                        "joules_per_token": 9.949051004439566,
                        "flops_per_joule": 322925683.52160925,
                        "joules_per_flop": 3.0966877242301567e-09
                    },
                    "per-process_emissions": [
                        0.004315485277787902,
                        0.0042972463955821385,
                        0.0043167455364286475,
                        0.004319647962044171
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0577": {
            "setup": {
                "experiment_id": "0577",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 04:44:41 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_burst_0.05_0.1_6.0_10",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.05,
                    "delay_max": 0.1,
                    "simulate_burst": true,
                    "burst_interval": 6.0,
                    "burst_size": 10
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 39.23304691398516,
                        "average_latency_ms_per_batch": 4904.130864248145,
                        "throughput_queries_per_sec": 3.2625556786509136,
                        "throughput_tokens_per_sec": 417.60712686731694
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38403047424,
                        "gpu_max_memory_reserved_bytes": 38403047424
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            4.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 18.8,
                        "cpu_memory_usage_bytes": 1972031488
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0577",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_1": 784.4801101681451,
                            "process_0": 973.6899397521186,
                            "process_3": 644.4757901814364,
                            "process_2": 1042.8676550273683
                        },
                        "ram_power": {
                            "process_1": 0.7232837677001953,
                            "process_0": 0.6863322257995605,
                            "process_3": 0.707676887512207,
                            "process_2": 0.7061877250671387
                        },
                        "cpu_energy": {
                            "process_1": 0.001149483217195666,
                            "process_0": 0.0011975580211583293,
                            "process_3": 0.001149846760934452,
                            "process_2": 0.001217592709717792
                        },
                        "gpu_energy": {
                            "process_1": 0.01029475073579178,
                            "process_0": 0.010279423779088859,
                            "process_3": 0.010305127410763504,
                            "process_2": 0.010197870380510565
                        },
                        "ram_energy": {
                            "process_1": 6.620360187652463e-06,
                            "process_0": 6.241649325909776e-06,
                            "process_3": 6.4965924096096245e-06,
                            "process_2": 6.589430550247742e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.011450854313175099,
                            "process_0": 0.011483223449573097,
                            "process_3": 0.011461470764107567,
                            "process_2": 0.011422052520778606
                        },
                        "total_energy_joules": {
                            "process_1": 41223.07552743035,
                            "process_0": 41339.60441846315,
                            "process_3": 41261.29475078724,
                            "process_2": 41119.38907480298
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 861.3783737822671,
                        "ram_power_avg": 0.7058701515197754,
                        "cpu_energy_total": 0.004714480709006239,
                        "gpu_energy_total": 0.04107717230615471,
                        "ram_energy_total": 2.5948032473419607e-05,
                        "total_energy_kwh": 0.04581760104763437,
                        "total_energy_joules": 164943.3637714837
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09933106507212237,
                        "joules_per_token": 10.067343980193098,
                        "flops_per_joule": 319131252.72375727,
                        "joules_per_flop": 3.1335069551010364e-09
                    },
                    "per-process_emissions": [
                        0.004362202950604054,
                        0.0043745339731148716,
                        0.0043662472875867775,
                        0.00435123090779061
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0577": {
            "setup": {
                "experiment_id": "0577",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 04:44:41 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_burst_0.05_0.1_6.0_10",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.05,
                    "delay_max": 0.1,
                    "simulate_burst": true,
                    "burst_interval": 6.0,
                    "burst_size": 10
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 39.23304691398516,
                        "average_latency_ms_per_batch": 4904.130864248145,
                        "throughput_queries_per_sec": 3.2625556786509136,
                        "throughput_tokens_per_sec": 417.60712686731694
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38403047424,
                        "gpu_max_memory_reserved_bytes": 38403047424
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            4.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 18.8,
                        "cpu_memory_usage_bytes": 1972031488
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0577",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_1": 784.4801101681451,
                            "process_0": 973.6899397521186,
                            "process_3": 644.4757901814364,
                            "process_2": 1042.8676550273683
                        },
                        "ram_power": {
                            "process_1": 0.7232837677001953,
                            "process_0": 0.6863322257995605,
                            "process_3": 0.707676887512207,
                            "process_2": 0.7061877250671387
                        },
                        "cpu_energy": {
                            "process_1": 0.001149483217195666,
                            "process_0": 0.0011975580211583293,
                            "process_3": 0.001149846760934452,
                            "process_2": 0.001217592709717792
                        },
                        "gpu_energy": {
                            "process_1": 0.01029475073579178,
                            "process_0": 0.010279423779088859,
                            "process_3": 0.010305127410763504,
                            "process_2": 0.010197870380510565
                        },
                        "ram_energy": {
                            "process_1": 6.620360187652463e-06,
                            "process_0": 6.241649325909776e-06,
                            "process_3": 6.4965924096096245e-06,
                            "process_2": 6.589430550247742e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.011450854313175099,
                            "process_0": 0.011483223449573097,
                            "process_3": 0.011461470764107567,
                            "process_2": 0.011422052520778606
                        },
                        "total_energy_joules": {
                            "process_1": 41223.07552743035,
                            "process_0": 41339.60441846315,
                            "process_3": 41261.29475078724,
                            "process_2": 41119.38907480298
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 861.3783737822671,
                        "ram_power_avg": 0.7058701515197754,
                        "cpu_energy_total": 0.004714480709006239,
                        "gpu_energy_total": 0.04107717230615471,
                        "ram_energy_total": 2.5948032473419607e-05,
                        "total_energy_kwh": 0.04581760104763437,
                        "total_energy_joules": 164943.3637714837
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09933106507212237,
                        "joules_per_token": 10.067343980193098,
                        "flops_per_joule": 319131252.72375727,
                        "joules_per_flop": 3.1335069551010364e-09
                    },
                    "per-process_emissions": [
                        0.004362202950604054,
                        0.0043745339731148716,
                        0.0043662472875867775,
                        0.00435123090779061
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0578": {
            "setup": {
                "experiment_id": "0578",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 04:46:17 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_1_temp_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 0.8,
                    "decoder_top_k": 1,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.668071740015876,
                        "average_latency_ms_per_batch": 4833.508967501984,
                        "throughput_queries_per_sec": 3.3102245403030652,
                        "throughput_tokens_per_sec": 423.70874115879235
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38449184768,
                        "gpu_max_memory_reserved_bytes": 38449184768
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            26.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 19.5,
                        "cpu_memory_usage_bytes": 1972264960
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0578",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_1": 20763.100149884525,
                            "process_3": 524.7966738275261,
                            "process_2": 769.2091305569588,
                            "process_0": 7891.479979896407
                        },
                        "ram_power": {
                            "process_1": 0.7074251174926758,
                            "process_3": 0.7168793678283691,
                            "process_2": 0.7075867652893068,
                            "process_0": 0.6876411437988281
                        },
                        "cpu_energy": {
                            "process_1": 0.001028689659158772,
                            "process_3": 0.0010644790880560322,
                            "process_2": 0.001072975555402081,
                            "process_0": 0.0010469016549741357
                        },
                        "gpu_energy": {
                            "process_1": 0.010204083441035294,
                            "process_3": 0.010318037698864302,
                            "process_2": 0.010296109347988747,
                            "process_0": 0.010207091776777588
                        },
                        "ram_energy": {
                            "process_1": 7.602886197080792e-06,
                            "process_3": 7.376955260593422e-06,
                            "process_2": 7.321802527381626e-06,
                            "process_0": 7.4184290400185994e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.011240375986391148,
                            "process_3": 0.01138989374218093,
                            "process_2": 0.01137640670591821,
                            "process_0": 0.011261411860791742
                        },
                        "total_energy_joules": {
                            "process_1": 40465.353551008135,
                            "process_3": 41003.61747185134,
                            "process_2": 40955.06414130556,
                            "process_0": 40541.082698850274
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 7487.1464835413535,
                        "ram_power_avg": 0.7048830986022949,
                        "cpu_energy_total": 0.004213045957591021,
                        "gpu_energy_total": 0.04102532226466593,
                        "ram_energy_total": 2.9720073025074438e-05,
                        "total_energy_kwh": 0.04526808829528203,
                        "total_energy_joules": 162965.1178630153
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.10053685239421611,
                        "joules_per_token": 9.946601432068805,
                        "flops_per_joule": 323005211.17108494,
                        "joules_per_flop": 3.095925283602728e-09
                    },
                    "per-process_emissions": [
                        0.004282021232015708,
                        0.004338980021083825,
                        0.004333842134619542,
                        0.004290034848368615
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0578": {
            "setup": {
                "experiment_id": "0578",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 04:46:17 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_1_temp_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 0.8,
                    "decoder_top_k": 1,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.668071740015876,
                        "average_latency_ms_per_batch": 4833.508967501984,
                        "throughput_queries_per_sec": 3.3102245403030652,
                        "throughput_tokens_per_sec": 423.70874115879235
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38449184768,
                        "gpu_max_memory_reserved_bytes": 38449184768
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            26.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 19.5,
                        "cpu_memory_usage_bytes": 1972264960
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0578",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_1": 20763.100149884525,
                            "process_3": 524.7966738275261,
                            "process_2": 769.2091305569588,
                            "process_0": 7891.479979896407
                        },
                        "ram_power": {
                            "process_1": 0.7074251174926758,
                            "process_3": 0.7168793678283691,
                            "process_2": 0.7075867652893068,
                            "process_0": 0.6876411437988281
                        },
                        "cpu_energy": {
                            "process_1": 0.001028689659158772,
                            "process_3": 0.0010644790880560322,
                            "process_2": 0.001072975555402081,
                            "process_0": 0.0010469016549741357
                        },
                        "gpu_energy": {
                            "process_1": 0.010204083441035294,
                            "process_3": 0.010318037698864302,
                            "process_2": 0.010296109347988747,
                            "process_0": 0.010207091776777588
                        },
                        "ram_energy": {
                            "process_1": 7.602886197080792e-06,
                            "process_3": 7.376955260593422e-06,
                            "process_2": 7.321802527381626e-06,
                            "process_0": 7.4184290400185994e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.011240375986391148,
                            "process_3": 0.01138989374218093,
                            "process_2": 0.01137640670591821,
                            "process_0": 0.011261411860791742
                        },
                        "total_energy_joules": {
                            "process_1": 40465.353551008135,
                            "process_3": 41003.61747185134,
                            "process_2": 40955.06414130556,
                            "process_0": 40541.082698850274
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 7487.1464835413535,
                        "ram_power_avg": 0.7048830986022949,
                        "cpu_energy_total": 0.004213045957591021,
                        "gpu_energy_total": 0.04102532226466593,
                        "ram_energy_total": 2.9720073025074438e-05,
                        "total_energy_kwh": 0.04526808829528203,
                        "total_energy_joules": 162965.1178630153
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.10053685239421611,
                        "joules_per_token": 9.946601432068805,
                        "flops_per_joule": 323005211.17108494,
                        "joules_per_flop": 3.095925283602728e-09
                    },
                    "per-process_emissions": [
                        0.004282021232015708,
                        0.004338980021083825,
                        0.004333842134619542,
                        0.004290034848368615
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0579": {
            "setup": {
                "experiment_id": "0579",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 04:47:50 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_off",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.554466081026476,
                        "average_latency_ms_per_batch": 4819.3082601283095,
                        "throughput_queries_per_sec": 3.319978539736326,
                        "throughput_tokens_per_sec": 424.95725308624975
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38352715776,
                        "gpu_max_memory_reserved_bytes": 38352715776
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            19.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 19.9,
                        "cpu_memory_usage_bytes": 1972080640
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0579",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_2": 1283.9013473577068,
                            "process_0": 3632.155343042642,
                            "process_1": 989.5488047150285,
                            "process_3": 592.7294953833913
                        },
                        "ram_power": {
                            "process_2": 0.722834587097168,
                            "process_0": 0.6875381469726562,
                            "process_1": 0.7228002548217773,
                            "process_3": 0.7235941886901855
                        },
                        "cpu_energy": {
                            "process_2": 0.0011350406704368654,
                            "process_0": 0.0010860375167812892,
                            "process_1": 0.0011481196576569345,
                            "process_3": 0.001153324286100542
                        },
                        "gpu_energy": {
                            "process_2": 0.010229057349903314,
                            "process_0": 0.010180474811040341,
                            "process_1": 0.01026405432234867,
                            "process_3": 0.010285911562052164
                        },
                        "ram_energy": {
                            "process_2": 6.853984017987019e-06,
                            "process_0": 7.102496859116842e-06,
                            "process_1": 6.536570472974708e-06,
                            "process_3": 6.315519257624207e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.011370952004358172,
                            "process_0": 0.011273614824680747,
                            "process_1": 0.011418710550478581,
                            "process_3": 0.011445551367410326
                        },
                        "total_energy_joules": {
                            "process_2": 40935.42721568942,
                            "process_0": 40585.01336885069,
                            "process_1": 41107.3579817229,
                            "process_3": 41203.98492267718
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1624.5837476246923,
                        "ram_power_avg": 0.7141917943954468,
                        "cpu_energy_total": 0.004522522130975631,
                        "gpu_energy_total": 0.04095949804534449,
                        "ram_energy_total": 2.6808570607702776e-05,
                        "total_energy_kwh": 0.045508828746927826,
                        "total_energy_joules": 163831.78348894016
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.10000501521187456,
                        "joules_per_token": 9.999498503963633,
                        "flops_per_joule": 321296522.4932529,
                        "joules_per_flop": 3.112389739671084e-09
                    },
                    "per-process_emissions": [
                        0.0043317641660602455,
                        0.004294683567462131,
                        0.0043499577842048155,
                        0.004360182793414964
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0579": {
            "setup": {
                "experiment_id": "0579",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 04:47:50 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_off",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.554466081026476,
                        "average_latency_ms_per_batch": 4819.3082601283095,
                        "throughput_queries_per_sec": 3.319978539736326,
                        "throughput_tokens_per_sec": 424.95725308624975
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38352715776,
                        "gpu_max_memory_reserved_bytes": 38352715776
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            19.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 19.9,
                        "cpu_memory_usage_bytes": 1972080640
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0579",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_2": 1283.9013473577068,
                            "process_0": 3632.155343042642,
                            "process_1": 989.5488047150285,
                            "process_3": 592.7294953833913
                        },
                        "ram_power": {
                            "process_2": 0.722834587097168,
                            "process_0": 0.6875381469726562,
                            "process_1": 0.7228002548217773,
                            "process_3": 0.7235941886901855
                        },
                        "cpu_energy": {
                            "process_2": 0.0011350406704368654,
                            "process_0": 0.0010860375167812892,
                            "process_1": 0.0011481196576569345,
                            "process_3": 0.001153324286100542
                        },
                        "gpu_energy": {
                            "process_2": 0.010229057349903314,
                            "process_0": 0.010180474811040341,
                            "process_1": 0.01026405432234867,
                            "process_3": 0.010285911562052164
                        },
                        "ram_energy": {
                            "process_2": 6.853984017987019e-06,
                            "process_0": 7.102496859116842e-06,
                            "process_1": 6.536570472974708e-06,
                            "process_3": 6.315519257624207e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.011370952004358172,
                            "process_0": 0.011273614824680747,
                            "process_1": 0.011418710550478581,
                            "process_3": 0.011445551367410326
                        },
                        "total_energy_joules": {
                            "process_2": 40935.42721568942,
                            "process_0": 40585.01336885069,
                            "process_1": 41107.3579817229,
                            "process_3": 41203.98492267718
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1624.5837476246923,
                        "ram_power_avg": 0.7141917943954468,
                        "cpu_energy_total": 0.004522522130975631,
                        "gpu_energy_total": 0.04095949804534449,
                        "ram_energy_total": 2.6808570607702776e-05,
                        "total_energy_kwh": 0.045508828746927826,
                        "total_energy_joules": 163831.78348894016
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.10000501521187456,
                        "joules_per_token": 9.999498503963633,
                        "flops_per_joule": 321296522.4932529,
                        "joules_per_flop": 3.112389739671084e-09
                    },
                    "per-process_emissions": [
                        0.0043317641660602455,
                        0.004294683567462131,
                        0.0043499577842048155,
                        0.004360182793414964
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0580": {
            "setup": {
                "experiment_id": "0580",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 04:49:28 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_burst_0.05_0.1_4.0_5",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.05,
                    "delay_max": 0.1,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 43.45327152789105,
                        "average_latency_ms_per_batch": 5431.658940986381,
                        "throughput_queries_per_sec": 2.945693051392955,
                        "throughput_tokens_per_sec": 377.04871057829826
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38352715776,
                        "gpu_max_memory_reserved_bytes": 38352715776
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            22.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 19.5,
                        "cpu_memory_usage_bytes": 1972322304
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0580",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_2": 1116.8522861193646,
                            "process_1": 5980.035746365662,
                            "process_3": 42.344201922276,
                            "process_0": 18824.14572906785
                        },
                        "ram_power": {
                            "process_2": 0.7223224639892578,
                            "process_1": 0.7238717079162598,
                            "process_3": 0.7298183441162109,
                            "process_0": 0.6862564086914062
                        },
                        "cpu_energy": {
                            "process_2": 0.0012344763715263982,
                            "process_1": 0.001211761009784823,
                            "process_3": 0.0013141047987428464,
                            "process_0": 0.0011968392430244425
                        },
                        "gpu_energy": {
                            "process_2": 0.010443810855043978,
                            "process_1": 0.010517464525079845,
                            "process_3": 0.010516847580140265,
                            "process_0": 0.01052019119392611
                        },
                        "ram_energy": {
                            "process_2": 7.521459839379132e-06,
                            "process_1": 7.577776772041603e-06,
                            "process_3": 8.46617185836613e-06,
                            "process_0": 7.501624857206642e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.011685808686409758,
                            "process_1": 0.011736803311636708,
                            "process_3": 0.011839418550741474,
                            "process_0": 0.011724532061807761
                        },
                        "total_energy_joules": {
                            "process_2": 42068.91127107513,
                            "process_1": 42252.49192189215,
                            "process_3": 42621.906782669306,
                            "process_0": 42208.31542250794
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 6490.844490868789,
                        "ram_power_avg": 0.7155672311782837,
                        "cpu_energy_total": 0.0049571814230785095,
                        "gpu_energy_total": 0.0419983141541902,
                        "ram_energy_total": 3.106703332699351e-05,
                        "total_energy_kwh": 0.04698656261059571,
                        "total_energy_joules": 169151.62539814453
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09685984371380282,
                        "joules_per_token": 10.32419588611722,
                        "flops_per_joule": 311191702.62162554,
                        "joules_per_flop": 3.2134532880392654e-09
                    },
                    "per-process_emissions": [
                        0.004451708819087797,
                        0.004471135221568004,
                        0.004510226496904965,
                        0.004466460488945667
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0580": {
            "setup": {
                "experiment_id": "0580",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 04:49:28 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_burst_0.05_0.1_4.0_5",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.05,
                    "delay_max": 0.1,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 43.45327152789105,
                        "average_latency_ms_per_batch": 5431.658940986381,
                        "throughput_queries_per_sec": 2.945693051392955,
                        "throughput_tokens_per_sec": 377.04871057829826
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38352715776,
                        "gpu_max_memory_reserved_bytes": 38352715776
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            22.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 19.5,
                        "cpu_memory_usage_bytes": 1972322304
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0580",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_2": 1116.8522861193646,
                            "process_1": 5980.035746365662,
                            "process_3": 42.344201922276,
                            "process_0": 18824.14572906785
                        },
                        "ram_power": {
                            "process_2": 0.7223224639892578,
                            "process_1": 0.7238717079162598,
                            "process_3": 0.7298183441162109,
                            "process_0": 0.6862564086914062
                        },
                        "cpu_energy": {
                            "process_2": 0.0012344763715263982,
                            "process_1": 0.001211761009784823,
                            "process_3": 0.0013141047987428464,
                            "process_0": 0.0011968392430244425
                        },
                        "gpu_energy": {
                            "process_2": 0.010443810855043978,
                            "process_1": 0.010517464525079845,
                            "process_3": 0.010516847580140265,
                            "process_0": 0.01052019119392611
                        },
                        "ram_energy": {
                            "process_2": 7.521459839379132e-06,
                            "process_1": 7.577776772041603e-06,
                            "process_3": 8.46617185836613e-06,
                            "process_0": 7.501624857206642e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.011685808686409758,
                            "process_1": 0.011736803311636708,
                            "process_3": 0.011839418550741474,
                            "process_0": 0.011724532061807761
                        },
                        "total_energy_joules": {
                            "process_2": 42068.91127107513,
                            "process_1": 42252.49192189215,
                            "process_3": 42621.906782669306,
                            "process_0": 42208.31542250794
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 6490.844490868789,
                        "ram_power_avg": 0.7155672311782837,
                        "cpu_energy_total": 0.0049571814230785095,
                        "gpu_energy_total": 0.0419983141541902,
                        "ram_energy_total": 3.106703332699351e-05,
                        "total_energy_kwh": 0.04698656261059571,
                        "total_energy_joules": 169151.62539814453
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09685984371380282,
                        "joules_per_token": 10.32419588611722,
                        "flops_per_joule": 311191702.62162554,
                        "joules_per_flop": 3.2134532880392654e-09
                    },
                    "per-process_emissions": [
                        0.004451708819087797,
                        0.004471135221568004,
                        0.004510226496904965,
                        0.004466460488945667
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0581": {
            "setup": {
                "experiment_id": "0581",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 04:51:02 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_500_temp_0.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 0.4,
                    "decoder_top_k": 500,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.95431367197307,
                        "average_latency_ms_per_batch": 4869.2892089966335,
                        "throughput_queries_per_sec": 3.2859005315268512,
                        "throughput_tokens_per_sec": 420.59526803543696
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38449184768,
                        "gpu_max_memory_reserved_bytes": 38449184768
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 19.1,
                        "cpu_memory_usage_bytes": 1975959552
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0581",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_3": 407.93298536779577,
                            "process_2": 1005.2294202933246,
                            "process_0": 7.881145495122132,
                            "process_1": 1008.9894422605188
                        },
                        "ram_power": {
                            "process_3": 0.7246870994567871,
                            "process_2": 0.7248659133911133,
                            "process_0": 0.688927173614502,
                            "process_1": 0.7089343070983887
                        },
                        "cpu_energy": {
                            "process_3": 0.0011797621955884098,
                            "process_2": 0.001229859766935988,
                            "process_0": 0.0012074061062412513,
                            "process_1": 0.0011656243672459824
                        },
                        "gpu_energy": {
                            "process_3": 0.010302731575512425,
                            "process_2": 0.010224863457663602,
                            "process_0": 0.01029881323904469,
                            "process_1": 0.010224863457663602
                        },
                        "ram_energy": {
                            "process_3": 7.161121462682665e-06,
                            "process_2": 7.190675782322542e-06,
                            "process_0": 6.975197666411023e-06,
                            "process_1": 7.052602430843538e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.011489654892563522,
                            "process_2": 0.011461913900381909,
                            "process_0": 0.011513194542952354,
                            "process_1": 0.01139754042734043
                        },
                        "total_energy_joules": {
                            "process_3": 41362.75761322868,
                            "process_2": 41262.89004137487,
                            "process_0": 41447.50035462847,
                            "process_1": 41031.145538425546
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 607.5082483541903,
                        "ram_power_avg": 0.7118536233901978,
                        "cpu_energy_total": 0.004782652436011631,
                        "gpu_energy_total": 0.04105127172988432,
                        "ram_energy_total": 2.837959734225977e-05,
                        "total_energy_kwh": 0.045862303763238216,
                        "total_energy_joules": 165104.29354765758
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09923424550598217,
                        "joules_per_token": 10.077166354227147,
                        "flops_per_joule": 318820190.4250891,
                        "joules_per_flop": 3.136564214037639e-09
                    },
                    "per-process_emissions": [
                        0.004376984031322074,
                        0.004366416100350488,
                        0.004385951461137699,
                        0.004341893025795337
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0581": {
            "setup": {
                "experiment_id": "0581",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 04:51:02 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_500_temp_0.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 0.4,
                    "decoder_top_k": 500,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.95431367197307,
                        "average_latency_ms_per_batch": 4869.2892089966335,
                        "throughput_queries_per_sec": 3.2859005315268512,
                        "throughput_tokens_per_sec": 420.59526803543696
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38449184768,
                        "gpu_max_memory_reserved_bytes": 38449184768
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 19.1,
                        "cpu_memory_usage_bytes": 1975959552
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0581",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_3": 407.93298536779577,
                            "process_2": 1005.2294202933246,
                            "process_0": 7.881145495122132,
                            "process_1": 1008.9894422605188
                        },
                        "ram_power": {
                            "process_3": 0.7246870994567871,
                            "process_2": 0.7248659133911133,
                            "process_0": 0.688927173614502,
                            "process_1": 0.7089343070983887
                        },
                        "cpu_energy": {
                            "process_3": 0.0011797621955884098,
                            "process_2": 0.001229859766935988,
                            "process_0": 0.0012074061062412513,
                            "process_1": 0.0011656243672459824
                        },
                        "gpu_energy": {
                            "process_3": 0.010302731575512425,
                            "process_2": 0.010224863457663602,
                            "process_0": 0.01029881323904469,
                            "process_1": 0.010224863457663602
                        },
                        "ram_energy": {
                            "process_3": 7.161121462682665e-06,
                            "process_2": 7.190675782322542e-06,
                            "process_0": 6.975197666411023e-06,
                            "process_1": 7.052602430843538e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.011489654892563522,
                            "process_2": 0.011461913900381909,
                            "process_0": 0.011513194542952354,
                            "process_1": 0.01139754042734043
                        },
                        "total_energy_joules": {
                            "process_3": 41362.75761322868,
                            "process_2": 41262.89004137487,
                            "process_0": 41447.50035462847,
                            "process_1": 41031.145538425546
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 607.5082483541903,
                        "ram_power_avg": 0.7118536233901978,
                        "cpu_energy_total": 0.004782652436011631,
                        "gpu_energy_total": 0.04105127172988432,
                        "ram_energy_total": 2.837959734225977e-05,
                        "total_energy_kwh": 0.045862303763238216,
                        "total_energy_joules": 165104.29354765758
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09923424550598217,
                        "joules_per_token": 10.077166354227147,
                        "flops_per_joule": 318820190.4250891,
                        "joules_per_flop": 3.136564214037639e-09
                    },
                    "per-process_emissions": [
                        0.004376984031322074,
                        0.004366416100350488,
                        0.004385951461137699,
                        0.004341893025795337
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0582": {
            "setup": {
                "experiment_id": "0582",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 04:52:39 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_50_temp_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.72662547789514,
                        "average_latency_ms_per_batch": 4840.828184736893,
                        "throughput_queries_per_sec": 3.3052195594233074,
                        "throughput_tokens_per_sec": 423.06810360618334
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38451281920,
                        "gpu_max_memory_reserved_bytes": 38451281920
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            27.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 17.1,
                        "cpu_memory_usage_bytes": 1979101184
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0582",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 19214.22577925543,
                            "process_1": 3108.6541487463473,
                            "process_2": 2492.6565673550676,
                            "process_3": 198.67432276521694
                        },
                        "ram_power": {
                            "process_0": 0.6890373229980469,
                            "process_1": 0.7322115898132324,
                            "process_2": 0.7330327033996582,
                            "process_3": 0.733198642730713
                        },
                        "cpu_energy": {
                            "process_0": 0.0010486427749074208,
                            "process_1": 0.0010583863786832806,
                            "process_2": 0.0010674547927683306,
                            "process_3": 0.001038803322433523
                        },
                        "gpu_energy": {
                            "process_0": 0.010239704302868802,
                            "process_1": 0.010271220716965956,
                            "process_2": 0.010275488498159291,
                            "process_3": 0.010288436286298719
                        },
                        "ram_energy": {
                            "process_0": 7.099121708232237e-06,
                            "process_1": 6.883478878061405e-06,
                            "process_2": 6.912172775097182e-06,
                            "process_3": 7.2152139562401925e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.011295446199484456,
                            "process_1": 0.0113364905745273,
                            "process_2": 0.011349855463702718,
                            "process_3": 0.011334454822688482
                        },
                        "total_energy_joules": {
                            "process_0": 40663.60631814404,
                            "process_1": 40811.36606829828,
                            "process_2": 40859.47966932978,
                            "process_3": 40804.03736167854
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 6253.552704530516,
                        "ram_power_avg": 0.7218700647354126,
                        "cpu_energy_total": 0.0042132872687925555,
                        "gpu_energy_total": 0.04107484980429277,
                        "ram_energy_total": 2.8109987317631017e-05,
                        "total_energy_kwh": 0.04531624706040295,
                        "total_energy_joules": 163138.48941745065
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.10043000924248739,
                        "joules_per_token": 9.957183191983072,
                        "flops_per_joule": 322661945.05558133,
                        "joules_per_flop": 3.0992189048750117e-09
                    },
                    "per-process_emissions": [
                        0.004303000229693603,
                        0.004318636084366175,
                        0.0043237274388975505,
                        0.004317860564703177
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0582": {
            "setup": {
                "experiment_id": "0582",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 04:52:39 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_50_temp_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.72662547789514,
                        "average_latency_ms_per_batch": 4840.828184736893,
                        "throughput_queries_per_sec": 3.3052195594233074,
                        "throughput_tokens_per_sec": 423.06810360618334
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38451281920,
                        "gpu_max_memory_reserved_bytes": 38451281920
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            27.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 17.1,
                        "cpu_memory_usage_bytes": 1979101184
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0582",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 19214.22577925543,
                            "process_1": 3108.6541487463473,
                            "process_2": 2492.6565673550676,
                            "process_3": 198.67432276521694
                        },
                        "ram_power": {
                            "process_0": 0.6890373229980469,
                            "process_1": 0.7322115898132324,
                            "process_2": 0.7330327033996582,
                            "process_3": 0.733198642730713
                        },
                        "cpu_energy": {
                            "process_0": 0.0010486427749074208,
                            "process_1": 0.0010583863786832806,
                            "process_2": 0.0010674547927683306,
                            "process_3": 0.001038803322433523
                        },
                        "gpu_energy": {
                            "process_0": 0.010239704302868802,
                            "process_1": 0.010271220716965956,
                            "process_2": 0.010275488498159291,
                            "process_3": 0.010288436286298719
                        },
                        "ram_energy": {
                            "process_0": 7.099121708232237e-06,
                            "process_1": 6.883478878061405e-06,
                            "process_2": 6.912172775097182e-06,
                            "process_3": 7.2152139562401925e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.011295446199484456,
                            "process_1": 0.0113364905745273,
                            "process_2": 0.011349855463702718,
                            "process_3": 0.011334454822688482
                        },
                        "total_energy_joules": {
                            "process_0": 40663.60631814404,
                            "process_1": 40811.36606829828,
                            "process_2": 40859.47966932978,
                            "process_3": 40804.03736167854
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 6253.552704530516,
                        "ram_power_avg": 0.7218700647354126,
                        "cpu_energy_total": 0.0042132872687925555,
                        "gpu_energy_total": 0.04107484980429277,
                        "ram_energy_total": 2.8109987317631017e-05,
                        "total_energy_kwh": 0.04531624706040295,
                        "total_energy_joules": 163138.48941745065
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.10043000924248739,
                        "joules_per_token": 9.957183191983072,
                        "flops_per_joule": 322661945.05558133,
                        "joules_per_flop": 3.0992189048750117e-09
                    },
                    "per-process_emissions": [
                        0.004303000229693603,
                        0.004318636084366175,
                        0.0043237274388975505,
                        0.004317860564703177
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0583": {
            "setup": {
                "experiment_id": "0583",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 04:54:22 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_burst_0.4_0.5_2.0_8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.4,
                    "delay_max": 0.5,
                    "simulate_burst": true,
                    "burst_interval": 2.0,
                    "burst_size": 8
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 45.53785640001297,
                        "average_latency_ms_per_batch": 5692.232050001621,
                        "throughput_queries_per_sec": 2.810848162803806,
                        "throughput_tokens_per_sec": 359.78856483888717
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38352715776,
                        "gpu_max_memory_reserved_bytes": 38352715776
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 16.2,
                        "cpu_memory_usage_bytes": 1973506048
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0583",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_1": 1007.6888379969369,
                            "process_3": 72.76450871893142,
                            "process_2": 1142.8445066027932,
                            "process_0": 1018.5374380637244
                        },
                        "ram_power": {
                            "process_1": 0.7297954559326172,
                            "process_3": 0.7300686836242676,
                            "process_2": 0.7236800193786622,
                            "process_0": 0.6880760192871094
                        },
                        "cpu_energy": {
                            "process_1": 0.0013707705590350085,
                            "process_3": 0.001470960610968177,
                            "process_2": 0.0014290903882501877,
                            "process_0": 0.0013496321307848117
                        },
                        "gpu_energy": {
                            "process_1": 0.01070810439980896,
                            "process_3": 0.010730697751219154,
                            "process_2": 0.010709233567377296,
                            "process_0": 0.010608389320037048
                        },
                        "ram_energy": {
                            "process_1": 8.372326748498901e-06,
                            "process_3": 8.26419548174065e-06,
                            "process_2": 7.900867314707737e-06,
                            "process_0": 7.837415698741602e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.012087247285592469,
                            "process_3": 0.012209922557669072,
                            "process_2": 0.01214622482294219,
                            "process_0": 0.011965858866520601
                        },
                        "total_energy_joules": {
                            "process_1": 43514.09022813289,
                            "process_3": 43955.72120760866,
                            "process_2": 43726.40936259188,
                            "process_0": 43077.091919474165
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 810.4588228455965,
                        "ram_power_avg": 0.7179050445556641,
                        "cpu_energy_total": 0.005620453689038185,
                        "gpu_energy_total": 0.04275642503844246,
                        "ram_energy_total": 3.2374805243688886e-05,
                        "total_energy_kwh": 0.048409253532724336,
                        "total_energy_joules": 174273.3127178076
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.0940132470341562,
                        "joules_per_token": 10.636798871936499,
                        "flops_per_joule": 302046145.14959687,
                        "joules_per_flop": 3.310752400116617e-09
                    },
                    "per-process_emissions": [
                        0.004604636853446451,
                        0.004651369998344033,
                        0.004627104346299827,
                        0.004558393935201023
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0583": {
            "setup": {
                "experiment_id": "0583",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 04:54:22 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_burst_0.4_0.5_2.0_8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.4,
                    "delay_max": 0.5,
                    "simulate_burst": true,
                    "burst_interval": 2.0,
                    "burst_size": 8
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 45.53785640001297,
                        "average_latency_ms_per_batch": 5692.232050001621,
                        "throughput_queries_per_sec": 2.810848162803806,
                        "throughput_tokens_per_sec": 359.78856483888717
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38352715776,
                        "gpu_max_memory_reserved_bytes": 38352715776
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 16.2,
                        "cpu_memory_usage_bytes": 1973506048
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0583",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_1": 1007.6888379969369,
                            "process_3": 72.76450871893142,
                            "process_2": 1142.8445066027932,
                            "process_0": 1018.5374380637244
                        },
                        "ram_power": {
                            "process_1": 0.7297954559326172,
                            "process_3": 0.7300686836242676,
                            "process_2": 0.7236800193786622,
                            "process_0": 0.6880760192871094
                        },
                        "cpu_energy": {
                            "process_1": 0.0013707705590350085,
                            "process_3": 0.001470960610968177,
                            "process_2": 0.0014290903882501877,
                            "process_0": 0.0013496321307848117
                        },
                        "gpu_energy": {
                            "process_1": 0.01070810439980896,
                            "process_3": 0.010730697751219154,
                            "process_2": 0.010709233567377296,
                            "process_0": 0.010608389320037048
                        },
                        "ram_energy": {
                            "process_1": 8.372326748498901e-06,
                            "process_3": 8.26419548174065e-06,
                            "process_2": 7.900867314707737e-06,
                            "process_0": 7.837415698741602e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.012087247285592469,
                            "process_3": 0.012209922557669072,
                            "process_2": 0.01214622482294219,
                            "process_0": 0.011965858866520601
                        },
                        "total_energy_joules": {
                            "process_1": 43514.09022813289,
                            "process_3": 43955.72120760866,
                            "process_2": 43726.40936259188,
                            "process_0": 43077.091919474165
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 810.4588228455965,
                        "ram_power_avg": 0.7179050445556641,
                        "cpu_energy_total": 0.005620453689038185,
                        "gpu_energy_total": 0.04275642503844246,
                        "ram_energy_total": 3.2374805243688886e-05,
                        "total_energy_kwh": 0.048409253532724336,
                        "total_energy_joules": 174273.3127178076
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.0940132470341562,
                        "joules_per_token": 10.636798871936499,
                        "flops_per_joule": 302046145.14959687,
                        "joules_per_flop": 3.310752400116617e-09
                    },
                    "per-process_emissions": [
                        0.004604636853446451,
                        0.004651369998344033,
                        0.004627104346299827,
                        0.004558393935201023
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0584": {
            "setup": {
                "experiment_id": "0584",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 04:55:58 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_burst_0.1_0.2_4.0_20",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.1,
                    "delay_max": 0.2,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 20
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 39.40410794294439,
                        "average_latency_ms_per_batch": 4925.513492868049,
                        "throughput_queries_per_sec": 3.2483922789303845,
                        "throughput_tokens_per_sec": 415.7942117030892
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38352715776,
                        "gpu_max_memory_reserved_bytes": 38352715776
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            28.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 11.3,
                        "cpu_memory_usage_bytes": 1971249152
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0584",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 998.326948202785,
                            "process_2": 1302.9899821836252,
                            "process_1": 1180.695718841989,
                            "process_3": 1122.5563559926593
                        },
                        "ram_power": {
                            "process_0": 0.6860861778259277,
                            "process_2": 0.7063608169555664,
                            "process_1": 0.7292160987854004,
                            "process_3": 0.7231521606445312
                        },
                        "cpu_energy": {
                            "process_0": 0.0010581185150058446,
                            "process_2": 0.001060261501412242,
                            "process_1": 0.0010751834158818385,
                            "process_3": 0.001069819680940782
                        },
                        "gpu_energy": {
                            "process_0": 0.010246019863478395,
                            "process_2": 0.010314807974067364,
                            "process_1": 0.010314807974067364,
                            "process_3": 0.010310227970402153
                        },
                        "ram_energy": {
                            "process_0": 6.852888139772881e-06,
                            "process_2": 7.052001937945975e-06,
                            "process_1": 7.407046821605142e-06,
                            "process_3": 7.309083600690938e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.011310991266624012,
                            "process_2": 0.011382121477417552,
                            "process_1": 0.011397398436770807,
                            "process_3": 0.011387356734943625
                        },
                        "total_energy_joules": {
                            "process_0": 40719.56855984644,
                            "process_2": 40975.637318703186,
                            "process_1": 41030.6343723749,
                            "process_3": 40994.48424579705
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1151.1422513052646,
                        "ram_power_avg": 0.7112038135528564,
                        "cpu_energy_total": 0.0042633831132407075,
                        "gpu_energy_total": 0.041185863782015275,
                        "ram_energy_total": 2.8621020500014937e-05,
                        "total_energy_kwh": 0.04547786791575599,
                        "total_energy_joules": 163720.3244967216
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.10007309752386963,
                        "joules_per_token": 9.992695586958105,
                        "flops_per_joule": 321515257.6240958,
                        "joules_per_flop": 3.110272300573569e-09
                    },
                    "per-process_emissions": [
                        0.004308922123020417,
                        0.004336019176822217,
                        0.004341838934487839,
                        0.004338013548176774
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0584": {
            "setup": {
                "experiment_id": "0584",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 04:55:58 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_burst_0.1_0.2_4.0_20",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.1,
                    "delay_max": 0.2,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 20
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 39.40410794294439,
                        "average_latency_ms_per_batch": 4925.513492868049,
                        "throughput_queries_per_sec": 3.2483922789303845,
                        "throughput_tokens_per_sec": 415.7942117030892
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38352715776,
                        "gpu_max_memory_reserved_bytes": 38352715776
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            28.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 11.3,
                        "cpu_memory_usage_bytes": 1971249152
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0584",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 998.326948202785,
                            "process_2": 1302.9899821836252,
                            "process_1": 1180.695718841989,
                            "process_3": 1122.5563559926593
                        },
                        "ram_power": {
                            "process_0": 0.6860861778259277,
                            "process_2": 0.7063608169555664,
                            "process_1": 0.7292160987854004,
                            "process_3": 0.7231521606445312
                        },
                        "cpu_energy": {
                            "process_0": 0.0010581185150058446,
                            "process_2": 0.001060261501412242,
                            "process_1": 0.0010751834158818385,
                            "process_3": 0.001069819680940782
                        },
                        "gpu_energy": {
                            "process_0": 0.010246019863478395,
                            "process_2": 0.010314807974067364,
                            "process_1": 0.010314807974067364,
                            "process_3": 0.010310227970402153
                        },
                        "ram_energy": {
                            "process_0": 6.852888139772881e-06,
                            "process_2": 7.052001937945975e-06,
                            "process_1": 7.407046821605142e-06,
                            "process_3": 7.309083600690938e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.011310991266624012,
                            "process_2": 0.011382121477417552,
                            "process_1": 0.011397398436770807,
                            "process_3": 0.011387356734943625
                        },
                        "total_energy_joules": {
                            "process_0": 40719.56855984644,
                            "process_2": 40975.637318703186,
                            "process_1": 41030.6343723749,
                            "process_3": 40994.48424579705
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1151.1422513052646,
                        "ram_power_avg": 0.7112038135528564,
                        "cpu_energy_total": 0.0042633831132407075,
                        "gpu_energy_total": 0.041185863782015275,
                        "ram_energy_total": 2.8621020500014937e-05,
                        "total_energy_kwh": 0.04547786791575599,
                        "total_energy_joules": 163720.3244967216
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.10007309752386963,
                        "joules_per_token": 9.992695586958105,
                        "flops_per_joule": 321515257.6240958,
                        "joules_per_flop": 3.110272300573569e-09
                    },
                    "per-process_emissions": [
                        0.004308922123020417,
                        0.004336019176822217,
                        0.004341838934487839,
                        0.004338013548176774
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0585": {
            "setup": {
                "experiment_id": "0585",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 04:57:40 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_burst_0.2_0.4_4.0_8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.2,
                    "delay_max": 0.4,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 8
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 46.457421964849345,
                        "average_latency_ms_per_batch": 5807.177745606168,
                        "throughput_queries_per_sec": 2.7552109993715854,
                        "throughput_tokens_per_sec": 352.66700791956293
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38352715776,
                        "gpu_max_memory_reserved_bytes": 38352715776
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 9.8,
                        "cpu_memory_usage_bytes": 1954091008
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0585",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_2": 991.2285890259398,
                            "process_3": 909.9453722582473,
                            "process_1": 890.9836605752113,
                            "process_0": 723.5896484138398
                        },
                        "ram_power": {
                            "process_2": 0.7058200836181641,
                            "process_3": 0.7234811782836914,
                            "process_1": 0.7311229705810547,
                            "process_0": 0.6800193786621094
                        },
                        "cpu_energy": {
                            "process_2": 0.0013158990853353316,
                            "process_3": 0.0013334341030658832,
                            "process_1": 0.00130305669309746,
                            "process_0": 0.0013333238854775123
                        },
                        "gpu_energy": {
                            "process_2": 0.010570950956752867,
                            "process_3": 0.010732539697134058,
                            "process_1": 0.010649122130400812,
                            "process_0": 0.010687937717008822
                        },
                        "ram_energy": {
                            "process_2": 8.687008843334224e-06,
                            "process_3": 8.941354261096295e-06,
                            "process_1": 8.891868528459895e-06,
                            "process_0": 8.405649894962976e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.011895537050931533,
                            "process_3": 0.012074915154461043,
                            "process_1": 0.011961070692026732,
                            "process_0": 0.012029667252381296
                        },
                        "total_energy_joules": {
                            "process_2": 42823.93338335352,
                            "process_3": 43469.69455605975,
                            "process_1": 43059.85449129623,
                            "process_0": 43306.802108572665
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 878.9368175683095,
                        "ram_power_avg": 0.7101109027862549,
                        "cpu_energy_total": 0.0052857137669761875,
                        "gpu_energy_total": 0.04264055050129656,
                        "ram_energy_total": 3.492588152785339e-05,
                        "total_energy_kwh": 0.0479611901498006,
                        "total_energy_joules": 172660.28453928215
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09489153828118739,
                        "joules_per_token": 10.538347445024545,
                        "flops_per_joule": 304867922.86553967,
                        "joules_per_flop": 3.2801089422616776e-09
                    },
                    "per-process_emissions": [
                        0.004531604839552368,
                        0.0045999389280919345,
                        0.0045565698801275836,
                        0.004582701739794655
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0585": {
            "setup": {
                "experiment_id": "0585",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 04:57:40 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_burst_0.2_0.4_4.0_8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.2,
                    "delay_max": 0.4,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 8
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 46.457421964849345,
                        "average_latency_ms_per_batch": 5807.177745606168,
                        "throughput_queries_per_sec": 2.7552109993715854,
                        "throughput_tokens_per_sec": 352.66700791956293
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38352715776,
                        "gpu_max_memory_reserved_bytes": 38352715776
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 9.8,
                        "cpu_memory_usage_bytes": 1954091008
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0585",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_2": 991.2285890259398,
                            "process_3": 909.9453722582473,
                            "process_1": 890.9836605752113,
                            "process_0": 723.5896484138398
                        },
                        "ram_power": {
                            "process_2": 0.7058200836181641,
                            "process_3": 0.7234811782836914,
                            "process_1": 0.7311229705810547,
                            "process_0": 0.6800193786621094
                        },
                        "cpu_energy": {
                            "process_2": 0.0013158990853353316,
                            "process_3": 0.0013334341030658832,
                            "process_1": 0.00130305669309746,
                            "process_0": 0.0013333238854775123
                        },
                        "gpu_energy": {
                            "process_2": 0.010570950956752867,
                            "process_3": 0.010732539697134058,
                            "process_1": 0.010649122130400812,
                            "process_0": 0.010687937717008822
                        },
                        "ram_energy": {
                            "process_2": 8.687008843334224e-06,
                            "process_3": 8.941354261096295e-06,
                            "process_1": 8.891868528459895e-06,
                            "process_0": 8.405649894962976e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.011895537050931533,
                            "process_3": 0.012074915154461043,
                            "process_1": 0.011961070692026732,
                            "process_0": 0.012029667252381296
                        },
                        "total_energy_joules": {
                            "process_2": 42823.93338335352,
                            "process_3": 43469.69455605975,
                            "process_1": 43059.85449129623,
                            "process_0": 43306.802108572665
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 878.9368175683095,
                        "ram_power_avg": 0.7101109027862549,
                        "cpu_energy_total": 0.0052857137669761875,
                        "gpu_energy_total": 0.04264055050129656,
                        "ram_energy_total": 3.492588152785339e-05,
                        "total_energy_kwh": 0.0479611901498006,
                        "total_energy_joules": 172660.28453928215
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09489153828118739,
                        "joules_per_token": 10.538347445024545,
                        "flops_per_joule": 304867922.86553967,
                        "joules_per_flop": 3.2801089422616776e-09
                    },
                    "per-process_emissions": [
                        0.004531604839552368,
                        0.0045999389280919345,
                        0.0045565698801275836,
                        0.004582701739794655
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0586": {
            "setup": {
                "experiment_id": "0586",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 05:01:15 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_3",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 3,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 160.26247926132055,
                        "average_latency_ms_per_batch": 3727.0344014260595,
                        "throughput_queries_per_sec": 0.79868975314731,
                        "throughput_tokens_per_sec": 102.23228840285569
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38042337280,
                        "gpu_max_memory_reserved_bytes": 38042337280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            21.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 8.8,
                        "cpu_memory_usage_bytes": 1967140864
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0586",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_1": 131.46674896284964,
                            "process_2": 561.7132013277275,
                            "process_3": 574.5172331140717,
                            "process_0": 477.47043436554884
                        },
                        "ram_power": {
                            "process_1": 0.7279844284057617,
                            "process_2": 0.7230534553527832,
                            "process_3": 0.7038989067077637,
                            "process_0": 0.6858358383178712
                        },
                        "cpu_energy": {
                            "process_1": 0.0046908804872891775,
                            "process_2": 0.004777053751906351,
                            "process_3": 0.004616486699327653,
                            "process_0": 0.0048834061222296455
                        },
                        "gpu_energy": {
                            "process_1": 0.032848799612352764,
                            "process_2": 0.03307467007082643,
                            "process_3": 0.03306842006582755,
                            "process_0": 0.033080072852927245
                        },
                        "ram_energy": {
                            "process_1": 2.9201230858903717e-05,
                            "process_2": 2.917942639302137e-05,
                            "process_3": 2.79821043093709e-05,
                            "process_0": 2.8108556005933297e-05
                        },
                        "total_energy_kwh": {
                            "process_1": 0.03756888133050086,
                            "process_2": 0.037880903249125825,
                            "process_3": 0.03771288886946457,
                            "process_0": 0.03799158753116282
                        },
                        "total_energy_joules": {
                            "process_1": 135247.9727898031,
                            "process_2": 136371.25169685297,
                            "process_3": 135766.39993007245,
                            "process_0": 136769.71511218615
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 436.2919044425495,
                        "ram_power_avg": 0.7101931571960449,
                        "cpu_energy_total": 0.018967827060752826,
                        "gpu_energy_total": 0.132071962601934,
                        "ram_energy_total": 0.00011447131756722929,
                        "total_energy_kwh": 0.15115426098025408,
                        "total_energy_joules": 544155.3395289147
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.030109049401562303,
                        "joules_per_token": 33.21260617241911,
                        "flops_per_joule": 96734477.24400572,
                        "joules_per_flop": 1.0337575893210983e-08
                    },
                    "per-process_emissions": [
                        0.014311865342854304,
                        0.014430730092754484,
                        0.014366725014822529,
                        0.014472895269996478
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0586": {
            "setup": {
                "experiment_id": "0586",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 05:01:15 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_3",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 3,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 160.26247926132055,
                        "average_latency_ms_per_batch": 3727.0344014260595,
                        "throughput_queries_per_sec": 0.79868975314731,
                        "throughput_tokens_per_sec": 102.23228840285569
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38042337280,
                        "gpu_max_memory_reserved_bytes": 38042337280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            21.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 8.8,
                        "cpu_memory_usage_bytes": 1967140864
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0586",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_1": 131.46674896284964,
                            "process_2": 561.7132013277275,
                            "process_3": 574.5172331140717,
                            "process_0": 477.47043436554884
                        },
                        "ram_power": {
                            "process_1": 0.7279844284057617,
                            "process_2": 0.7230534553527832,
                            "process_3": 0.7038989067077637,
                            "process_0": 0.6858358383178712
                        },
                        "cpu_energy": {
                            "process_1": 0.0046908804872891775,
                            "process_2": 0.004777053751906351,
                            "process_3": 0.004616486699327653,
                            "process_0": 0.0048834061222296455
                        },
                        "gpu_energy": {
                            "process_1": 0.032848799612352764,
                            "process_2": 0.03307467007082643,
                            "process_3": 0.03306842006582755,
                            "process_0": 0.033080072852927245
                        },
                        "ram_energy": {
                            "process_1": 2.9201230858903717e-05,
                            "process_2": 2.917942639302137e-05,
                            "process_3": 2.79821043093709e-05,
                            "process_0": 2.8108556005933297e-05
                        },
                        "total_energy_kwh": {
                            "process_1": 0.03756888133050086,
                            "process_2": 0.037880903249125825,
                            "process_3": 0.03771288886946457,
                            "process_0": 0.03799158753116282
                        },
                        "total_energy_joules": {
                            "process_1": 135247.9727898031,
                            "process_2": 136371.25169685297,
                            "process_3": 135766.39993007245,
                            "process_0": 136769.71511218615
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 436.2919044425495,
                        "ram_power_avg": 0.7101931571960449,
                        "cpu_energy_total": 0.018967827060752826,
                        "gpu_energy_total": 0.132071962601934,
                        "ram_energy_total": 0.00011447131756722929,
                        "total_energy_kwh": 0.15115426098025408,
                        "total_energy_joules": 544155.3395289147
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.030109049401562303,
                        "joules_per_token": 33.21260617241911,
                        "flops_per_joule": 96734477.24400572,
                        "joules_per_flop": 1.0337575893210983e-08
                    },
                    "per-process_emissions": [
                        0.014311865342854304,
                        0.014430730092754484,
                        0.014366725014822529,
                        0.014472895269996478
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0587": {
            "setup": {
                "experiment_id": "0587",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 05:02:53 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_burst_0.2_0.4_6.0_10",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.2,
                    "delay_max": 0.4,
                    "simulate_burst": true,
                    "burst_interval": 6.0,
                    "burst_size": 10
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 41.639797626121435,
                        "average_latency_ms_per_batch": 5204.974703265179,
                        "throughput_queries_per_sec": 3.073982278907695,
                        "throughput_tokens_per_sec": 393.469731700185
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38352715776,
                        "gpu_max_memory_reserved_bytes": 38352715776
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            27.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 6.8,
                        "cpu_memory_usage_bytes": 1974063104
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0587",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_0": 1529.6754709845782,
                            "process_1": 1480.4148172178077,
                            "process_3": 507.35411748150193,
                            "process_2": 1147.5683590646445
                        },
                        "ram_power": {
                            "process_0": 0.6882705688476564,
                            "process_1": 0.7309741973876953,
                            "process_3": 0.7081475257873535,
                            "process_2": 0.7235040664672852
                        },
                        "cpu_energy": {
                            "process_0": 0.0012237494581531791,
                            "process_1": 0.0012312622016561363,
                            "process_3": 0.0011971733122209116,
                            "process_2": 0.0011805621760668148
                        },
                        "gpu_energy": {
                            "process_0": 0.01045832864443419,
                            "process_1": 0.01045832864443419,
                            "process_3": 0.010486151166691826,
                            "process_2": 0.010460524479524835
                        },
                        "ram_energy": {
                            "process_0": 7.503969701393054e-06,
                            "process_1": 7.66678738904301e-06,
                            "process_3": 7.581106226092858e-06,
                            "process_2": 7.682072919904278e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.011689582072288759,
                            "process_1": 0.011697257633479374,
                            "process_3": 0.011690905585138828,
                            "process_2": 0.011648768728511552
                        },
                        "total_energy_joules": {
                            "process_0": 42082.49546023953,
                            "process_1": 42110.12748052575,
                            "process_3": 42087.26010649978,
                            "process_2": 41935.56742264159
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1166.2531911871329,
                        "ram_power_avg": 0.7127240896224976,
                        "cpu_energy_total": 0.004832747148097042,
                        "gpu_energy_total": 0.04186333293508504,
                        "ram_energy_total": 3.04339362364332e-05,
                        "total_energy_kwh": 0.04672651401941851,
                        "total_energy_joules": 168215.45046990665
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09739890095845304,
                        "joules_per_token": 10.267056303094888,
                        "flops_per_joule": 312923587.94521624,
                        "joules_per_flop": 3.195668330937937e-09
                    },
                    "per-process_emissions": [
                        0.004453146290438403,
                        0.004456070295473968,
                        0.004453650482658636,
                        0.004437598447126476
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0587": {
            "setup": {
                "experiment_id": "0587",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 05:02:53 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_burst_0.2_0.4_6.0_10",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.2,
                    "delay_max": 0.4,
                    "simulate_burst": true,
                    "burst_interval": 6.0,
                    "burst_size": 10
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 41.639797626121435,
                        "average_latency_ms_per_batch": 5204.974703265179,
                        "throughput_queries_per_sec": 3.073982278907695,
                        "throughput_tokens_per_sec": 393.469731700185
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38352715776,
                        "gpu_max_memory_reserved_bytes": 38352715776
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            27.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 6.8,
                        "cpu_memory_usage_bytes": 1974063104
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0587",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_0": 1529.6754709845782,
                            "process_1": 1480.4148172178077,
                            "process_3": 507.35411748150193,
                            "process_2": 1147.5683590646445
                        },
                        "ram_power": {
                            "process_0": 0.6882705688476564,
                            "process_1": 0.7309741973876953,
                            "process_3": 0.7081475257873535,
                            "process_2": 0.7235040664672852
                        },
                        "cpu_energy": {
                            "process_0": 0.0012237494581531791,
                            "process_1": 0.0012312622016561363,
                            "process_3": 0.0011971733122209116,
                            "process_2": 0.0011805621760668148
                        },
                        "gpu_energy": {
                            "process_0": 0.01045832864443419,
                            "process_1": 0.01045832864443419,
                            "process_3": 0.010486151166691826,
                            "process_2": 0.010460524479524835
                        },
                        "ram_energy": {
                            "process_0": 7.503969701393054e-06,
                            "process_1": 7.66678738904301e-06,
                            "process_3": 7.581106226092858e-06,
                            "process_2": 7.682072919904278e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.011689582072288759,
                            "process_1": 0.011697257633479374,
                            "process_3": 0.011690905585138828,
                            "process_2": 0.011648768728511552
                        },
                        "total_energy_joules": {
                            "process_0": 42082.49546023953,
                            "process_1": 42110.12748052575,
                            "process_3": 42087.26010649978,
                            "process_2": 41935.56742264159
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1166.2531911871329,
                        "ram_power_avg": 0.7127240896224976,
                        "cpu_energy_total": 0.004832747148097042,
                        "gpu_energy_total": 0.04186333293508504,
                        "ram_energy_total": 3.04339362364332e-05,
                        "total_energy_kwh": 0.04672651401941851,
                        "total_energy_joules": 168215.45046990665
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09739890095845304,
                        "joules_per_token": 10.267056303094888,
                        "flops_per_joule": 312923587.94521624,
                        "joules_per_flop": 3.195668330937937e-09
                    },
                    "per-process_emissions": [
                        0.004453146290438403,
                        0.004456070295473968,
                        0.004453650482658636,
                        0.004437598447126476
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0588": {
            "setup": {
                "experiment_id": "0588",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 05:04:25 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_300_temp_0.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 0.0,
                    "decoder_top_k": 300,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.16855461098021,
                        "average_latency_ms_per_batch": 4771.069326372526,
                        "throughput_queries_per_sec": 3.353545904595962,
                        "throughput_tokens_per_sec": 429.25387578828315
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38325452800,
                        "gpu_max_memory_reserved_bytes": 38325452800
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 6.7,
                        "cpu_memory_usage_bytes": 1946259456
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0588",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 113.35688747141592,
                            "process_3": 89.79435379752248,
                            "process_2": 118.20387658569533,
                            "process_1": 118.86071005430063
                        },
                        "ram_power": {
                            "process_0": 0.6785573959350586,
                            "process_3": 0.7064309120178223,
                            "process_2": 0.7277812957763672,
                            "process_1": 0.7220749855041504
                        },
                        "cpu_energy": {
                            "process_0": 0.001073305024216097,
                            "process_3": 0.0011050903386167195,
                            "process_2": 0.001083921632962301,
                            "process_1": 0.001096117932280322
                        },
                        "gpu_energy": {
                            "process_0": 0.010210686501874733,
                            "process_3": 0.010214077337920457,
                            "process_2": 0.010210686501874733,
                            "process_1": 0.010212355114319394
                        },
                        "ram_energy": {
                            "process_0": 7.249164286368287e-06,
                            "process_3": 6.266745563310578e-06,
                            "process_2": 7.466058804453991e-06,
                            "process_1": 6.759428883786037e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.011291240690377199,
                            "process_3": 0.011325434422100487,
                            "process_2": 0.011302074193641484,
                            "process_1": 0.011315232475483501
                        },
                        "total_energy_joules": {
                            "process_0": 40648.466485357916,
                            "process_3": 40771.563919561755,
                            "process_2": 40687.467097109344,
                            "process_1": 40734.836911740604
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 110.05395697723358,
                        "ram_power_avg": 0.7087111473083496,
                        "cpu_energy_total": 0.004358434928075439,
                        "gpu_energy_total": 0.04084780545598932,
                        "ram_energy_total": 2.7741397537918892e-05,
                        "total_energy_kwh": 0.04523398178160267,
                        "total_energy_joules": 162842.33441376963
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.10061265738410222,
                        "joules_per_token": 9.939107325059181,
                        "flops_per_joule": 323248757.75308824,
                        "joules_per_flop": 3.093592708448533e-09
                    },
                    "per-process_emissions": [
                        0.004301398140999194,
                        0.00431442424309918,
                        0.004305525164067723,
                        0.00431053781153544
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0588": {
            "setup": {
                "experiment_id": "0588",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 05:04:25 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_300_temp_0.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 0.0,
                    "decoder_top_k": 300,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.16855461098021,
                        "average_latency_ms_per_batch": 4771.069326372526,
                        "throughput_queries_per_sec": 3.353545904595962,
                        "throughput_tokens_per_sec": 429.25387578828315
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38325452800,
                        "gpu_max_memory_reserved_bytes": 38325452800
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 6.7,
                        "cpu_memory_usage_bytes": 1946259456
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0588",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 113.35688747141592,
                            "process_3": 89.79435379752248,
                            "process_2": 118.20387658569533,
                            "process_1": 118.86071005430063
                        },
                        "ram_power": {
                            "process_0": 0.6785573959350586,
                            "process_3": 0.7064309120178223,
                            "process_2": 0.7277812957763672,
                            "process_1": 0.7220749855041504
                        },
                        "cpu_energy": {
                            "process_0": 0.001073305024216097,
                            "process_3": 0.0011050903386167195,
                            "process_2": 0.001083921632962301,
                            "process_1": 0.001096117932280322
                        },
                        "gpu_energy": {
                            "process_0": 0.010210686501874733,
                            "process_3": 0.010214077337920457,
                            "process_2": 0.010210686501874733,
                            "process_1": 0.010212355114319394
                        },
                        "ram_energy": {
                            "process_0": 7.249164286368287e-06,
                            "process_3": 6.266745563310578e-06,
                            "process_2": 7.466058804453991e-06,
                            "process_1": 6.759428883786037e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.011291240690377199,
                            "process_3": 0.011325434422100487,
                            "process_2": 0.011302074193641484,
                            "process_1": 0.011315232475483501
                        },
                        "total_energy_joules": {
                            "process_0": 40648.466485357916,
                            "process_3": 40771.563919561755,
                            "process_2": 40687.467097109344,
                            "process_1": 40734.836911740604
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 110.05395697723358,
                        "ram_power_avg": 0.7087111473083496,
                        "cpu_energy_total": 0.004358434928075439,
                        "gpu_energy_total": 0.04084780545598932,
                        "ram_energy_total": 2.7741397537918892e-05,
                        "total_energy_kwh": 0.04523398178160267,
                        "total_energy_joules": 162842.33441376963
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.10061265738410222,
                        "joules_per_token": 9.939107325059181,
                        "flops_per_joule": 323248757.75308824,
                        "joules_per_flop": 3.093592708448533e-09
                    },
                    "per-process_emissions": [
                        0.004301398140999194,
                        0.00431442424309918,
                        0.004305525164067723,
                        0.00431053781153544
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0589": {
            "setup": {
                "experiment_id": "0589",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 05:05:59 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_topp_0.5_temp_1.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_p",
                    "decoder_temperature": 1.4,
                    "decoder_top_k": null,
                    "decoder_top_p": 0.5
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 39.12953448097687,
                        "average_latency_ms_per_batch": 4891.191810122109,
                        "throughput_queries_per_sec": 3.271186373613215,
                        "throughput_tokens_per_sec": 418.71185582249154
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38403047424,
                        "gpu_max_memory_reserved_bytes": 38403047424
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.0,
                        "cpu_memory_usage_bytes": 1958367232
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0589",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_3": 532.8159480431173,
                            "process_2": 0.0,
                            "process_0": 825.2726203818771,
                            "process_1": 516.001310731236
                        },
                        "ram_power": {
                            "process_3": 0.7261791229248048,
                            "process_2": 0.7339210510253906,
                            "process_0": 0.6827602386474609,
                            "process_1": 0.7265124320983887
                        },
                        "cpu_energy": {
                            "process_3": 0.001184900857038883,
                            "process_2": 0.0012733283262532495,
                            "process_0": 0.0012674022209703253,
                            "process_1": 0.0012279576613345854
                        },
                        "gpu_energy": {
                            "process_3": 0.010403047766875062,
                            "process_2": 0.01038876581100645,
                            "process_0": 0.010392268869363619,
                            "process_1": 0.010389081089035912
                        },
                        "ram_energy": {
                            "process_3": 7.354962712953505e-06,
                            "process_2": 7.293773661145898e-06,
                            "process_0": 6.681857669600567e-06,
                            "process_1": 7.363635730427673e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.011595303586626901,
                            "process_2": 0.011669387910920846,
                            "process_0": 0.011666352948003543,
                            "process_1": 0.011624402386100928
                        },
                        "total_energy_joules": {
                            "process_3": 41743.092911856846,
                            "process_2": 42009.796479315046,
                            "process_0": 41998.87061281276,
                            "process_1": 41847.84858996334
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 468.5224697890576,
                        "ram_power_avg": 0.7173432111740112,
                        "cpu_energy_total": 0.004953589065597043,
                        "gpu_energy_total": 0.041573163536281044,
                        "ram_energy_total": 2.869422977412764e-05,
                        "total_energy_kwh": 0.04655544683165222,
                        "total_energy_joules": 167599.60859394798
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09775679154295845,
                        "joules_per_token": 10.229468297970458,
                        "flops_per_joule": 314073420.28103507,
                        "joules_per_flop": 3.183968892067317e-09
                    },
                    "per-process_emissions": [
                        0.004417230901325518,
                        0.004445453324665297,
                        0.00444429715554195,
                        0.004428316088985149
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0589": {
            "setup": {
                "experiment_id": "0589",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 05:05:59 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_topp_0.5_temp_1.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_p",
                    "decoder_temperature": 1.4,
                    "decoder_top_k": null,
                    "decoder_top_p": 0.5
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 39.12953448097687,
                        "average_latency_ms_per_batch": 4891.191810122109,
                        "throughput_queries_per_sec": 3.271186373613215,
                        "throughput_tokens_per_sec": 418.71185582249154
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38403047424,
                        "gpu_max_memory_reserved_bytes": 38403047424
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.0,
                        "cpu_memory_usage_bytes": 1958367232
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0589",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_3": 532.8159480431173,
                            "process_2": 0.0,
                            "process_0": 825.2726203818771,
                            "process_1": 516.001310731236
                        },
                        "ram_power": {
                            "process_3": 0.7261791229248048,
                            "process_2": 0.7339210510253906,
                            "process_0": 0.6827602386474609,
                            "process_1": 0.7265124320983887
                        },
                        "cpu_energy": {
                            "process_3": 0.001184900857038883,
                            "process_2": 0.0012733283262532495,
                            "process_0": 0.0012674022209703253,
                            "process_1": 0.0012279576613345854
                        },
                        "gpu_energy": {
                            "process_3": 0.010403047766875062,
                            "process_2": 0.01038876581100645,
                            "process_0": 0.010392268869363619,
                            "process_1": 0.010389081089035912
                        },
                        "ram_energy": {
                            "process_3": 7.354962712953505e-06,
                            "process_2": 7.293773661145898e-06,
                            "process_0": 6.681857669600567e-06,
                            "process_1": 7.363635730427673e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.011595303586626901,
                            "process_2": 0.011669387910920846,
                            "process_0": 0.011666352948003543,
                            "process_1": 0.011624402386100928
                        },
                        "total_energy_joules": {
                            "process_3": 41743.092911856846,
                            "process_2": 42009.796479315046,
                            "process_0": 41998.87061281276,
                            "process_1": 41847.84858996334
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 468.5224697890576,
                        "ram_power_avg": 0.7173432111740112,
                        "cpu_energy_total": 0.004953589065597043,
                        "gpu_energy_total": 0.041573163536281044,
                        "ram_energy_total": 2.869422977412764e-05,
                        "total_energy_kwh": 0.04655544683165222,
                        "total_energy_joules": 167599.60859394798
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09775679154295845,
                        "joules_per_token": 10.229468297970458,
                        "flops_per_joule": 314073420.28103507,
                        "joules_per_flop": 3.183968892067317e-09
                    },
                    "per-process_emissions": [
                        0.004417230901325518,
                        0.004445453324665297,
                        0.00444429715554195,
                        0.004428316088985149
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0590": {
            "setup": {
                "experiment_id": "0590",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 05:07:44 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_burst_0.4_0.5_6.0_5",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.4,
                    "delay_max": 0.5,
                    "simulate_burst": true,
                    "burst_interval": 6.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 49.282187891949434,
                        "average_latency_ms_per_batch": 6160.273486493679,
                        "throughput_queries_per_sec": 2.597287285228455,
                        "throughput_tokens_per_sec": 332.45277250924227
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38352715776,
                        "gpu_max_memory_reserved_bytes": 38352715776
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            22.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.6,
                        "cpu_memory_usage_bytes": 1972621312
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0590",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_3": 95.74807119781538,
                            "process_1": 51.56806197184582,
                            "process_0": 94.48017675479906,
                            "process_2": 30.65988589385081
                        },
                        "ram_power": {
                            "process_3": 0.7310571670532227,
                            "process_1": 0.7077469825744629,
                            "process_0": 0.687769889831543,
                            "process_2": 0.7066082954406738
                        },
                        "cpu_energy": {
                            "process_3": 0.001504847377178521,
                            "process_1": 0.0014840955762501834,
                            "process_0": 0.0015315877924422239,
                            "process_2": 0.0014742257091111242
                        },
                        "gpu_energy": {
                            "process_3": 0.010877888980081352,
                            "process_1": 0.010849372290602233,
                            "process_0": 0.010866659248877752,
                            "process_2": 0.010849372290602233
                        },
                        "ram_energy": {
                            "process_3": 9.323819598533822e-06,
                            "process_1": 8.940542810883074e-06,
                            "process_0": 8.952122511076126e-06,
                            "process_2": 8.47379629184722e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.012392060176858405,
                            "process_1": 0.0123424084096633,
                            "process_0": 0.012407199163831054,
                            "process_2": 0.012332071796005203
                        },
                        "total_energy_joules": {
                            "process_3": 44611.41663669026,
                            "process_1": 44432.67027478788,
                            "process_0": 44665.9169897918,
                            "process_2": 44395.45846561873
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 68.11404895457777,
                        "ram_power_avg": 0.7082955837249756,
                        "cpu_energy_total": 0.005994756454982052,
                        "gpu_energy_total": 0.04344329281016357,
                        "ram_energy_total": 3.5690281212340244e-05,
                        "total_energy_kwh": 0.04947373954635796,
                        "total_energy_joules": 178105.46236688865
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09199044084481671,
                        "joules_per_token": 10.870694724541544,
                        "flops_per_joule": 295547265.13907284,
                        "joules_per_flop": 3.3835535562456974e-09
                    },
                    "per-process_emissions": [
                        0.004720755324374209,
                        0.004701840483661235,
                        0.00472652252146144,
                        0.004697902750688182
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0590": {
            "setup": {
                "experiment_id": "0590",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 05:07:44 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_burst_0.4_0.5_6.0_5",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.4,
                    "delay_max": 0.5,
                    "simulate_burst": true,
                    "burst_interval": 6.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 49.282187891949434,
                        "average_latency_ms_per_batch": 6160.273486493679,
                        "throughput_queries_per_sec": 2.597287285228455,
                        "throughput_tokens_per_sec": 332.45277250924227
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38352715776,
                        "gpu_max_memory_reserved_bytes": 38352715776
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            22.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.6,
                        "cpu_memory_usage_bytes": 1972621312
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0590",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_3": 95.74807119781538,
                            "process_1": 51.56806197184582,
                            "process_0": 94.48017675479906,
                            "process_2": 30.65988589385081
                        },
                        "ram_power": {
                            "process_3": 0.7310571670532227,
                            "process_1": 0.7077469825744629,
                            "process_0": 0.687769889831543,
                            "process_2": 0.7066082954406738
                        },
                        "cpu_energy": {
                            "process_3": 0.001504847377178521,
                            "process_1": 0.0014840955762501834,
                            "process_0": 0.0015315877924422239,
                            "process_2": 0.0014742257091111242
                        },
                        "gpu_energy": {
                            "process_3": 0.010877888980081352,
                            "process_1": 0.010849372290602233,
                            "process_0": 0.010866659248877752,
                            "process_2": 0.010849372290602233
                        },
                        "ram_energy": {
                            "process_3": 9.323819598533822e-06,
                            "process_1": 8.940542810883074e-06,
                            "process_0": 8.952122511076126e-06,
                            "process_2": 8.47379629184722e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.012392060176858405,
                            "process_1": 0.0123424084096633,
                            "process_0": 0.012407199163831054,
                            "process_2": 0.012332071796005203
                        },
                        "total_energy_joules": {
                            "process_3": 44611.41663669026,
                            "process_1": 44432.67027478788,
                            "process_0": 44665.9169897918,
                            "process_2": 44395.45846561873
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 68.11404895457777,
                        "ram_power_avg": 0.7082955837249756,
                        "cpu_energy_total": 0.005994756454982052,
                        "gpu_energy_total": 0.04344329281016357,
                        "ram_energy_total": 3.5690281212340244e-05,
                        "total_energy_kwh": 0.04947373954635796,
                        "total_energy_joules": 178105.46236688865
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09199044084481671,
                        "joules_per_token": 10.870694724541544,
                        "flops_per_joule": 295547265.13907284,
                        "joules_per_flop": 3.3835535562456974e-09
                    },
                    "per-process_emissions": [
                        0.004720755324374209,
                        0.004701840483661235,
                        0.00472652252146144,
                        0.004697902750688182
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0591": {
            "setup": {
                "experiment_id": "0591",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 05:09:18 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_topp_0.98_temp_1.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_p",
                    "decoder_temperature": 1.4,
                    "decoder_top_k": null,
                    "decoder_top_p": 0.98
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 39.64313437003875,
                        "average_latency_ms_per_batch": 4955.391796254844,
                        "throughput_queries_per_sec": 3.228806249405422,
                        "throughput_tokens_per_sec": 413.28719992389404
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38403047424,
                        "gpu_max_memory_reserved_bytes": 38403047424
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.4,
                        "cpu_memory_usage_bytes": 1981075456
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0591",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_0": 926.234091588737,
                            "process_3": 879.3212624957816,
                            "process_1": 1004.3511770022282,
                            "process_2": 1221.71046346126
                        },
                        "ram_power": {
                            "process_0": 0.690680980682373,
                            "process_3": 0.7338180541992188,
                            "process_1": 0.7182068824768066,
                            "process_2": 0.7270174026489258
                        },
                        "cpu_energy": {
                            "process_0": 0.0011482741417239593,
                            "process_3": 0.0011484820502719235,
                            "process_1": 0.0011472934138728305,
                            "process_2": 0.0011466979563992937
                        },
                        "gpu_energy": {
                            "process_0": 0.010431324733938041,
                            "process_3": 0.010430336955371544,
                            "process_1": 0.010413831942171825,
                            "process_2": 0.010381242471655838
                        },
                        "ram_energy": {
                            "process_0": 7.128518236805363e-06,
                            "process_3": 6.773925422142391e-06,
                            "process_1": 7.014093253699216e-06,
                            "process_2": 7.463247939756732e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.0115867273938988,
                            "process_3": 0.011585592931065609,
                            "process_1": 0.011568139449298355,
                            "process_2": 0.011535403675994893
                        },
                        "total_energy_joules": {
                            "process_0": 41712.21861803568,
                            "process_3": 41708.13455183619,
                            "process_1": 41645.30201747408,
                            "process_2": 41527.45323358162
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1007.9042486370017,
                        "ram_power_avg": 0.717430830001831,
                        "cpu_energy_total": 0.0045907475622680065,
                        "gpu_energy_total": 0.04165673610313725,
                        "ram_energy_total": 2.8379784852403704e-05,
                        "total_energy_kwh": 0.04627586345025766,
                        "total_energy_joules": 166593.10842092757
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09834740557576287,
                        "joules_per_token": 10.168036402644505,
                        "flops_per_joule": 315970947.46478415,
                        "joules_per_flop": 3.1648479330886986e-09
                    },
                    "per-process_emissions": [
                        0.004413963800705748,
                        0.004413531627089444,
                        0.004406882723210209,
                        0.004394412030370254
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0591": {
            "setup": {
                "experiment_id": "0591",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 05:09:18 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_topp_0.98_temp_1.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_p",
                    "decoder_temperature": 1.4,
                    "decoder_top_k": null,
                    "decoder_top_p": 0.98
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 39.64313437003875,
                        "average_latency_ms_per_batch": 4955.391796254844,
                        "throughput_queries_per_sec": 3.228806249405422,
                        "throughput_tokens_per_sec": 413.28719992389404
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38403047424,
                        "gpu_max_memory_reserved_bytes": 38403047424
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.4,
                        "cpu_memory_usage_bytes": 1981075456
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0591",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_0": 926.234091588737,
                            "process_3": 879.3212624957816,
                            "process_1": 1004.3511770022282,
                            "process_2": 1221.71046346126
                        },
                        "ram_power": {
                            "process_0": 0.690680980682373,
                            "process_3": 0.7338180541992188,
                            "process_1": 0.7182068824768066,
                            "process_2": 0.7270174026489258
                        },
                        "cpu_energy": {
                            "process_0": 0.0011482741417239593,
                            "process_3": 0.0011484820502719235,
                            "process_1": 0.0011472934138728305,
                            "process_2": 0.0011466979563992937
                        },
                        "gpu_energy": {
                            "process_0": 0.010431324733938041,
                            "process_3": 0.010430336955371544,
                            "process_1": 0.010413831942171825,
                            "process_2": 0.010381242471655838
                        },
                        "ram_energy": {
                            "process_0": 7.128518236805363e-06,
                            "process_3": 6.773925422142391e-06,
                            "process_1": 7.014093253699216e-06,
                            "process_2": 7.463247939756732e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.0115867273938988,
                            "process_3": 0.011585592931065609,
                            "process_1": 0.011568139449298355,
                            "process_2": 0.011535403675994893
                        },
                        "total_energy_joules": {
                            "process_0": 41712.21861803568,
                            "process_3": 41708.13455183619,
                            "process_1": 41645.30201747408,
                            "process_2": 41527.45323358162
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1007.9042486370017,
                        "ram_power_avg": 0.717430830001831,
                        "cpu_energy_total": 0.0045907475622680065,
                        "gpu_energy_total": 0.04165673610313725,
                        "ram_energy_total": 2.8379784852403704e-05,
                        "total_energy_kwh": 0.04627586345025766,
                        "total_energy_joules": 166593.10842092757
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09834740557576287,
                        "joules_per_token": 10.168036402644505,
                        "flops_per_joule": 315970947.46478415,
                        "joules_per_flop": 3.1648479330886986e-09
                    },
                    "per-process_emissions": [
                        0.004413963800705748,
                        0.004413531627089444,
                        0.004406882723210209,
                        0.004394412030370254
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0592": {
            "setup": {
                "experiment_id": "0592",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 05:10:49 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_topp_0.7_temp_1.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_p",
                    "decoder_temperature": 1.2,
                    "decoder_top_k": null,
                    "decoder_top_p": 0.7
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 39.11931954493048,
                        "average_latency_ms_per_batch": 4889.91494311631,
                        "throughput_queries_per_sec": 3.2720405541048754,
                        "throughput_tokens_per_sec": 418.82119092542405
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38403047424,
                        "gpu_max_memory_reserved_bytes": 38403047424
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            28.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.8,
                        "cpu_memory_usage_bytes": 1958572032
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0592",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_3": 566.4753542456075,
                            "process_0": 120.17941389092626,
                            "process_2": 3404.1717363063603,
                            "process_1": 826.5455185105978
                        },
                        "ram_power": {
                            "process_3": 0.733121395111084,
                            "process_0": 0.6828474998474121,
                            "process_2": 0.7334132194519043,
                            "process_1": 0.7257170677185059
                        },
                        "cpu_energy": {
                            "process_3": 0.0011611953875271864,
                            "process_0": 0.001117446644282609,
                            "process_2": 0.0011768998439383722,
                            "process_1": 0.0011298101073989528
                        },
                        "gpu_energy": {
                            "process_3": 0.01041649527763333,
                            "process_0": 0.01036794968324628,
                            "process_2": 0.01034522272061622,
                            "process_1": 0.01041649527763333
                        },
                        "ram_energy": {
                            "process_3": 7.222479157195827e-06,
                            "process_0": 6.903467433743975e-06,
                            "process_2": 7.364976932192258e-06,
                            "process_1": 7.318056614524997e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.011584913144317716,
                            "process_0": 0.011492299794962631,
                            "process_2": 0.011529487541486786,
                            "process_1": 0.011553623441646805
                        },
                        "total_energy_joules": {
                            "process_3": 41705.68731954378,
                            "process_0": 41372.27926186547,
                            "process_2": 41506.15514935243,
                            "process_1": 41593.044389928495
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1229.343005738373,
                        "ram_power_avg": 0.7187747955322266,
                        "cpu_energy_total": 0.004585351983147121,
                        "gpu_energy_total": 0.04154616295912916,
                        "ram_energy_total": 2.8808980137657057e-05,
                        "total_energy_kwh": 0.04616032392241394,
                        "total_energy_joules": 166177.16612069018
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.0985935696370025,
                        "joules_per_token": 10.142649299358531,
                        "flops_per_joule": 316761824.36902285,
                        "joules_per_flop": 3.1569460808352165e-09
                    },
                    "per-process_emissions": [
                        0.004413272662327834,
                        0.004377991606891014,
                        0.004392158278929391,
                        0.004401352850095351
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0592": {
            "setup": {
                "experiment_id": "0592",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 05:10:49 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_topp_0.7_temp_1.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_p",
                    "decoder_temperature": 1.2,
                    "decoder_top_k": null,
                    "decoder_top_p": 0.7
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 39.11931954493048,
                        "average_latency_ms_per_batch": 4889.91494311631,
                        "throughput_queries_per_sec": 3.2720405541048754,
                        "throughput_tokens_per_sec": 418.82119092542405
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38403047424,
                        "gpu_max_memory_reserved_bytes": 38403047424
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            28.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.8,
                        "cpu_memory_usage_bytes": 1958572032
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0592",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_3": 566.4753542456075,
                            "process_0": 120.17941389092626,
                            "process_2": 3404.1717363063603,
                            "process_1": 826.5455185105978
                        },
                        "ram_power": {
                            "process_3": 0.733121395111084,
                            "process_0": 0.6828474998474121,
                            "process_2": 0.7334132194519043,
                            "process_1": 0.7257170677185059
                        },
                        "cpu_energy": {
                            "process_3": 0.0011611953875271864,
                            "process_0": 0.001117446644282609,
                            "process_2": 0.0011768998439383722,
                            "process_1": 0.0011298101073989528
                        },
                        "gpu_energy": {
                            "process_3": 0.01041649527763333,
                            "process_0": 0.01036794968324628,
                            "process_2": 0.01034522272061622,
                            "process_1": 0.01041649527763333
                        },
                        "ram_energy": {
                            "process_3": 7.222479157195827e-06,
                            "process_0": 6.903467433743975e-06,
                            "process_2": 7.364976932192258e-06,
                            "process_1": 7.318056614524997e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.011584913144317716,
                            "process_0": 0.011492299794962631,
                            "process_2": 0.011529487541486786,
                            "process_1": 0.011553623441646805
                        },
                        "total_energy_joules": {
                            "process_3": 41705.68731954378,
                            "process_0": 41372.27926186547,
                            "process_2": 41506.15514935243,
                            "process_1": 41593.044389928495
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1229.343005738373,
                        "ram_power_avg": 0.7187747955322266,
                        "cpu_energy_total": 0.004585351983147121,
                        "gpu_energy_total": 0.04154616295912916,
                        "ram_energy_total": 2.8808980137657057e-05,
                        "total_energy_kwh": 0.04616032392241394,
                        "total_energy_joules": 166177.16612069018
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.0985935696370025,
                        "joules_per_token": 10.142649299358531,
                        "flops_per_joule": 316761824.36902285,
                        "joules_per_flop": 3.1569460808352165e-09
                    },
                    "per-process_emissions": [
                        0.004413272662327834,
                        0.004377991606891014,
                        0.004392158278929391,
                        0.004401352850095351
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0593": {
            "setup": {
                "experiment_id": "0593",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 05:12:22 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_400_temp_0.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 0.0,
                    "decoder_top_k": 400,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.14480813400587,
                        "average_latency_ms_per_batch": 4768.101016750734,
                        "throughput_queries_per_sec": 3.3556336041939283,
                        "throughput_tokens_per_sec": 429.5211013368228
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38325452800,
                        "gpu_max_memory_reserved_bytes": 38325452800
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            6.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.4,
                        "cpu_memory_usage_bytes": 1968541696
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0593",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_3": 123.68547223036862,
                            "process_0": 123.47680658324981,
                            "process_2": 139.06916899428967,
                            "process_1": 102.45975672560536
                        },
                        "ram_power": {
                            "process_3": 0.7282376289367677,
                            "process_0": 0.6863293647766113,
                            "process_2": 0.7297396659851074,
                            "process_1": 0.7210235595703125
                        },
                        "cpu_energy": {
                            "process_3": 0.001027646158128846,
                            "process_0": 0.0011082236872480282,
                            "process_2": 0.0010907125715039002,
                            "process_1": 0.0011109071182145273
                        },
                        "gpu_energy": {
                            "process_3": 0.010202050939410512,
                            "process_0": 0.010202050939410512,
                            "process_2": 0.01019888454798945,
                            "process_1": 0.010202050939410512
                        },
                        "ram_energy": {
                            "process_3": 7.147887773458017e-06,
                            "process_0": 6.108145478889411e-06,
                            "process_2": 6.429971732276855e-06,
                            "process_1": 6.417657810413173e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.011236844985312813,
                            "process_0": 0.011316382772137432,
                            "process_2": 0.011296027091225626,
                            "process_1": 0.011319375715435455
                        },
                        "total_energy_joules": {
                            "process_3": 40452.641947126125,
                            "process_0": 40738.977979694755,
                            "process_2": 40665.69752841225,
                            "process_1": 40749.75257556764
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 122.17280113337837,
                        "ram_power_avg": 0.7163325548171997,
                        "cpu_energy_total": 0.004337489535095301,
                        "gpu_energy_total": 0.040805037366220986,
                        "ram_energy_total": 2.6103662795037455e-05,
                        "total_energy_kwh": 0.04516863056411132,
                        "total_energy_joules": 162607.07003080077
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.10075822654511006,
                        "joules_per_token": 9.924747926684617,
                        "flops_per_joule": 323716442.9498255,
                        "joules_per_flop": 3.0891232798915784e-09
                    },
                    "per-process_emissions": [
                        0.004280676097154917,
                        0.004310976017045755,
                        0.004303221520402402,
                        0.004312116178795137
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0593": {
            "setup": {
                "experiment_id": "0593",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 05:12:22 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_400_temp_0.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 0.0,
                    "decoder_top_k": 400,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.14480813400587,
                        "average_latency_ms_per_batch": 4768.101016750734,
                        "throughput_queries_per_sec": 3.3556336041939283,
                        "throughput_tokens_per_sec": 429.5211013368228
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38325452800,
                        "gpu_max_memory_reserved_bytes": 38325452800
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            6.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.4,
                        "cpu_memory_usage_bytes": 1968541696
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0593",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_3": 123.68547223036862,
                            "process_0": 123.47680658324981,
                            "process_2": 139.06916899428967,
                            "process_1": 102.45975672560536
                        },
                        "ram_power": {
                            "process_3": 0.7282376289367677,
                            "process_0": 0.6863293647766113,
                            "process_2": 0.7297396659851074,
                            "process_1": 0.7210235595703125
                        },
                        "cpu_energy": {
                            "process_3": 0.001027646158128846,
                            "process_0": 0.0011082236872480282,
                            "process_2": 0.0010907125715039002,
                            "process_1": 0.0011109071182145273
                        },
                        "gpu_energy": {
                            "process_3": 0.010202050939410512,
                            "process_0": 0.010202050939410512,
                            "process_2": 0.01019888454798945,
                            "process_1": 0.010202050939410512
                        },
                        "ram_energy": {
                            "process_3": 7.147887773458017e-06,
                            "process_0": 6.108145478889411e-06,
                            "process_2": 6.429971732276855e-06,
                            "process_1": 6.417657810413173e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.011236844985312813,
                            "process_0": 0.011316382772137432,
                            "process_2": 0.011296027091225626,
                            "process_1": 0.011319375715435455
                        },
                        "total_energy_joules": {
                            "process_3": 40452.641947126125,
                            "process_0": 40738.977979694755,
                            "process_2": 40665.69752841225,
                            "process_1": 40749.75257556764
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 122.17280113337837,
                        "ram_power_avg": 0.7163325548171997,
                        "cpu_energy_total": 0.004337489535095301,
                        "gpu_energy_total": 0.040805037366220986,
                        "ram_energy_total": 2.6103662795037455e-05,
                        "total_energy_kwh": 0.04516863056411132,
                        "total_energy_joules": 162607.07003080077
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.10075822654511006,
                        "joules_per_token": 9.924747926684617,
                        "flops_per_joule": 323716442.9498255,
                        "joules_per_flop": 3.0891232798915784e-09
                    },
                    "per-process_emissions": [
                        0.004280676097154917,
                        0.004310976017045755,
                        0.004303221520402402,
                        0.004312116178795137
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0594": {
            "setup": {
                "experiment_id": "0594",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 05:13:55 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_100_temp_0.6",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 0.6,
                    "decoder_top_k": 100,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.64081799500855,
                        "average_latency_ms_per_batch": 4830.102249376068,
                        "throughput_queries_per_sec": 3.312559273888417,
                        "throughput_tokens_per_sec": 424.0075870577174
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 37878759424,
                        "gpu_max_memory_reserved_bytes": 37878759424
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            2.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.3,
                        "cpu_memory_usage_bytes": 1976172544
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0594",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 134.45693232089494,
                            "process_3": 134.20997818029426,
                            "process_2": 134.581327707021,
                            "process_1": 129.70833197123565
                        },
                        "ram_power": {
                            "process_0": 0.6889801025390625,
                            "process_3": 0.7246971130371094,
                            "process_2": 0.7251663208007812,
                            "process_1": 0.7248988151550293
                        },
                        "cpu_energy": {
                            "process_0": 0.001238739025337054,
                            "process_3": 0.0012142816928189858,
                            "process_2": 0.0012787719937132351,
                            "process_1": 0.0011516319723432389
                        },
                        "gpu_energy": {
                            "process_0": 0.010308044079762269,
                            "process_3": 0.010314551862746768,
                            "process_2": 0.01030747602375115,
                            "process_1": 0.01030747602375115
                        },
                        "ram_energy": {
                            "process_0": 6.578536396351095e-06,
                            "process_3": 6.757004631855176e-06,
                            "process_2": 7.174516880298051e-06,
                            "process_1": 6.37959698656615e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.011553361641495678,
                            "process_3": 0.01153559056019761,
                            "process_2": 0.01159342253434468,
                            "process_1": 0.01146548759308095
                        },
                        "total_energy_joules": {
                            "process_0": 41592.10190938444,
                            "process_3": 41528.1260167114,
                            "process_2": 41736.321123640846,
                            "process_1": 41275.755335091424
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 133.23914254486147,
                        "ram_power_avg": 0.7159355878829956,
                        "cpu_energy_total": 0.004883424684212514,
                        "gpu_energy_total": 0.041237547990011336,
                        "ram_energy_total": 2.6889654895070475e-05,
                        "total_energy_kwh": 0.04614786232911892,
                        "total_energy_joules": 166132.30438482808
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09862019346970702,
                        "joules_per_token": 10.139911156300542,
                        "flops_per_joule": 316847361.52778715,
                        "joules_per_flop": 3.156093821258793e-09
                    },
                    "per-process_emissions": [
                        0.0044012531173277785,
                        0.00439448322390728,
                        0.004416514314458606,
                        0.0043677774985841885
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0594": {
            "setup": {
                "experiment_id": "0594",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 05:13:55 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_100_temp_0.6",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 0.6,
                    "decoder_top_k": 100,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.64081799500855,
                        "average_latency_ms_per_batch": 4830.102249376068,
                        "throughput_queries_per_sec": 3.312559273888417,
                        "throughput_tokens_per_sec": 424.0075870577174
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 37878759424,
                        "gpu_max_memory_reserved_bytes": 37878759424
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            2.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.3,
                        "cpu_memory_usage_bytes": 1976172544
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0594",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 134.45693232089494,
                            "process_3": 134.20997818029426,
                            "process_2": 134.581327707021,
                            "process_1": 129.70833197123565
                        },
                        "ram_power": {
                            "process_0": 0.6889801025390625,
                            "process_3": 0.7246971130371094,
                            "process_2": 0.7251663208007812,
                            "process_1": 0.7248988151550293
                        },
                        "cpu_energy": {
                            "process_0": 0.001238739025337054,
                            "process_3": 0.0012142816928189858,
                            "process_2": 0.0012787719937132351,
                            "process_1": 0.0011516319723432389
                        },
                        "gpu_energy": {
                            "process_0": 0.010308044079762269,
                            "process_3": 0.010314551862746768,
                            "process_2": 0.01030747602375115,
                            "process_1": 0.01030747602375115
                        },
                        "ram_energy": {
                            "process_0": 6.578536396351095e-06,
                            "process_3": 6.757004631855176e-06,
                            "process_2": 7.174516880298051e-06,
                            "process_1": 6.37959698656615e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.011553361641495678,
                            "process_3": 0.01153559056019761,
                            "process_2": 0.01159342253434468,
                            "process_1": 0.01146548759308095
                        },
                        "total_energy_joules": {
                            "process_0": 41592.10190938444,
                            "process_3": 41528.1260167114,
                            "process_2": 41736.321123640846,
                            "process_1": 41275.755335091424
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 133.23914254486147,
                        "ram_power_avg": 0.7159355878829956,
                        "cpu_energy_total": 0.004883424684212514,
                        "gpu_energy_total": 0.041237547990011336,
                        "ram_energy_total": 2.6889654895070475e-05,
                        "total_energy_kwh": 0.04614786232911892,
                        "total_energy_joules": 166132.30438482808
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09862019346970702,
                        "joules_per_token": 10.139911156300542,
                        "flops_per_joule": 316847361.52778715,
                        "joules_per_flop": 3.156093821258793e-09
                    },
                    "per-process_emissions": [
                        0.0044012531173277785,
                        0.00439448322390728,
                        0.004416514314458606,
                        0.0043677774985841885
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0595": {
            "setup": {
                "experiment_id": "0595",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 05:15:29 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_topp_0.9_temp_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_p",
                    "decoder_temperature": 0.8,
                    "decoder_top_k": null,
                    "decoder_top_p": 0.9
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 39.10162747092545,
                        "average_latency_ms_per_batch": 4887.703433865681,
                        "throughput_queries_per_sec": 3.2735210342632453,
                        "throughput_tokens_per_sec": 419.0106923856954
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38403047424,
                        "gpu_max_memory_reserved_bytes": 38403047424
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.1,
                        "cpu_memory_usage_bytes": 1982341120
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0595",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_1": 613.9557085168443,
                            "process_0": 0.0,
                            "process_3": 575.1850221700853,
                            "process_2": 0.0
                        },
                        "ram_power": {
                            "process_1": 0.725987434387207,
                            "process_0": 0.6898570060729982,
                            "process_3": 0.7342100143432617,
                            "process_2": 0.7182025909423828
                        },
                        "cpu_energy": {
                            "process_1": 0.0010536783802417632,
                            "process_0": 0.0010363419245586554,
                            "process_3": 0.0010106247029671067,
                            "process_2": 0.0009964819032466038
                        },
                        "gpu_energy": {
                            "process_1": 0.01040856054906314,
                            "process_0": 0.010370070518272101,
                            "process_3": 0.010431287789467447,
                            "process_2": 0.010370070518272101
                        },
                        "ram_energy": {
                            "process_1": 7.23826781949103e-06,
                            "process_0": 7.139621454077833e-06,
                            "process_3": 7.387565601695115e-06,
                            "process_2": 6.893391925211e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.011469477197124392,
                            "process_0": 0.011413552064284828,
                            "process_3": 0.011449300058036252,
                            "process_2": 0.01137344581344392
                        },
                        "total_energy_joules": {
                            "process_1": 41290.11790964781,
                            "process_0": 41088.78743142538,
                            "process_3": 41217.48020893051,
                            "process_2": 40944.40492839811
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 297.2851826717324,
                        "ram_power_avg": 0.7170642614364624,
                        "cpu_energy_total": 0.004097126911014129,
                        "gpu_energy_total": 0.04157998937507479,
                        "ram_energy_total": 2.865884680047498e-05,
                        "total_energy_kwh": 0.04570577513288939,
                        "total_energy_joules": 164540.79047840182
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09957409316172344,
                        "joules_per_token": 10.042772856347767,
                        "flops_per_joule": 319912054.36547065,
                        "joules_per_flop": 3.1258590801883013e-09
                    },
                    "per-process_emissions": [
                        0.004369297338244537,
                        0.004347992658889305,
                        0.00436161085710891,
                        0.004332714182631461
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0595": {
            "setup": {
                "experiment_id": "0595",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 05:15:29 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_topp_0.9_temp_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_p",
                    "decoder_temperature": 0.8,
                    "decoder_top_k": null,
                    "decoder_top_p": 0.9
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 39.10162747092545,
                        "average_latency_ms_per_batch": 4887.703433865681,
                        "throughput_queries_per_sec": 3.2735210342632453,
                        "throughput_tokens_per_sec": 419.0106923856954
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38403047424,
                        "gpu_max_memory_reserved_bytes": 38403047424
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.1,
                        "cpu_memory_usage_bytes": 1982341120
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0595",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_1": 613.9557085168443,
                            "process_0": 0.0,
                            "process_3": 575.1850221700853,
                            "process_2": 0.0
                        },
                        "ram_power": {
                            "process_1": 0.725987434387207,
                            "process_0": 0.6898570060729982,
                            "process_3": 0.7342100143432617,
                            "process_2": 0.7182025909423828
                        },
                        "cpu_energy": {
                            "process_1": 0.0010536783802417632,
                            "process_0": 0.0010363419245586554,
                            "process_3": 0.0010106247029671067,
                            "process_2": 0.0009964819032466038
                        },
                        "gpu_energy": {
                            "process_1": 0.01040856054906314,
                            "process_0": 0.010370070518272101,
                            "process_3": 0.010431287789467447,
                            "process_2": 0.010370070518272101
                        },
                        "ram_energy": {
                            "process_1": 7.23826781949103e-06,
                            "process_0": 7.139621454077833e-06,
                            "process_3": 7.387565601695115e-06,
                            "process_2": 6.893391925211e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.011469477197124392,
                            "process_0": 0.011413552064284828,
                            "process_3": 0.011449300058036252,
                            "process_2": 0.01137344581344392
                        },
                        "total_energy_joules": {
                            "process_1": 41290.11790964781,
                            "process_0": 41088.78743142538,
                            "process_3": 41217.48020893051,
                            "process_2": 40944.40492839811
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 297.2851826717324,
                        "ram_power_avg": 0.7170642614364624,
                        "cpu_energy_total": 0.004097126911014129,
                        "gpu_energy_total": 0.04157998937507479,
                        "ram_energy_total": 2.865884680047498e-05,
                        "total_energy_kwh": 0.04570577513288939,
                        "total_energy_joules": 164540.79047840182
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09957409316172344,
                        "joules_per_token": 10.042772856347767,
                        "flops_per_joule": 319912054.36547065,
                        "joules_per_flop": 3.1258590801883013e-09
                    },
                    "per-process_emissions": [
                        0.004369297338244537,
                        0.004347992658889305,
                        0.00436161085710891,
                        0.004332714182631461
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0596": {
            "setup": {
                "experiment_id": "0596",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 05:17:07 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_burst_0.4_0.5_4.0_20",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.4,
                    "delay_max": 0.5,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 20
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 41.86416207294678,
                        "average_latency_ms_per_batch": 5233.020259118348,
                        "throughput_queries_per_sec": 3.057507750351354,
                        "throughput_tokens_per_sec": 391.36099204497333
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 37782290432,
                        "gpu_max_memory_reserved_bytes": 37782290432
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.4,
                        "cpu_memory_usage_bytes": 1972453376
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0596",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 1217.7507212028358,
                            "process_2": 1255.2024454081563,
                            "process_3": 34.34265943238576,
                            "process_1": 34.61668183020828
                        },
                        "ram_power": {
                            "process_0": 0.6876926422119142,
                            "process_2": 0.7313904762268066,
                            "process_3": 0.7310085296630859,
                            "process_1": 0.7155461311340332
                        },
                        "cpu_energy": {
                            "process_0": 0.0011985770119335936,
                            "process_2": 0.0010919756811908887,
                            "process_3": 0.0012314198965796095,
                            "process_1": 0.0012801622701281302
                        },
                        "gpu_energy": {
                            "process_0": 0.01050373368075519,
                            "process_2": 0.010504552570299808,
                            "process_3": 0.010513583133080928,
                            "process_1": 0.010513583133080928
                        },
                        "ram_energy": {
                            "process_0": 6.6562704289593995e-06,
                            "process_2": 6.371195041643108e-06,
                            "process_3": 7.283193692511918e-06,
                            "process_1": 7.427200207289871e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.011708966963117743,
                            "process_2": 0.011602899446532342,
                            "process_3": 0.011752286223353047,
                            "process_1": 0.01180117260341635
                        },
                        "total_energy_joules": {
                            "process_0": 42152.28106722388,
                            "process_2": 41770.43800751643,
                            "process_3": 42308.23040407097,
                            "process_1": 42484.22137229886
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 635.4781269683966,
                        "ram_power_avg": 0.71640944480896,
                        "cpu_energy_total": 0.004802134859832222,
                        "gpu_energy_total": 0.042035452517216854,
                        "ram_energy_total": 2.7737859370404295e-05,
                        "total_energy_kwh": 0.04686532523641948,
                        "total_energy_joules": 168715.17085111013
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09711041346992297,
                        "joules_per_token": 10.297556814642952,
                        "flops_per_joule": 311996734.1604221,
                        "joules_per_flop": 3.2051617549490798e-09
                    },
                    "per-process_emissions": [
                        0.004460530964599705,
                        0.004420124544156496,
                        0.004477033436786343,
                        0.004495656703271459
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0596": {
            "setup": {
                "experiment_id": "0596",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 05:17:07 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_burst_0.4_0.5_4.0_20",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.4,
                    "delay_max": 0.5,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 20
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 41.86416207294678,
                        "average_latency_ms_per_batch": 5233.020259118348,
                        "throughput_queries_per_sec": 3.057507750351354,
                        "throughput_tokens_per_sec": 391.36099204497333
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 37782290432,
                        "gpu_max_memory_reserved_bytes": 37782290432
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.4,
                        "cpu_memory_usage_bytes": 1972453376
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0596",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 1217.7507212028358,
                            "process_2": 1255.2024454081563,
                            "process_3": 34.34265943238576,
                            "process_1": 34.61668183020828
                        },
                        "ram_power": {
                            "process_0": 0.6876926422119142,
                            "process_2": 0.7313904762268066,
                            "process_3": 0.7310085296630859,
                            "process_1": 0.7155461311340332
                        },
                        "cpu_energy": {
                            "process_0": 0.0011985770119335936,
                            "process_2": 0.0010919756811908887,
                            "process_3": 0.0012314198965796095,
                            "process_1": 0.0012801622701281302
                        },
                        "gpu_energy": {
                            "process_0": 0.01050373368075519,
                            "process_2": 0.010504552570299808,
                            "process_3": 0.010513583133080928,
                            "process_1": 0.010513583133080928
                        },
                        "ram_energy": {
                            "process_0": 6.6562704289593995e-06,
                            "process_2": 6.371195041643108e-06,
                            "process_3": 7.283193692511918e-06,
                            "process_1": 7.427200207289871e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.011708966963117743,
                            "process_2": 0.011602899446532342,
                            "process_3": 0.011752286223353047,
                            "process_1": 0.01180117260341635
                        },
                        "total_energy_joules": {
                            "process_0": 42152.28106722388,
                            "process_2": 41770.43800751643,
                            "process_3": 42308.23040407097,
                            "process_1": 42484.22137229886
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 635.4781269683966,
                        "ram_power_avg": 0.71640944480896,
                        "cpu_energy_total": 0.004802134859832222,
                        "gpu_energy_total": 0.042035452517216854,
                        "ram_energy_total": 2.7737859370404295e-05,
                        "total_energy_kwh": 0.04686532523641948,
                        "total_energy_joules": 168715.17085111013
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09711041346992297,
                        "joules_per_token": 10.297556814642952,
                        "flops_per_joule": 311996734.1604221,
                        "joules_per_flop": 3.2051617549490798e-09
                    },
                    "per-process_emissions": [
                        0.004460530964599705,
                        0.004420124544156496,
                        0.004477033436786343,
                        0.004495656703271459
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0597": {
            "setup": {
                "experiment_id": "0597",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 05:18:44 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_burst_0.1_0.2_2.0_5",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.1,
                    "delay_max": 0.2,
                    "simulate_burst": true,
                    "burst_interval": 2.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 41.43798003002303,
                        "average_latency_ms_per_batch": 5179.747503752878,
                        "throughput_queries_per_sec": 3.088953658147918,
                        "throughput_tokens_per_sec": 395.3860682429335
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38352715776,
                        "gpu_max_memory_reserved_bytes": 38352715776
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            14.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.4,
                        "cpu_memory_usage_bytes": 1973723136
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0597",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_3": 1044.0661629130836,
                            "process_2": 1044.5111765510687,
                            "process_0": 1502.621824088664,
                            "process_1": 981.2417389435645
                        },
                        "ram_power": {
                            "process_3": 0.7239203453063965,
                            "process_2": 0.7308611869812012,
                            "process_0": 0.6881661415100098,
                            "process_1": 0.7308926582336427
                        },
                        "cpu_energy": {
                            "process_3": 0.0012277510897274627,
                            "process_2": 0.0012245647419094892,
                            "process_0": 0.0012276736141611762,
                            "process_1": 0.0012149752716231887
                        },
                        "gpu_energy": {
                            "process_3": 0.01045861725577879,
                            "process_2": 0.0103897160895432,
                            "process_0": 0.010464185038010498,
                            "process_1": 0.01046706865142788
                        },
                        "ram_energy": {
                            "process_3": 7.542009082680765e-06,
                            "process_2": 7.589218477870199e-06,
                            "process_0": 6.80046088765329e-06,
                            "process_1": 7.539994556359921e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.011693910354588931,
                            "process_2": 0.011621870049930557,
                            "process_0": 0.011698659113059333,
                            "process_1": 0.01168958391760743
                        },
                        "total_energy_joules": {
                            "process_3": 42098.077276520155,
                            "process_2": 41838.732179750004,
                            "process_0": 42115.1728070136,
                            "process_1": 42082.502103386745
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1143.1102256240952,
                        "ram_power_avg": 0.7184600830078125,
                        "cpu_energy_total": 0.004894964717421317,
                        "gpu_energy_total": 0.04177958703476037,
                        "ram_energy_total": 2.9471683004564177e-05,
                        "total_energy_kwh": 0.04670402343518625,
                        "total_energy_joules": 168134.4843666705
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09744580394506994,
                        "joules_per_token": 10.262114524332917,
                        "flops_per_joule": 313074278.05273366,
                        "joules_per_flop": 3.1941301796488114e-09
                    },
                    "per-process_emissions": [
                        0.0044547951495806535,
                        0.004427351395521046,
                        0.004456604189119953,
                        0.00445314699341255
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0597": {
            "setup": {
                "experiment_id": "0597",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 05:18:44 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_burst_0.1_0.2_2.0_5",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.1,
                    "delay_max": 0.2,
                    "simulate_burst": true,
                    "burst_interval": 2.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 41.43798003002303,
                        "average_latency_ms_per_batch": 5179.747503752878,
                        "throughput_queries_per_sec": 3.088953658147918,
                        "throughput_tokens_per_sec": 395.3860682429335
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38352715776,
                        "gpu_max_memory_reserved_bytes": 38352715776
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            14.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.4,
                        "cpu_memory_usage_bytes": 1973723136
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0597",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_3": 1044.0661629130836,
                            "process_2": 1044.5111765510687,
                            "process_0": 1502.621824088664,
                            "process_1": 981.2417389435645
                        },
                        "ram_power": {
                            "process_3": 0.7239203453063965,
                            "process_2": 0.7308611869812012,
                            "process_0": 0.6881661415100098,
                            "process_1": 0.7308926582336427
                        },
                        "cpu_energy": {
                            "process_3": 0.0012277510897274627,
                            "process_2": 0.0012245647419094892,
                            "process_0": 0.0012276736141611762,
                            "process_1": 0.0012149752716231887
                        },
                        "gpu_energy": {
                            "process_3": 0.01045861725577879,
                            "process_2": 0.0103897160895432,
                            "process_0": 0.010464185038010498,
                            "process_1": 0.01046706865142788
                        },
                        "ram_energy": {
                            "process_3": 7.542009082680765e-06,
                            "process_2": 7.589218477870199e-06,
                            "process_0": 6.80046088765329e-06,
                            "process_1": 7.539994556359921e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.011693910354588931,
                            "process_2": 0.011621870049930557,
                            "process_0": 0.011698659113059333,
                            "process_1": 0.01168958391760743
                        },
                        "total_energy_joules": {
                            "process_3": 42098.077276520155,
                            "process_2": 41838.732179750004,
                            "process_0": 42115.1728070136,
                            "process_1": 42082.502103386745
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1143.1102256240952,
                        "ram_power_avg": 0.7184600830078125,
                        "cpu_energy_total": 0.004894964717421317,
                        "gpu_energy_total": 0.04177958703476037,
                        "ram_energy_total": 2.9471683004564177e-05,
                        "total_energy_kwh": 0.04670402343518625,
                        "total_energy_joules": 168134.4843666705
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09744580394506994,
                        "joules_per_token": 10.262114524332917,
                        "flops_per_joule": 313074278.05273366,
                        "joules_per_flop": 3.1941301796488114e-09
                    },
                    "per-process_emissions": [
                        0.0044547951495806535,
                        0.004427351395521046,
                        0.004456604189119953,
                        0.00445314699341255
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0598": {
            "setup": {
                "experiment_id": "0598",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 05:20:16 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_5_temp_0.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 0.4,
                    "decoder_top_k": 5,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.46796165802516,
                        "average_latency_ms_per_batch": 4808.495207253145,
                        "throughput_queries_per_sec": 3.327444306456948,
                        "throughput_tokens_per_sec": 425.91287122648936
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 32073842688,
                        "gpu_max_memory_reserved_bytes": 32073842688
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            27.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.5,
                        "cpu_memory_usage_bytes": 1957064704
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0598",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_0": 228.58981481437252,
                            "process_3": 1668.896613321707,
                            "process_1": 12797.309270826758,
                            "process_2": 232.60746245081137
                        },
                        "ram_power": {
                            "process_0": 0.6811981201171875,
                            "process_3": 0.7093548774719238,
                            "process_1": 0.7246971130371094,
                            "process_2": 0.7090873718261719
                        },
                        "cpu_energy": {
                            "process_0": 0.0011516726180925618,
                            "process_3": 0.001054317396721672,
                            "process_1": 0.0010997728266574993,
                            "process_2": 0.0010946039337850382
                        },
                        "gpu_energy": {
                            "process_0": 0.010240081525395794,
                            "process_3": 0.010277032388284368,
                            "process_1": 0.010240081525395794,
                            "process_2": 0.010244363473262297
                        },
                        "ram_energy": {
                            "process_0": 5.949247448668064e-06,
                            "process_3": 7.020334014662719e-06,
                            "process_1": 6.679236749264096e-06,
                            "process_2": 7.330379676533023e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.011397703390937026,
                            "process_3": 0.011338370119020702,
                            "process_1": 0.01134653358880256,
                            "process_2": 0.011346297786723865
                        },
                        "total_energy_joules": {
                            "process_0": 41031.732207373294,
                            "process_3": 40818.13242847453,
                            "process_1": 40847.52091968922,
                            "process_2": 40846.67203220591
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 3731.850790353412,
                        "ram_power_avg": 0.7060843706130981,
                        "cpu_energy_total": 0.004400366775256771,
                        "gpu_energy_total": 0.041001558912338254,
                        "ram_energy_total": 2.6979197889127902e-05,
                        "total_energy_kwh": 0.04542890488548415,
                        "total_energy_joules": 163544.05758774295
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.10018095577217673,
                        "joules_per_token": 9.981937108626889,
                        "flops_per_joule": 321861785.04604423,
                        "joules_per_flop": 3.1069236748840626e-09
                    },
                    "per-process_emissions": [
                        0.00434195510677746,
                        0.004319352096840937,
                        0.004322461970654335,
                        0.0043223721418524565
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0598": {
            "setup": {
                "experiment_id": "0598",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 05:20:16 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_5_temp_0.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 0.4,
                    "decoder_top_k": 5,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.46796165802516,
                        "average_latency_ms_per_batch": 4808.495207253145,
                        "throughput_queries_per_sec": 3.327444306456948,
                        "throughput_tokens_per_sec": 425.91287122648936
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 32073842688,
                        "gpu_max_memory_reserved_bytes": 32073842688
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            27.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.5,
                        "cpu_memory_usage_bytes": 1957064704
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0598",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_0": 228.58981481437252,
                            "process_3": 1668.896613321707,
                            "process_1": 12797.309270826758,
                            "process_2": 232.60746245081137
                        },
                        "ram_power": {
                            "process_0": 0.6811981201171875,
                            "process_3": 0.7093548774719238,
                            "process_1": 0.7246971130371094,
                            "process_2": 0.7090873718261719
                        },
                        "cpu_energy": {
                            "process_0": 0.0011516726180925618,
                            "process_3": 0.001054317396721672,
                            "process_1": 0.0010997728266574993,
                            "process_2": 0.0010946039337850382
                        },
                        "gpu_energy": {
                            "process_0": 0.010240081525395794,
                            "process_3": 0.010277032388284368,
                            "process_1": 0.010240081525395794,
                            "process_2": 0.010244363473262297
                        },
                        "ram_energy": {
                            "process_0": 5.949247448668064e-06,
                            "process_3": 7.020334014662719e-06,
                            "process_1": 6.679236749264096e-06,
                            "process_2": 7.330379676533023e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.011397703390937026,
                            "process_3": 0.011338370119020702,
                            "process_1": 0.01134653358880256,
                            "process_2": 0.011346297786723865
                        },
                        "total_energy_joules": {
                            "process_0": 41031.732207373294,
                            "process_3": 40818.13242847453,
                            "process_1": 40847.52091968922,
                            "process_2": 40846.67203220591
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 3731.850790353412,
                        "ram_power_avg": 0.7060843706130981,
                        "cpu_energy_total": 0.004400366775256771,
                        "gpu_energy_total": 0.041001558912338254,
                        "ram_energy_total": 2.6979197889127902e-05,
                        "total_energy_kwh": 0.04542890488548415,
                        "total_energy_joules": 163544.05758774295
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.10018095577217673,
                        "joules_per_token": 9.981937108626889,
                        "flops_per_joule": 321861785.04604423,
                        "joules_per_flop": 3.1069236748840626e-09
                    },
                    "per-process_emissions": [
                        0.00434195510677746,
                        0.004319352096840937,
                        0.004322461970654335,
                        0.0043223721418524565
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0599": {
            "setup": {
                "experiment_id": "0599",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 05:21:44 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_40",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 40,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 32.301218430977315,
                        "average_latency_ms_per_batch": 8075.304607744329,
                        "throughput_queries_per_sec": 3.96269881501579,
                        "throughput_tokens_per_sec": 507.2254483220211
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 40011563008,
                        "gpu_max_memory_reserved_bytes": 40011563008
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            2.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.1,
                        "cpu_memory_usage_bytes": 1974964224
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0599",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_2": 879.591488569952,
                            "process_0": 858.7185196960552,
                            "process_1": 748.53479634949,
                            "process_3": 698.519039795782
                        },
                        "ram_power": {
                            "process_2": 0.7164416313171387,
                            "process_0": 0.6878542900085449,
                            "process_1": 0.7301630973815918,
                            "process_3": 0.7237000465393066
                        },
                        "cpu_energy": {
                            "process_2": 0.0008422545026205625,
                            "process_0": 0.0008946150416813907,
                            "process_1": 0.0008739265569038255,
                            "process_3": 0.0009198689626864509
                        },
                        "gpu_energy": {
                            "process_2": 0.008144438182213776,
                            "process_0": 0.008124624833026672,
                            "process_1": 0.008191356553080453,
                            "process_3": 0.008211351291299351
                        },
                        "ram_energy": {
                            "process_2": 5.444007700937272e-06,
                            "process_0": 5.922910892194794e-06,
                            "process_1": 5.760014782245072e-06,
                            "process_3": 6.444908924642764e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.008992136692535279,
                            "process_0": 0.009025162785600256,
                            "process_1": 0.009071043124766523,
                            "process_3": 0.009137665162910443
                        },
                        "total_energy_joules": {
                            "process_2": 32371.692093127003,
                            "process_0": 32490.586028160924,
                            "process_1": 32655.75524915948,
                            "process_3": 32895.59458647759
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 796.3409611028198,
                        "ram_power_avg": 0.7145397663116455,
                        "cpu_energy_total": 0.0035306650638922293,
                        "gpu_energy_total": 0.03267177085962025,
                        "ram_energy_total": 2.3571842300019902e-05,
                        "total_energy_kwh": 0.0362260077658125,
                        "total_energy_joules": 130413.62795692499
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.12563104222061486,
                        "joules_per_token": 7.959816159480284,
                        "flops_per_joule": 403627927.0311403,
                        "joules_per_flop": 2.4775292615539186e-09
                    },
                    "per-process_emissions": [
                        0.0034255544730213143,
                        0.003438135763174418,
                        0.003455613878379807,
                        0.0034809935438107334
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0599": {
            "setup": {
                "experiment_id": "0599",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 05:21:44 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_40",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 40,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 32.301218430977315,
                        "average_latency_ms_per_batch": 8075.304607744329,
                        "throughput_queries_per_sec": 3.96269881501579,
                        "throughput_tokens_per_sec": 507.2254483220211
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 40011563008,
                        "gpu_max_memory_reserved_bytes": 40011563008
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            2.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.1,
                        "cpu_memory_usage_bytes": 1974964224
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0599",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_2": 879.591488569952,
                            "process_0": 858.7185196960552,
                            "process_1": 748.53479634949,
                            "process_3": 698.519039795782
                        },
                        "ram_power": {
                            "process_2": 0.7164416313171387,
                            "process_0": 0.6878542900085449,
                            "process_1": 0.7301630973815918,
                            "process_3": 0.7237000465393066
                        },
                        "cpu_energy": {
                            "process_2": 0.0008422545026205625,
                            "process_0": 0.0008946150416813907,
                            "process_1": 0.0008739265569038255,
                            "process_3": 0.0009198689626864509
                        },
                        "gpu_energy": {
                            "process_2": 0.008144438182213776,
                            "process_0": 0.008124624833026672,
                            "process_1": 0.008191356553080453,
                            "process_3": 0.008211351291299351
                        },
                        "ram_energy": {
                            "process_2": 5.444007700937272e-06,
                            "process_0": 5.922910892194794e-06,
                            "process_1": 5.760014782245072e-06,
                            "process_3": 6.444908924642764e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.008992136692535279,
                            "process_0": 0.009025162785600256,
                            "process_1": 0.009071043124766523,
                            "process_3": 0.009137665162910443
                        },
                        "total_energy_joules": {
                            "process_2": 32371.692093127003,
                            "process_0": 32490.586028160924,
                            "process_1": 32655.75524915948,
                            "process_3": 32895.59458647759
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 796.3409611028198,
                        "ram_power_avg": 0.7145397663116455,
                        "cpu_energy_total": 0.0035306650638922293,
                        "gpu_energy_total": 0.03267177085962025,
                        "ram_energy_total": 2.3571842300019902e-05,
                        "total_energy_kwh": 0.0362260077658125,
                        "total_energy_joules": 130413.62795692499
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.12563104222061486,
                        "joules_per_token": 7.959816159480284,
                        "flops_per_joule": 403627927.0311403,
                        "joules_per_flop": 2.4775292615539186e-09
                    },
                    "per-process_emissions": [
                        0.0034255544730213143,
                        0.003438135763174418,
                        0.003455613878379807,
                        0.0034809935438107334
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0600": {
            "setup": {
                "experiment_id": "0600",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 05:23:18 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_300_temp_0.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 0.4,
                    "decoder_top_k": 300,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.497941877052654,
                        "average_latency_ms_per_batch": 4812.242734631582,
                        "throughput_queries_per_sec": 3.324853063802264,
                        "throughput_tokens_per_sec": 425.5811921666898
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38449184768,
                        "gpu_max_memory_reserved_bytes": 38449184768
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.3,
                        "cpu_memory_usage_bytes": 1977954304
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0600",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_1": 36228.078238040725,
                            "process_2": 210.01284724228663,
                            "process_3": 211.58392456116567,
                            "process_0": 210.65064221878993
                        },
                        "ram_power": {
                            "process_1": 0.7250432968139648,
                            "process_2": 0.7254180908203126,
                            "process_3": 0.7091217041015625,
                            "process_0": 0.6896510124206543
                        },
                        "cpu_energy": {
                            "process_1": 0.0009980522933219617,
                            "process_2": 0.00113090726968403,
                            "process_3": 0.001176104810374454,
                            "process_0": 0.0011445822387850058
                        },
                        "gpu_energy": {
                            "process_1": 0.010250579033790785,
                            "process_2": 0.01025040708920777,
                            "process_3": 0.010253185702541856,
                            "process_0": 0.010251490701186583
                        },
                        "ram_energy": {
                            "process_1": 6.838006110480455e-06,
                            "process_2": 7.781056059963446e-06,
                            "process_3": 7.5367509186194e-06,
                            "process_0": 6.721122499516061e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.011255469333223226,
                            "process_2": 0.01138909541495177,
                            "process_3": 0.011436827263834926,
                            "process_0": 0.011402794062471109
                        },
                        "total_energy_joules": {
                            "process_1": 40519.68959960361,
                            "process_2": 41000.74349382637,
                            "process_3": 41172.57814980573,
                            "process_0": 41050.058624895995
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 9215.081413015741,
                        "ram_power_avg": 0.7123085260391235,
                        "cpu_energy_total": 0.004449646612165452,
                        "gpu_energy_total": 0.041005662526726994,
                        "ram_energy_total": 2.8876935588579363e-05,
                        "total_energy_kwh": 0.04548418607448103,
                        "total_energy_joules": 163743.06986813172
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.10005919647893884,
                        "joules_per_token": 9.994083854256086,
                        "flops_per_joule": 321470596.2900035,
                        "joules_per_flop": 3.110704405132857e-09
                    },
                    "per-process_emissions": [
                        0.004287771042491388,
                        0.0043386758983258765,
                        0.004356859346157915,
                        0.004343894398098369
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0600": {
            "setup": {
                "experiment_id": "0600",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 05:23:18 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_300_temp_0.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 0.4,
                    "decoder_top_k": 300,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.497941877052654,
                        "average_latency_ms_per_batch": 4812.242734631582,
                        "throughput_queries_per_sec": 3.324853063802264,
                        "throughput_tokens_per_sec": 425.5811921666898
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38449184768,
                        "gpu_max_memory_reserved_bytes": 38449184768
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.3,
                        "cpu_memory_usage_bytes": 1977954304
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0600",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_1": 36228.078238040725,
                            "process_2": 210.01284724228663,
                            "process_3": 211.58392456116567,
                            "process_0": 210.65064221878993
                        },
                        "ram_power": {
                            "process_1": 0.7250432968139648,
                            "process_2": 0.7254180908203126,
                            "process_3": 0.7091217041015625,
                            "process_0": 0.6896510124206543
                        },
                        "cpu_energy": {
                            "process_1": 0.0009980522933219617,
                            "process_2": 0.00113090726968403,
                            "process_3": 0.001176104810374454,
                            "process_0": 0.0011445822387850058
                        },
                        "gpu_energy": {
                            "process_1": 0.010250579033790785,
                            "process_2": 0.01025040708920777,
                            "process_3": 0.010253185702541856,
                            "process_0": 0.010251490701186583
                        },
                        "ram_energy": {
                            "process_1": 6.838006110480455e-06,
                            "process_2": 7.781056059963446e-06,
                            "process_3": 7.5367509186194e-06,
                            "process_0": 6.721122499516061e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.011255469333223226,
                            "process_2": 0.01138909541495177,
                            "process_3": 0.011436827263834926,
                            "process_0": 0.011402794062471109
                        },
                        "total_energy_joules": {
                            "process_1": 40519.68959960361,
                            "process_2": 41000.74349382637,
                            "process_3": 41172.57814980573,
                            "process_0": 41050.058624895995
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 9215.081413015741,
                        "ram_power_avg": 0.7123085260391235,
                        "cpu_energy_total": 0.004449646612165452,
                        "gpu_energy_total": 0.041005662526726994,
                        "ram_energy_total": 2.8876935588579363e-05,
                        "total_energy_kwh": 0.04548418607448103,
                        "total_energy_joules": 163743.06986813172
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.10005919647893884,
                        "joules_per_token": 9.994083854256086,
                        "flops_per_joule": 321470596.2900035,
                        "joules_per_flop": 3.110704405132857e-09
                    },
                    "per-process_emissions": [
                        0.004287771042491388,
                        0.0043386758983258765,
                        0.004356859346157915,
                        0.004343894398098369
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0601": {
            "setup": {
                "experiment_id": "0601",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 05:24:52 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_100_temp_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 100,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.910677327017765,
                        "average_latency_ms_per_batch": 4863.834665877221,
                        "throughput_queries_per_sec": 3.2895855017954867,
                        "throughput_tokens_per_sec": 421.0669442298223
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38147194880,
                        "gpu_max_memory_reserved_bytes": 38147194880
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            28.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.6,
                        "cpu_memory_usage_bytes": 1979957248
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0601",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_3": 814.3299482756216,
                            "process_2": 2.7921557552715077,
                            "process_1": 2346.5022616066644,
                            "process_0": 247.9972109958649
                        },
                        "ram_power": {
                            "process_3": 0.7262206077575685,
                            "process_2": 0.7341113090515138,
                            "process_1": 0.7324891090393066,
                            "process_0": 0.6893577575683594
                        },
                        "cpu_energy": {
                            "process_3": 0.0012960593899606463,
                            "process_2": 0.0013614040354659662,
                            "process_1": 0.0012964243531005195,
                            "process_0": 0.0013087029959697246
                        },
                        "gpu_energy": {
                            "process_3": 0.01032120436807027,
                            "process_2": 0.010319438533322511,
                            "process_1": 0.010319438533322511,
                            "process_0": 0.010308533246821128
                        },
                        "ram_energy": {
                            "process_3": 7.289190350563548e-06,
                            "process_2": 7.756466864440214e-06,
                            "process_1": 7.78531595890059e-06,
                            "process_0": 7.041199469467106e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.011624552948381483,
                            "process_2": 0.01168859903565292,
                            "process_1": 0.011623648202381933,
                            "process_0": 0.011624277442260323
                        },
                        "total_energy_joules": {
                            "process_3": 41848.39061417334,
                            "process_2": 42078.956528350514,
                            "process_1": 41845.13352857496,
                            "process_0": 41847.39879213716
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 852.9053941583556,
                        "ram_power_avg": 0.720544695854187,
                        "cpu_energy_total": 0.0052625907744968566,
                        "gpu_energy_total": 0.04126861468153642,
                        "ram_energy_total": 2.987217264337146e-05,
                        "total_energy_kwh": 0.04656107762867666,
                        "total_energy_joules": 167619.87946323596
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09774496946582938,
                        "joules_per_token": 10.230705533644773,
                        "flops_per_joule": 314035438.26321155,
                        "joules_per_flop": 3.1843539873415216e-09
                    },
                    "per-process_emissions": [
                        0.004428373445685926,
                        0.00445277180263198,
                        0.004428028782697397,
                        0.00442826849162907
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0601": {
            "setup": {
                "experiment_id": "0601",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 05:24:52 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_topk_100_temp_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": "top_k",
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 100,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.910677327017765,
                        "average_latency_ms_per_batch": 4863.834665877221,
                        "throughput_queries_per_sec": 3.2895855017954867,
                        "throughput_tokens_per_sec": 421.0669442298223
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38147194880,
                        "gpu_max_memory_reserved_bytes": 38147194880
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            28.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.6,
                        "cpu_memory_usage_bytes": 1979957248
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0601",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_3": 814.3299482756216,
                            "process_2": 2.7921557552715077,
                            "process_1": 2346.5022616066644,
                            "process_0": 247.9972109958649
                        },
                        "ram_power": {
                            "process_3": 0.7262206077575685,
                            "process_2": 0.7341113090515138,
                            "process_1": 0.7324891090393066,
                            "process_0": 0.6893577575683594
                        },
                        "cpu_energy": {
                            "process_3": 0.0012960593899606463,
                            "process_2": 0.0013614040354659662,
                            "process_1": 0.0012964243531005195,
                            "process_0": 0.0013087029959697246
                        },
                        "gpu_energy": {
                            "process_3": 0.01032120436807027,
                            "process_2": 0.010319438533322511,
                            "process_1": 0.010319438533322511,
                            "process_0": 0.010308533246821128
                        },
                        "ram_energy": {
                            "process_3": 7.289190350563548e-06,
                            "process_2": 7.756466864440214e-06,
                            "process_1": 7.78531595890059e-06,
                            "process_0": 7.041199469467106e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.011624552948381483,
                            "process_2": 0.01168859903565292,
                            "process_1": 0.011623648202381933,
                            "process_0": 0.011624277442260323
                        },
                        "total_energy_joules": {
                            "process_3": 41848.39061417334,
                            "process_2": 42078.956528350514,
                            "process_1": 41845.13352857496,
                            "process_0": 41847.39879213716
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 852.9053941583556,
                        "ram_power_avg": 0.720544695854187,
                        "cpu_energy_total": 0.0052625907744968566,
                        "gpu_energy_total": 0.04126861468153642,
                        "ram_energy_total": 2.987217264337146e-05,
                        "total_energy_kwh": 0.04656107762867666,
                        "total_energy_joules": 167619.87946323596
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09774496946582938,
                        "joules_per_token": 10.230705533644773,
                        "flops_per_joule": 314035438.26321155,
                        "joules_per_flop": 3.1843539873415216e-09
                    },
                    "per-process_emissions": [
                        0.004428373445685926,
                        0.00445277180263198,
                        0.004428028782697397,
                        0.00442826849162907
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0602": {
            "setup": {
                "experiment_id": "0602",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 05:26:29 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_burst_0.1_0.2_4.0_8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.1,
                    "delay_max": 0.2,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 8
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 43.40148545982083,
                        "average_latency_ms_per_batch": 5425.185682477604,
                        "throughput_queries_per_sec": 2.9492078126795156,
                        "throughput_tokens_per_sec": 377.498600022978
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38352715776,
                        "gpu_max_memory_reserved_bytes": 38352715776
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            4.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.5,
                        "cpu_memory_usage_bytes": 1971462144
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0602",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 1304.9655981499768,
                            "process_2": 1536.633934062893,
                            "process_1": 1005.6291889542947,
                            "process_3": 790.6367420360577
                        },
                        "ram_power": {
                            "process_0": 0.6864452362060547,
                            "process_2": 0.7246499061584473,
                            "process_1": 0.7307395935058594,
                            "process_3": 0.7234811782836914
                        },
                        "cpu_energy": {
                            "process_0": 0.0012884636082526413,
                            "process_2": 0.001292266233531336,
                            "process_1": 0.001293767860062872,
                            "process_3": 0.0012039801656537749
                        },
                        "gpu_energy": {
                            "process_0": 0.01052037230518188,
                            "process_2": 0.010506341182846768,
                            "process_1": 0.010533967038279357,
                            "process_3": 0.010532648981669723
                        },
                        "ram_energy": {
                            "process_0": 7.533298103528976e-06,
                            "process_2": 7.974885159829684e-06,
                            "process_1": 8.039238315012976e-06,
                            "process_3": 7.783012049890154e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.011816369211538048,
                            "process_2": 0.011806582301537935,
                            "process_1": 0.011835774136657243,
                            "process_3": 0.011744412159373383
                        },
                        "total_energy_joules": {
                            "process_0": 42538.92916153697,
                            "process_2": 42503.696285536564,
                            "process_1": 42608.786891966076,
                            "process_3": 42279.88377374418
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1159.4663658008055,
                        "ram_power_avg": 0.7163289785385132,
                        "cpu_energy_total": 0.005078477867500624,
                        "gpu_energy_total": 0.04209332950797773,
                        "ram_energy_total": 3.133043362826179e-05,
                        "total_energy_kwh": 0.04720313780910661,
                        "total_energy_joules": 169931.29611278378
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09641543597199366,
                        "joules_per_token": 10.371783210008775,
                        "flops_per_joule": 309763907.60844696,
                        "joules_per_flop": 3.228265060705642e-09
                    },
                    "per-process_emissions": [
                        0.00450144585113542,
                        0.0044977175277708765,
                        0.004508838157359577,
                        0.00447403381211329
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0602": {
            "setup": {
                "experiment_id": "0602",
                "cycle_id": 3,
                "date_time": "April 25, 2025 at 05:26:29 PM",
                "model": "meta-llama/Llama-3.2-3B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_burst_0.1_0.2_4.0_8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoding_mode": null,
                    "decoder_temperature": 1.0,
                    "decoder_top_k": null,
                    "decoder_top_p": null
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.1,
                    "delay_max": 0.2,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 8
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": null,
                    "load_in_8bit": null,
                    "load_in_4bit": null,
                    "cached_flops_for_quantised_models": 52638582308864
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 3212749824,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 43.40148545982083,
                        "average_latency_ms_per_batch": 5425.185682477604,
                        "throughput_queries_per_sec": 2.9492078126795156,
                        "throughput_tokens_per_sec": 377.498600022978
                    }
                },
                "compute_metrics": {
                    "flops": 52638582308864,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 25722925568,
                        "gpu_max_memory_allocated_bytes": 25722925568,
                        "gpu_current_memory_reserved_bytes": 38352715776,
                        "gpu_max_memory_reserved_bytes": 38352715776
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            4.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.5,
                        "cpu_memory_usage_bytes": 1971462144
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0602",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 1304.9655981499768,
                            "process_2": 1536.633934062893,
                            "process_1": 1005.6291889542947,
                            "process_3": 790.6367420360577
                        },
                        "ram_power": {
                            "process_0": 0.6864452362060547,
                            "process_2": 0.7246499061584473,
                            "process_1": 0.7307395935058594,
                            "process_3": 0.7234811782836914
                        },
                        "cpu_energy": {
                            "process_0": 0.0012884636082526413,
                            "process_2": 0.001292266233531336,
                            "process_1": 0.001293767860062872,
                            "process_3": 0.0012039801656537749
                        },
                        "gpu_energy": {
                            "process_0": 0.01052037230518188,
                            "process_2": 0.010506341182846768,
                            "process_1": 0.010533967038279357,
                            "process_3": 0.010532648981669723
                        },
                        "ram_energy": {
                            "process_0": 7.533298103528976e-06,
                            "process_2": 7.974885159829684e-06,
                            "process_1": 8.039238315012976e-06,
                            "process_3": 7.783012049890154e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.011816369211538048,
                            "process_2": 0.011806582301537935,
                            "process_1": 0.011835774136657243,
                            "process_3": 0.011744412159373383
                        },
                        "total_energy_joules": {
                            "process_0": 42538.92916153697,
                            "process_2": 42503.696285536564,
                            "process_1": 42608.786891966076,
                            "process_3": 42279.88377374418
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1159.4663658008055,
                        "ram_power_avg": 0.7163289785385132,
                        "cpu_energy_total": 0.005078477867500624,
                        "gpu_energy_total": 0.04209332950797773,
                        "ram_energy_total": 3.133043362826179e-05,
                        "total_energy_kwh": 0.04720313780910661,
                        "total_energy_joules": 169931.29611278378
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09641543597199366,
                        "joules_per_token": 10.371783210008775,
                        "flops_per_joule": 309763907.60844696,
                        "joules_per_flop": 3.228265060705642e-09
                    },
                    "per-process_emissions": [
                        0.00450144585113542,
                        0.0044977175277708765,
                        0.004508838157359577,
                        0.00447403381211329
                    ]
                },
                "local_energy_results": {}
            }
        }
    }
]