{
  "config_name": "R5_Real_Time_Mobile_Inference",
  "max_input_tokens": 128,
  "max_output_tokens": 128,
  "number_input_prompts": 128,
  "decode_token_to_text": true,
  "decoder_config": {
    "decoding_mode": "top_p",
    "decoder_temperature": 1.0,
    "decoder_top_k": null,
    "decoder_top_p": 0.9
  },
  "query_rate": 1.0,
  "latency_simulation": {
    "simulate": true,
    "delay_min": 0.2,
    "delay_max": 0.6,
    "simulate_burst": true,
    "burst_interval": 5.0,
    "burst_size": 8
  },
  "fp_precision": "torch.float16",
  "quantisation": {
    "quantization": true,
    "load_in_8bit": true,
    "load_in_4bit": false,
    "cached_flops_for_quantised_models": 52638582308864
  },
  "batching_options": {
    "batch_size___fixed_batching": 1,
    "adaptive_batching": false,
    "adaptive_max_tokens": 0,
    "max_batch_size___adaptive_batching": 0
  },
  "sharding_config": {
    "fsdp_config": {
      "use_orig_params": false,
      "cpu_offload": false
    },
    "sharding_strategy": "NO_SHARD"
  },
  "accelerate_config": {
    "distributed_type": "DistributedType.MULTI_GPU",
    "num_processes": 1
  },
  "inference_type": "pure_generative",
  "backend": "pytorch"
}