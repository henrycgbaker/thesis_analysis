{
  "config_name": "R7_anti_platonic_ideal",
  "max_input_tokens": 128,
  "max_output_tokens": 128,
  "number_input_prompts": 128,
  "decode_token_to_text": true,
  "decoder_config": {
    "decoding_mode": "greedy",
    "decoder_temperature": 0.8,
    "decoder_top_k": null,
    "decoder_top_p": null
  },
  "query_rate": 1.0,
  "latency_simulation": {
    "simulate": true,
    "delay_min": 0.4,
    "delay_max": 0.5,
    "simulate_burst": true,
    "burst_interval": 4.0,
    "burst_size": 8
  },
  "fp_precision": "torch.float32",
  "quantisation": {
    "quantization": false,
    "load_in_8bit": null,
    "load_in_4bit": null,
    "cached_flops_for_quantised_models": 52638582308864
  },
  "batching_options": {
    "batch_size___fixed_batching": 1,
    "adaptive_batching": false,
    "adaptive_max_tokens": 0,
    "max_batch_size___adaptive_batching": 0
  },
  "sharding_config": {
    "fsdp_config": {
      "use_orig_params": false,
      "cpu_offload": false
    },
    "sharding_strategy": "NO_SHARD"
  },
  "accelerate_config": {
    "distributed_type": "DistributedType.MULTI_GPU",
    "num_processes": 4
  },
  "inference_type": "pure_generative",
  "backend": "pytorch"
}