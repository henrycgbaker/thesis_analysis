{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b6f5ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All rows have consistent 'total_generated_tokens' = 16384\n",
      "----------------------------------------------------------------------------------------------------\n",
      "✅ FLOPs value is constant: 16949970993152\n",
      "Original distribution:\n",
      "flops\n",
      "16949970993152    53\n",
      "Name: count, dtype: int64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "✅ Total generated tokens value is constant: 16384\n",
      "Original distribution:\n",
      "total_generated_tokens\n",
      "16384    53\n",
      "Name: count, dtype: int64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import warnings\n",
    "warnings.simplefilter('always')\n",
    "\n",
    "from scripts.a_data_loading_cleaning import run_load_clean_diagnose_data\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "csv_path = \"results/controlled_results_latency_full.csv\"\n",
    "df = run_load_clean_diagnose_data(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be4f6a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['config_name', 'experiment_id', 'date_time', 'model', 'num_processes',\n",
       "       'batch_size___fixed_batching', 'decoder_temperature', 'decoder_top_k',\n",
       "       'decoder_top_p', 'latency_simulation_simulate',\n",
       "       'latency_simulation_delay_max', 'latency_simulation_delay_min',\n",
       "       'latency_simulation_simulate_burst', 'latency_simulation_burst_size',\n",
       "       'latency_simulation_burst_interval', 'fp_precision', 'quantization',\n",
       "       'load_in_8bit', 'load_in_4bit', 'total_input_tokens',\n",
       "       'total_generated_tokens', 'date_time', 'total_params',\n",
       "       'max_input_tokens', 'max_output_tokens', 'number_input_prompts',\n",
       "       'total_energy_kwh', 'total_energy_joules', 'flops', 'tokens_per_joule',\n",
       "       'joules_per_token', 'flops_per_joule', 'joules_per_flop',\n",
       "       'total_inference_time_sec', 'average_latency_ms_per_batch',\n",
       "       'throughput_queries_per_sec', 'throughput_tokens_per_sec',\n",
       "       'total_energy_kwh_process_0', 'total_energy_kwh_process_1',\n",
       "       'total_energy_kwh_process_2', 'total_energy_kwh_process_3',\n",
       "       'gpu_power_avg', 'ram_power_avg', 'cpu_energy_total',\n",
       "       'gpu_energy_total', 'flops_per_token', 'energy_per_token_kwh',\n",
       "       'divergence_energy_flops'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
